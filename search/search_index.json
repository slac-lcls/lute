{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Setup LUTE is publically available on GitHub . In order to run it, the first step is to clone the repository: # Navigate to the directory of your choice. git clone@github.com:slac-lcls/lute The repository directory structure is as follows: lute |--- config # Configuration YAML files (see below) and templates for third party config |--- docs # Documentation (including this page) |--- launch_scripts # Entry points for using SLURM and communicating with Airflow |--- lute # Code |--- run_task.py # Script to run an individual managed Task |--- ... |--- utilities # Help utility programs |--- workflows # This directory contains workflow definitions. It is synced elsewhere and not used directly. In general, most interactions with the software will be through scripts located in the launch_scripts directory. Some users (for certain use-cases) may also choose to run the run_task.py script directly - it's location has been highlighted within hierarchy. To begin with you will need a YAML file, templates for which are available in the config directory. The structure of the YAML file and how to use the various launch scripts are described in more detail below. A note on utilties In the utilities directory there are two useful programs to provide assistance with using the software: utilities/dbview : LUTE stores all parameters for every analysis routine it runs (as well as results) in a database. This database is stored in the work_dir defined in the YAML file (see below). The dbview utility is a TUI application (Text-based user interface) which runs in the terminal. It allows you to navigate a LUTE database using the arrow keys, etc. Usage is: utilities/dbview -p <path/to/lute.db> . utilities/lute_help : This utility provides help and usage information for running LUTE software. E.g., it provides access to parameter descriptions to assist in properly filling out a configuration YAML. It's usage is described in slightly more detail below. Basic Usage Overview LUTE runs code as Task s that are managed by an Executor . The Executor provides modifications to the environment the Task runs in, as well as controls details of inter-process communication, reporting results to the eLog, etc. Combinations of specific Executor s and Task s are already provided, and are referred to as managed Task s. Managed Task s are submitted as a single unit. They can be run individually, or a series of independent steps can be submitted all at once in the form of a workflow, or directed acyclic graph ( DAG ). This latter option makes use of Airflow to manage the individual execution steps. Running analysis with LUTE is the process of submitting one or more managed Task s. This is generally a two step process. First, a configuration YAML file is prepared. This contains the parameterizations of all the Task s which you may run. Individual managed Task submission, or workflow ( DAG ) submission. These two steps are described below. Preparing a Configuration YAML All Task s are parameterized through a single configuration YAML file - even third party code which requires its own configuration files is managed through this YAML file. The basic structure is split into two documents, a brief header section which contains information that is applicable across all Task s, such as the experiment name, run numbers and the working directory, followed by per Task parameters: %YAML 1.3 --- title: \"Some title.\" experiment: \"MYEXP123\" # run: 12 # Does not need to be provided date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- TaskOne: param_a: 123 param_b: 456 param_c: sub_var: 3 sub_var2: 4 TaskTwo: new_param1: 3 new_param2: 4 # ... ... In the first document, the header, it is important that the work_dir is properly specified. This is the root directory from which Task outputs will be written, and the LUTE database will be stored. It may also be desirable to modify the task_timeout parameter which defines the time limit for individual Task jobs. By default it is set to 10 minutes, although this may not be sufficient for long running jobs. This value will be applied to all Task s so should account for the longest running job you expect. The actual analysis parameters are defined in the second document. As these vary from Task to Task , a full description will not be provided here. An actual template with real Task parameters is available in config/test.yaml . Your analysis POC can also help you set up and choose the correct Task s to include as a starting point. The template YAML file has further descriptions of what each parameter does and how to fill it out. You can also refer to the lute_help program described under the following sub-heading. Some things to consider and possible points of confusion: While we will be submitting managed Task s, the parameters are defined at the Task level. I.e. the managed Task and Task itself have different names, and the names in the YAML refer to the latter. This is because a single Task can be run using different Executor configurations, but using the same parameters. The list of managed Task s is in lute/managed_tasks.py . A table is also provided below for some routines of interest.. Managed Task The Task it Runs Task Description SmallDataProducer SubmitSMD Smalldata production CrystFELIndexer IndexCrystFEL Crystallographic indexing PartialatorMerger MergePartialator Crystallographic merging HKLComparer CompareHKL Crystallographic figures of merit HKLManipulator ManipulateHKL Crystallographic format conversions DimpleSolver DimpleSolve Crystallographic structure solution with molecular replacement PeakFinderPyAlgos FindPeaksPyAlgos Peak finding with PyAlgos algorithm. PeakFinderPsocake FindPeaksPsocake Peak finding with psocake algorithm. StreamFileConcatenator ConcatenateStreamFiles Stream file concatenation. How do I know what parameters are available, and what they do? A summary of Task parameters is available through the lute_help program. > utilities/lute_help -t [TaskName] Note, some parameters may say \"Unknown description\" - this either means they are using an old-style defintion that does not include parameter help, or they may have some internal use. In particular you will see this for lute_config on every Task , this parameter is filled in automatically and should be ignored. E.g. as an example: > utilities/lute_help -t IndexCrystFEL INFO:__main__:Fetching parameter information for IndexCrystFEL. IndexCrystFEL ------------- Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Required Parameters: -------------------- [...] All Parameters: ------------- [...] highres (number) Mark all pixels greater than `x` has bad. profile (boolean) - Default: False Display timing data to monitor performance. temp_dir (string) Specify a path for the temp files folder. wait_for_file (integer) - Default: 0 Wait at most `x` seconds for a file to be created. A value of -1 means wait forever. no_image_data (boolean) - Default: False Load only the metadata, no iamges. Can check indexability without high data requirements. [...] Running Managed Task s and Workflows (DAGs) After a YAML file has been filled in you can run a Task . There are multiple ways to submit a Task , but there are 3 that are most likely: Run a single managed Task interactively by running python ... Run a single managed Task as a batch job (e.g. on S3DF) via a SLURM submission submit_slurm.sh ... Run a DAG (workflow with multiple managed Task s). These will be covered in turn below; however, in general all methods will require two parameters: the path to a configuration YAML file, and the name of the managed Task or workflow you want to run. When submitting via SLURM or submitting an entire workflow there are additional parameters to control these processes. Running single managed Task s interactively The simplest submission method is just to run Python interactively. In most cases this is not practical for long-running analysis, but may be of use for short Task s or when debugging. From the root directory of the LUTE repository (or after installation) you can use the run_task.py script: > python -B [-O] run_task.py -t <ManagedTaskName> -c </path/to/config/yaml> The command-line arguments in square brackets [] are optional, while those in <> must be provided: -O is the flag controlling whether you run in debug or non-debug mode. By default, i.e. if you do NOT provide this flag you will run in debug mode which enables verbose printing. Passing -O will turn off debug to minimize output. -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. Submitting a single managed Task as a batch job On S3DF you can also submit individual managed Task s to run as batch jobs. To do so use launch_scripts/submit_slurm.sh > launch_scripts/submit_slurm.sh -t <ManagedTaskName> -c </path/to/config/yaml> [--debug] $SLURM_ARGS As before command-line arguments in square brackets [] are optional, while those in <> must be provided -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. --debug is the flag to control whether or not to run in debug mode. In addition to the LUTE-specific arguments, SLURM arguments must also be provided ( $SLURM_ARGS above). You can provide as many as you want; however you will need to at least provide: --partition=<partition/queue> - The queue to run on, in general for LCLS this is milano --account=lcls:<experiment> - The account to use for batch job accounting. You will likely also want to provide at a minimum: --ntasks=<...> to control the number of cores in allocated. In general, it is best to prefer the long-form of the SLURM-argument ( --arg=<...> ) in order to avoid potential clashes with present or future LUTE arguments. Workflow (DAG) submission Finally, you can submit a full workflow (e.g. SFX analysis, smalldata production and summary results, geometry optimization...). This can be done using a single script, submit_launch_airflow.sh , similarly to the SLURM submission above: > launch_scripts/submit_launch_airflow.sh /path/to/lute/launch_scripts/launch_airflow.py -c </path/to/yaml.yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS The submission process is slightly more complicated in this case. A more in-depth explanation is provided under \"Airflow Launch Steps\", in the advanced usage section below if interested. The parameters are as follows - as before command-line arguments in square brackets [] are optional, while those in <> must be provided: The first argument (must be first) is the full path to the launch_scripts/launch_airflow.py script located in whatever LUTE installation you are running. All other arguments can come afterwards in any order. -c </path/...> is the path to the configuration YAML to use. -w <dag_name> is the name of the DAG (workflow) to run. This replaces the task name provided when using the other two methods above. A DAG list is provided below. --debug controls whether to use debug mode (verbose printing) --test controls whether to use the test or production instance of Airflow to manage the DAG. The instances are running identical versions of Airflow, but the test instance may have \"test\" or more bleeding edge development DAGs. The $SLURM_ARGS must be provided in the same manner as when submitting an individual managed Task by hand to be run as batch job with the script above. Note that these parameters will be used as the starting point for the SLURM arguments of every managed Task in the DAG; however, individual steps in the DAG may have overrides built-in where appropriate to make sure that step is not submitted with potentially incompatible arguments. For example, a single threaded analysis Task may be capped to running on one core, even if in general everything should be running on 100 cores, per the SLURM argument provided. These caps are added during development and cannot be disabled through configuration changes in the YAML. DAG List find_peaks_index psocake_sfx_phasing pyalgos_sfx DAG Submission from the eLog You can use the script in the previous section to submit jobs through the eLog. To do so navigate to the Workflow > Definitions tab using the blue navigation bar at the top of the eLog. On this tab, in the top-right corner (underneath the help and zoom icons) you can click the + sign to add a new workflow. This will bring up a \"Workflow definition\" UI window. When filling out the eLog workflow definition the following fields are needed (all of them): Name : You can name the workflow anything you like. It should probably be something descriptive, e.g. if you are using LUTE to run smalldata_tools, you may call the workflow lute_smd . Executable : In this field you will put the full path to the submit_launch_airflow.sh script: /path/to/lute/launch_scripts/submit_launch_airflow.sh . Parameters : You will use the parameters as described above. Remember the first argument will be the full path to the launch_airflow.py script (this is NOT the same as the bash script used in the executable!): /full/path/to/lute/launch_scripts/launch_airflow.py -c <path/to/yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS Location : Be sure to set to S3DF . Trigger : You can have the workflow trigger automatically or manually. Which option to choose will depend on the type of workflow you are running. In general the options Manually triggered (which displays as MANUAL on the definitions page) and End of a run (which displays as END_OF_RUN on the definitions page) are safe options for ALL workflows. The latter will be automatically submitted for you when data acquisition has finished. If you are running a workflow with managed Task s that work as data is being acquired (e.g. SmallDataProducer ), you may also select Start of a run (which displays as START_OF_RUN on the definitions page). Upon clicking create you will see a new entry in the table on the definitions page. In order to run MANUAL workflows, or re-run automatic workflows, you must navigate to the Workflows > Control tab. For each acquisition run you will find a drop down menu under the Job column. To submit a workflow you select it from this drop down menu by the Name you provided when creating its definition. Advanced Usage Variable Substitution in YAML Files Using validator s, it is possible to define (generally, default) model parameters for a Task in terms of other parameters. It is also possible to use validated Pydantic model parameters to substitute values into a configuration file required to run a third party Task (e.g. some Task s may require their own JSON, TOML files, etc. to run properly). For more information on these types of substitutions, refer to the new_task.md documentation on Task creation. These types of substitutions, however, have a limitation in that they are not easily adapted at run time. They therefore address only a small number of the possible combinations in the dependencies between different input parameters. In order to support more complex relationships between parameters, variable substitutions can also be used in the configuration YAML itself. Using a syntax similar to Jinja templates, you can define values for YAML parameters in terms of other parameters or environment variables. The values are substituted before Pydantic attempts to validate the configuration. It is perhaps easiest to illustrate with an example. A test case is provided in config/test_var_subs.yaml and is reproduced here: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- OtherTask: useful_other_var: \"USE ME!\" NonExistentTask: test_sub: \"/path/to/{{ experiment }}/file_r{{ run:04d }}.input\" # Substitute `experiment` and `run` from header above test_env_sub: \"/path/to/{{ $EXPERIMENT }}/file.input\" # Substitute from the environment variable $EXPERIMENT test_nested: a: \"outfile_{{ run }}_one.out\" # Substitute `run` from header above b: c: \"outfile_{{ run }}_two.out\" # Also substitute `run` from header above d: \"{{ OtherTask.useful_other_var }}\" # Substitute `useful_other_var` from `OtherTask` test_fmt: \"{{ run:04d }}\" # Subsitute `run` and format as 0012 test_env_fmt: \"{{ $RUN:04d }}\" # Substitute environment variable $RUN and pad to 4 w/ zeros ... Input parameters in the config YAML can be substituted with either other input parameters or environment variables, with or without limited string formatting. All substitutions occur between double curly brackets: {{ VARIABLE_TO_SUBSTITUTE }} . Environment variables are indicated by $ in front of the variable name. Parameters from the header, i.e. the first YAML document (top section) containing the run , experiment , version fields, etc. can be substituted without any qualification. If you want to use the run parameter, you can substitute it using {{ run }} . All other parameters, i.e. from other Task s or within Task s, must use a qualified name. Nested levels are delimited using a . . E.g. consider a structure like: Task: param_set: a: 1 b: 2 c: 3 In order to use parameter c , you would use {{ Task.param_set.c }} as the substitution. Take care when using substitutions! This process will not try to guess for you. When a substitution is not available, e.g. due to misspelling, one of two things will happen: If it was an environment variable that does not exist, no substitution will be performed, although a message will be printed. I.e. you will be left with param: /my/failed/{{ $SUBSTITUTION }} as your parameter. This may or may not fail the model validation step, but is likely not what you intended. If it was an attempt at substituting another YAML parameter which does not exist, an exception will be thrown and the program will exit. Defining your own parameters The configuration file is not validated in its totality, only on a Task -by- Task basis, but it is read in its totality. E.g. when running MyTask only that portion of the configuration is validated even though the entire file has been read, and is available for substitutions. As a result, it is safe to introduce extra entries into the YAML file, as long as they are not entered under a specific Task 's configuration. This may be useful to create your own global substitutions, for example if there is a key variable that may be used across different Task s. E.g. Consider a case where you want to create a more generic configuration file where a single variable is used by multiple Task s. This single variable may be changed between experiments, for instance, but is likely static for the duration of a single set of analyses. In order to avoid a mistake when changing the configuration between experiments you can define this special variable (or variables) as a separate entry in the YAML, and make use of substitutions in each Task 's configuration. This way the variable only needs to be changed in one place. # Define our substitution. This is only for substitutiosns! MY_SPECIAL_SUB: \"EXPMT_DEPENDENT_VALUE\" # Can change here once per experiment! RunTask1: special_var: \"{{ MY_SPECIAL_SUB }}\" var_1: 1 var_2: \"a\" # ... RunTask2: special_var: \"{{ MY_SPECIAL_SUB }}\" var_3: \"abcd\" var_4: 123 # ... RunTask3: special_var: \"{{ MY_SPECIAL_SUB }}\" #... # ... and so on Gotchas! Order matters While in general you can use parameters that appear later in a YAML document to substitute for values of parameters that appear earlier, the substitutions themselves will be performed in order of appearance. It is therefore NOT possible to correctly use a later parameter as a substitution for an earlier one, if the later one itself depends on a substitution. The YAML document, however, can be rearranged without error. The order in the YAML document has no effect on execution order which is determined purely by the workflow definition. As mentioned above, the document is not validated in its entirety so rearrangements are allowed. For example consider the following situation which produces an incorrect substitution: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will incorrectly be \"{{ work_dir }}/additional_path/{{ $RUN }}\" # ... RunTaskTwo: # Remember `work_dir` and `run` come from the header document and don't need to # be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" ... This configuration can be rearranged to achieve the desired result: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskTwo: # Remember `work_dir` comes from the header document and doesn't need to be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will now be /sdf/data/lcls/ds/exp/experiment/scratch/additional_path/12 # ... ... On the otherhand, relationships such as these may point to inconsistencies in the dependencies between Task s which may warrant a refactor. Found unhashable key To avoid YAML parsing issues when using the substitution syntax, be sure to quote your substitutions. Before substitution is performed, a dictionary is first constructed by the pyyaml package which parses the document - it may fail to parse the document and raise an exception if the substitutions are not quoted. E.g. # USE THIS MyTask: var_sub: \"{{ other_var:04d }}\" # **DO NOT** USE THIS MyTask: var_sub: {{ other_var:04d }} During validation, Pydantic will by default cast variables if possible, because of this it is generally safe to use strings for substitutions. E.g. if your parameter is expecting an integer, and after substitution you pass \"2\" , Pydantic will cast this to the int 2 , and validation will succeed. As part of the substitution process limited type casting will also be handled if it is necessary for any formatting strings provided. E.g. \"{{ run:04d }}\" requires that run be an integer, so it will be treated as such in order to apply the formatting. Debug Environment Variables Special markers have been inserted at certain points in the execution flow for LUTE. These can be enabled by setting the environment variables detailed below. These are intended to allow developers to exit the program at certain points to investigate behaviour or a bug. For instance, when working on configuration parsing, an environment variable can be set which exits the program after passing this step. This allows you to run LUTE otherwise as normal (described above), without having to modify any additional code or insert your own early exits. Types of debug markers: LUTE_DEBUG_EXIT : Will exit the program at this point if the corresponding environment variable has been set. Developers can insert these markers as needed into their code to add new exit points, although as a rule of thumb they should be used sparingly, and generally only after major steps in the execution flow (e.g. after parsing, after beginning a task, after returning a result, etc.). In order to include a new marker in your code: from lute.execution.debug_utils import LUTE_DEBUG_EXIT def my_code() -> None: # ... LUTE_DEBUG_EXIT(\"MYENVVAR\", \"Additional message to print\") # If MYENVVAR is not set, the above function does nothing You can enable a marker by setting to 1, e.g. to enable the example marker above while running Tester : MYENVVAR=1 python -B run_task.py -t Tester -c config/test.yaml Currently used environment variables LUTE_DEBUG_EXIT_AT_YAML : Exits the program after reading in a YAML configuration file and performing variable substitutions, but BEFORE Pydantic validation. LUTE_DEBUG_BEFORE_TPP_EXEC : Exits the program after a ThirdPartyTask has prepared its submission command, but before exec is used to run it. Airflow Launch and DAG Execution Steps The Airflow launch process actually involves a number of steps, and is rather complicated. There are two wrapper steps prior to getting to the actual Airflow API communication. launch_scripts/submit_launch_airflow.sh is run. This script calls /sdf/group/lcls/ds/tools/lute_launcher with all the same parameters that it was called with. lute_launcher runs the launch_scripts/launch_airflow.py script which was provided as the first argument. This is the true launch script launch_airflow.py communicates with the Airflow API, requesting that a specific DAG be launched. It then continues to run, and gathers the individual logs and the exit status of each step of the DAG. Airflow will then enter a loop of communication where it asks the JID to submit each step of the requested DAG as batch job using launch_scripts/submit_slurm.sh . There are some specific reasons for this complexity: The use of submit_launch_airflow.sh as a thin-wrapper around lute_launcher is to allow the true Airflow launch script to be a long-lived job. This is for compatibility with the eLog and the ARP. When run from the eLog as a workflow, the job submission process must occur within 30 seconds due to a timeout built-in to the system. This is fine when submitting jobs to run on the batch-nodes, as the submission to the queue takes very little time. So here, submit_launch_airflow.sh serves as a thin script to have lute_launcher run as a batch job. It can then run as a long-lived job (for the duration of the entire DAG) collecting log files all in one place. This allows the log for each stage of the Airflow DAG to be inspected in a single file, and through the eLog browser interface. The use lute_launcher as a wrapper around launch_airflow.py is to manage authentication and credentials. The launch_airflow.py script requires loading credentials in order to authenticate against the Airflow API. For the average user this is not possible, unless the script is run from within the lute_launcher process.","title":"Setup"},{"location":"#setup","text":"LUTE is publically available on GitHub . In order to run it, the first step is to clone the repository: # Navigate to the directory of your choice. git clone@github.com:slac-lcls/lute The repository directory structure is as follows: lute |--- config # Configuration YAML files (see below) and templates for third party config |--- docs # Documentation (including this page) |--- launch_scripts # Entry points for using SLURM and communicating with Airflow |--- lute # Code |--- run_task.py # Script to run an individual managed Task |--- ... |--- utilities # Help utility programs |--- workflows # This directory contains workflow definitions. It is synced elsewhere and not used directly. In general, most interactions with the software will be through scripts located in the launch_scripts directory. Some users (for certain use-cases) may also choose to run the run_task.py script directly - it's location has been highlighted within hierarchy. To begin with you will need a YAML file, templates for which are available in the config directory. The structure of the YAML file and how to use the various launch scripts are described in more detail below.","title":"Setup"},{"location":"#a-note-on-utilties","text":"In the utilities directory there are two useful programs to provide assistance with using the software: utilities/dbview : LUTE stores all parameters for every analysis routine it runs (as well as results) in a database. This database is stored in the work_dir defined in the YAML file (see below). The dbview utility is a TUI application (Text-based user interface) which runs in the terminal. It allows you to navigate a LUTE database using the arrow keys, etc. Usage is: utilities/dbview -p <path/to/lute.db> . utilities/lute_help : This utility provides help and usage information for running LUTE software. E.g., it provides access to parameter descriptions to assist in properly filling out a configuration YAML. It's usage is described in slightly more detail below.","title":"A note on utilties"},{"location":"#basic-usage","text":"","title":"Basic Usage"},{"location":"#overview","text":"LUTE runs code as Task s that are managed by an Executor . The Executor provides modifications to the environment the Task runs in, as well as controls details of inter-process communication, reporting results to the eLog, etc. Combinations of specific Executor s and Task s are already provided, and are referred to as managed Task s. Managed Task s are submitted as a single unit. They can be run individually, or a series of independent steps can be submitted all at once in the form of a workflow, or directed acyclic graph ( DAG ). This latter option makes use of Airflow to manage the individual execution steps. Running analysis with LUTE is the process of submitting one or more managed Task s. This is generally a two step process. First, a configuration YAML file is prepared. This contains the parameterizations of all the Task s which you may run. Individual managed Task submission, or workflow ( DAG ) submission. These two steps are described below.","title":"Overview"},{"location":"#preparing-a-configuration-yaml","text":"All Task s are parameterized through a single configuration YAML file - even third party code which requires its own configuration files is managed through this YAML file. The basic structure is split into two documents, a brief header section which contains information that is applicable across all Task s, such as the experiment name, run numbers and the working directory, followed by per Task parameters: %YAML 1.3 --- title: \"Some title.\" experiment: \"MYEXP123\" # run: 12 # Does not need to be provided date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- TaskOne: param_a: 123 param_b: 456 param_c: sub_var: 3 sub_var2: 4 TaskTwo: new_param1: 3 new_param2: 4 # ... ... In the first document, the header, it is important that the work_dir is properly specified. This is the root directory from which Task outputs will be written, and the LUTE database will be stored. It may also be desirable to modify the task_timeout parameter which defines the time limit for individual Task jobs. By default it is set to 10 minutes, although this may not be sufficient for long running jobs. This value will be applied to all Task s so should account for the longest running job you expect. The actual analysis parameters are defined in the second document. As these vary from Task to Task , a full description will not be provided here. An actual template with real Task parameters is available in config/test.yaml . Your analysis POC can also help you set up and choose the correct Task s to include as a starting point. The template YAML file has further descriptions of what each parameter does and how to fill it out. You can also refer to the lute_help program described under the following sub-heading. Some things to consider and possible points of confusion: While we will be submitting managed Task s, the parameters are defined at the Task level. I.e. the managed Task and Task itself have different names, and the names in the YAML refer to the latter. This is because a single Task can be run using different Executor configurations, but using the same parameters. The list of managed Task s is in lute/managed_tasks.py . A table is also provided below for some routines of interest.. Managed Task The Task it Runs Task Description SmallDataProducer SubmitSMD Smalldata production CrystFELIndexer IndexCrystFEL Crystallographic indexing PartialatorMerger MergePartialator Crystallographic merging HKLComparer CompareHKL Crystallographic figures of merit HKLManipulator ManipulateHKL Crystallographic format conversions DimpleSolver DimpleSolve Crystallographic structure solution with molecular replacement PeakFinderPyAlgos FindPeaksPyAlgos Peak finding with PyAlgos algorithm. PeakFinderPsocake FindPeaksPsocake Peak finding with psocake algorithm. StreamFileConcatenator ConcatenateStreamFiles Stream file concatenation.","title":"Preparing a Configuration YAML"},{"location":"#how-do-i-know-what-parameters-are-available-and-what-they-do","text":"A summary of Task parameters is available through the lute_help program. > utilities/lute_help -t [TaskName] Note, some parameters may say \"Unknown description\" - this either means they are using an old-style defintion that does not include parameter help, or they may have some internal use. In particular you will see this for lute_config on every Task , this parameter is filled in automatically and should be ignored. E.g. as an example: > utilities/lute_help -t IndexCrystFEL INFO:__main__:Fetching parameter information for IndexCrystFEL. IndexCrystFEL ------------- Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Required Parameters: -------------------- [...] All Parameters: ------------- [...] highres (number) Mark all pixels greater than `x` has bad. profile (boolean) - Default: False Display timing data to monitor performance. temp_dir (string) Specify a path for the temp files folder. wait_for_file (integer) - Default: 0 Wait at most `x` seconds for a file to be created. A value of -1 means wait forever. no_image_data (boolean) - Default: False Load only the metadata, no iamges. Can check indexability without high data requirements. [...]","title":"How do I know what parameters are available, and what they do?"},{"location":"#running-managed-tasks-and-workflows-dags","text":"After a YAML file has been filled in you can run a Task . There are multiple ways to submit a Task , but there are 3 that are most likely: Run a single managed Task interactively by running python ... Run a single managed Task as a batch job (e.g. on S3DF) via a SLURM submission submit_slurm.sh ... Run a DAG (workflow with multiple managed Task s). These will be covered in turn below; however, in general all methods will require two parameters: the path to a configuration YAML file, and the name of the managed Task or workflow you want to run. When submitting via SLURM or submitting an entire workflow there are additional parameters to control these processes.","title":"Running Managed Tasks and Workflows (DAGs)"},{"location":"#running-single-managed-tasks-interactively","text":"The simplest submission method is just to run Python interactively. In most cases this is not practical for long-running analysis, but may be of use for short Task s or when debugging. From the root directory of the LUTE repository (or after installation) you can use the run_task.py script: > python -B [-O] run_task.py -t <ManagedTaskName> -c </path/to/config/yaml> The command-line arguments in square brackets [] are optional, while those in <> must be provided: -O is the flag controlling whether you run in debug or non-debug mode. By default, i.e. if you do NOT provide this flag you will run in debug mode which enables verbose printing. Passing -O will turn off debug to minimize output. -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML.","title":"Running single managed Tasks interactively"},{"location":"#submitting-a-single-managed-task-as-a-batch-job","text":"On S3DF you can also submit individual managed Task s to run as batch jobs. To do so use launch_scripts/submit_slurm.sh > launch_scripts/submit_slurm.sh -t <ManagedTaskName> -c </path/to/config/yaml> [--debug] $SLURM_ARGS As before command-line arguments in square brackets [] are optional, while those in <> must be provided -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. --debug is the flag to control whether or not to run in debug mode. In addition to the LUTE-specific arguments, SLURM arguments must also be provided ( $SLURM_ARGS above). You can provide as many as you want; however you will need to at least provide: --partition=<partition/queue> - The queue to run on, in general for LCLS this is milano --account=lcls:<experiment> - The account to use for batch job accounting. You will likely also want to provide at a minimum: --ntasks=<...> to control the number of cores in allocated. In general, it is best to prefer the long-form of the SLURM-argument ( --arg=<...> ) in order to avoid potential clashes with present or future LUTE arguments.","title":"Submitting a single managed Task as a batch job"},{"location":"#workflow-dag-submission","text":"Finally, you can submit a full workflow (e.g. SFX analysis, smalldata production and summary results, geometry optimization...). This can be done using a single script, submit_launch_airflow.sh , similarly to the SLURM submission above: > launch_scripts/submit_launch_airflow.sh /path/to/lute/launch_scripts/launch_airflow.py -c </path/to/yaml.yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS The submission process is slightly more complicated in this case. A more in-depth explanation is provided under \"Airflow Launch Steps\", in the advanced usage section below if interested. The parameters are as follows - as before command-line arguments in square brackets [] are optional, while those in <> must be provided: The first argument (must be first) is the full path to the launch_scripts/launch_airflow.py script located in whatever LUTE installation you are running. All other arguments can come afterwards in any order. -c </path/...> is the path to the configuration YAML to use. -w <dag_name> is the name of the DAG (workflow) to run. This replaces the task name provided when using the other two methods above. A DAG list is provided below. --debug controls whether to use debug mode (verbose printing) --test controls whether to use the test or production instance of Airflow to manage the DAG. The instances are running identical versions of Airflow, but the test instance may have \"test\" or more bleeding edge development DAGs. The $SLURM_ARGS must be provided in the same manner as when submitting an individual managed Task by hand to be run as batch job with the script above. Note that these parameters will be used as the starting point for the SLURM arguments of every managed Task in the DAG; however, individual steps in the DAG may have overrides built-in where appropriate to make sure that step is not submitted with potentially incompatible arguments. For example, a single threaded analysis Task may be capped to running on one core, even if in general everything should be running on 100 cores, per the SLURM argument provided. These caps are added during development and cannot be disabled through configuration changes in the YAML. DAG List find_peaks_index psocake_sfx_phasing pyalgos_sfx","title":"Workflow (DAG) submission"},{"location":"#dag-submission-from-the-elog","text":"You can use the script in the previous section to submit jobs through the eLog. To do so navigate to the Workflow > Definitions tab using the blue navigation bar at the top of the eLog. On this tab, in the top-right corner (underneath the help and zoom icons) you can click the + sign to add a new workflow. This will bring up a \"Workflow definition\" UI window. When filling out the eLog workflow definition the following fields are needed (all of them): Name : You can name the workflow anything you like. It should probably be something descriptive, e.g. if you are using LUTE to run smalldata_tools, you may call the workflow lute_smd . Executable : In this field you will put the full path to the submit_launch_airflow.sh script: /path/to/lute/launch_scripts/submit_launch_airflow.sh . Parameters : You will use the parameters as described above. Remember the first argument will be the full path to the launch_airflow.py script (this is NOT the same as the bash script used in the executable!): /full/path/to/lute/launch_scripts/launch_airflow.py -c <path/to/yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS Location : Be sure to set to S3DF . Trigger : You can have the workflow trigger automatically or manually. Which option to choose will depend on the type of workflow you are running. In general the options Manually triggered (which displays as MANUAL on the definitions page) and End of a run (which displays as END_OF_RUN on the definitions page) are safe options for ALL workflows. The latter will be automatically submitted for you when data acquisition has finished. If you are running a workflow with managed Task s that work as data is being acquired (e.g. SmallDataProducer ), you may also select Start of a run (which displays as START_OF_RUN on the definitions page). Upon clicking create you will see a new entry in the table on the definitions page. In order to run MANUAL workflows, or re-run automatic workflows, you must navigate to the Workflows > Control tab. For each acquisition run you will find a drop down menu under the Job column. To submit a workflow you select it from this drop down menu by the Name you provided when creating its definition.","title":"DAG Submission from the eLog"},{"location":"#advanced-usage","text":"","title":"Advanced Usage"},{"location":"#variable-substitution-in-yaml-files","text":"Using validator s, it is possible to define (generally, default) model parameters for a Task in terms of other parameters. It is also possible to use validated Pydantic model parameters to substitute values into a configuration file required to run a third party Task (e.g. some Task s may require their own JSON, TOML files, etc. to run properly). For more information on these types of substitutions, refer to the new_task.md documentation on Task creation. These types of substitutions, however, have a limitation in that they are not easily adapted at run time. They therefore address only a small number of the possible combinations in the dependencies between different input parameters. In order to support more complex relationships between parameters, variable substitutions can also be used in the configuration YAML itself. Using a syntax similar to Jinja templates, you can define values for YAML parameters in terms of other parameters or environment variables. The values are substituted before Pydantic attempts to validate the configuration. It is perhaps easiest to illustrate with an example. A test case is provided in config/test_var_subs.yaml and is reproduced here: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- OtherTask: useful_other_var: \"USE ME!\" NonExistentTask: test_sub: \"/path/to/{{ experiment }}/file_r{{ run:04d }}.input\" # Substitute `experiment` and `run` from header above test_env_sub: \"/path/to/{{ $EXPERIMENT }}/file.input\" # Substitute from the environment variable $EXPERIMENT test_nested: a: \"outfile_{{ run }}_one.out\" # Substitute `run` from header above b: c: \"outfile_{{ run }}_two.out\" # Also substitute `run` from header above d: \"{{ OtherTask.useful_other_var }}\" # Substitute `useful_other_var` from `OtherTask` test_fmt: \"{{ run:04d }}\" # Subsitute `run` and format as 0012 test_env_fmt: \"{{ $RUN:04d }}\" # Substitute environment variable $RUN and pad to 4 w/ zeros ... Input parameters in the config YAML can be substituted with either other input parameters or environment variables, with or without limited string formatting. All substitutions occur between double curly brackets: {{ VARIABLE_TO_SUBSTITUTE }} . Environment variables are indicated by $ in front of the variable name. Parameters from the header, i.e. the first YAML document (top section) containing the run , experiment , version fields, etc. can be substituted without any qualification. If you want to use the run parameter, you can substitute it using {{ run }} . All other parameters, i.e. from other Task s or within Task s, must use a qualified name. Nested levels are delimited using a . . E.g. consider a structure like: Task: param_set: a: 1 b: 2 c: 3 In order to use parameter c , you would use {{ Task.param_set.c }} as the substitution. Take care when using substitutions! This process will not try to guess for you. When a substitution is not available, e.g. due to misspelling, one of two things will happen: If it was an environment variable that does not exist, no substitution will be performed, although a message will be printed. I.e. you will be left with param: /my/failed/{{ $SUBSTITUTION }} as your parameter. This may or may not fail the model validation step, but is likely not what you intended. If it was an attempt at substituting another YAML parameter which does not exist, an exception will be thrown and the program will exit. Defining your own parameters The configuration file is not validated in its totality, only on a Task -by- Task basis, but it is read in its totality. E.g. when running MyTask only that portion of the configuration is validated even though the entire file has been read, and is available for substitutions. As a result, it is safe to introduce extra entries into the YAML file, as long as they are not entered under a specific Task 's configuration. This may be useful to create your own global substitutions, for example if there is a key variable that may be used across different Task s. E.g. Consider a case where you want to create a more generic configuration file where a single variable is used by multiple Task s. This single variable may be changed between experiments, for instance, but is likely static for the duration of a single set of analyses. In order to avoid a mistake when changing the configuration between experiments you can define this special variable (or variables) as a separate entry in the YAML, and make use of substitutions in each Task 's configuration. This way the variable only needs to be changed in one place. # Define our substitution. This is only for substitutiosns! MY_SPECIAL_SUB: \"EXPMT_DEPENDENT_VALUE\" # Can change here once per experiment! RunTask1: special_var: \"{{ MY_SPECIAL_SUB }}\" var_1: 1 var_2: \"a\" # ... RunTask2: special_var: \"{{ MY_SPECIAL_SUB }}\" var_3: \"abcd\" var_4: 123 # ... RunTask3: special_var: \"{{ MY_SPECIAL_SUB }}\" #... # ... and so on","title":"Variable Substitution in YAML Files"},{"location":"#gotchas","text":"Order matters While in general you can use parameters that appear later in a YAML document to substitute for values of parameters that appear earlier, the substitutions themselves will be performed in order of appearance. It is therefore NOT possible to correctly use a later parameter as a substitution for an earlier one, if the later one itself depends on a substitution. The YAML document, however, can be rearranged without error. The order in the YAML document has no effect on execution order which is determined purely by the workflow definition. As mentioned above, the document is not validated in its entirety so rearrangements are allowed. For example consider the following situation which produces an incorrect substitution: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will incorrectly be \"{{ work_dir }}/additional_path/{{ $RUN }}\" # ... RunTaskTwo: # Remember `work_dir` and `run` come from the header document and don't need to # be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" ... This configuration can be rearranged to achieve the desired result: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskTwo: # Remember `work_dir` comes from the header document and doesn't need to be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will now be /sdf/data/lcls/ds/exp/experiment/scratch/additional_path/12 # ... ... On the otherhand, relationships such as these may point to inconsistencies in the dependencies between Task s which may warrant a refactor. Found unhashable key To avoid YAML parsing issues when using the substitution syntax, be sure to quote your substitutions. Before substitution is performed, a dictionary is first constructed by the pyyaml package which parses the document - it may fail to parse the document and raise an exception if the substitutions are not quoted. E.g. # USE THIS MyTask: var_sub: \"{{ other_var:04d }}\" # **DO NOT** USE THIS MyTask: var_sub: {{ other_var:04d }} During validation, Pydantic will by default cast variables if possible, because of this it is generally safe to use strings for substitutions. E.g. if your parameter is expecting an integer, and after substitution you pass \"2\" , Pydantic will cast this to the int 2 , and validation will succeed. As part of the substitution process limited type casting will also be handled if it is necessary for any formatting strings provided. E.g. \"{{ run:04d }}\" requires that run be an integer, so it will be treated as such in order to apply the formatting.","title":"Gotchas!"},{"location":"#debug-environment-variables","text":"Special markers have been inserted at certain points in the execution flow for LUTE. These can be enabled by setting the environment variables detailed below. These are intended to allow developers to exit the program at certain points to investigate behaviour or a bug. For instance, when working on configuration parsing, an environment variable can be set which exits the program after passing this step. This allows you to run LUTE otherwise as normal (described above), without having to modify any additional code or insert your own early exits. Types of debug markers: LUTE_DEBUG_EXIT : Will exit the program at this point if the corresponding environment variable has been set. Developers can insert these markers as needed into their code to add new exit points, although as a rule of thumb they should be used sparingly, and generally only after major steps in the execution flow (e.g. after parsing, after beginning a task, after returning a result, etc.). In order to include a new marker in your code: from lute.execution.debug_utils import LUTE_DEBUG_EXIT def my_code() -> None: # ... LUTE_DEBUG_EXIT(\"MYENVVAR\", \"Additional message to print\") # If MYENVVAR is not set, the above function does nothing You can enable a marker by setting to 1, e.g. to enable the example marker above while running Tester : MYENVVAR=1 python -B run_task.py -t Tester -c config/test.yaml","title":"Debug Environment Variables"},{"location":"#currently-used-environment-variables","text":"LUTE_DEBUG_EXIT_AT_YAML : Exits the program after reading in a YAML configuration file and performing variable substitutions, but BEFORE Pydantic validation. LUTE_DEBUG_BEFORE_TPP_EXEC : Exits the program after a ThirdPartyTask has prepared its submission command, but before exec is used to run it.","title":"Currently used environment variables"},{"location":"#airflow-launch-and-dag-execution-steps","text":"The Airflow launch process actually involves a number of steps, and is rather complicated. There are two wrapper steps prior to getting to the actual Airflow API communication. launch_scripts/submit_launch_airflow.sh is run. This script calls /sdf/group/lcls/ds/tools/lute_launcher with all the same parameters that it was called with. lute_launcher runs the launch_scripts/launch_airflow.py script which was provided as the first argument. This is the true launch script launch_airflow.py communicates with the Airflow API, requesting that a specific DAG be launched. It then continues to run, and gathers the individual logs and the exit status of each step of the DAG. Airflow will then enter a loop of communication where it asks the JID to submit each step of the requested DAG as batch job using launch_scripts/submit_slurm.sh . There are some specific reasons for this complexity: The use of submit_launch_airflow.sh as a thin-wrapper around lute_launcher is to allow the true Airflow launch script to be a long-lived job. This is for compatibility with the eLog and the ARP. When run from the eLog as a workflow, the job submission process must occur within 30 seconds due to a timeout built-in to the system. This is fine when submitting jobs to run on the batch-nodes, as the submission to the queue takes very little time. So here, submit_launch_airflow.sh serves as a thin script to have lute_launcher run as a batch job. It can then run as a long-lived job (for the duration of the entire DAG) collecting log files all in one place. This allows the log for each stage of the Airflow DAG to be inspected in a single file, and through the eLog browser interface. The use lute_launcher as a wrapper around launch_airflow.py is to manage authentication and credentials. The launch_airflow.py script requires loading credentials in order to authenticate against the Airflow API. For the average user this is not possible, unless the script is run from within the lute_launcher process.","title":"Airflow Launch and DAG Execution Steps"},{"location":"usage/","text":"Setup LUTE is publically available on GitHub . In order to run it, the first step is to clone the repository: # Navigate to the directory of your choice. git clone@github.com:slac-lcls/lute The repository directory structure is as follows: lute |--- config # Configuration YAML files (see below) and templates for third party config |--- docs # Documentation (including this page) |--- launch_scripts # Entry points for using SLURM and communicating with Airflow |--- lute # Code |--- run_task.py # Script to run an individual managed Task |--- ... |--- utilities # Help utility programs |--- workflows # This directory contains workflow definitions. It is synced elsewhere and not used directly. In general, most interactions with the software will be through scripts located in the launch_scripts directory. Some users (for certain use-cases) may also choose to run the run_task.py script directly - it's location has been highlighted within hierarchy. To begin with you will need a YAML file, templates for which are available in the config directory. The structure of the YAML file and how to use the various launch scripts are described in more detail below. A note on utilties In the utilities directory there are two useful programs to provide assistance with using the software: utilities/dbview : LUTE stores all parameters for every analysis routine it runs (as well as results) in a database. This database is stored in the work_dir defined in the YAML file (see below). The dbview utility is a TUI application (Text-based user interface) which runs in the terminal. It allows you to navigate a LUTE database using the arrow keys, etc. Usage is: utilities/dbview -p <path/to/lute.db> . utilities/lute_help : This utility provides help and usage information for running LUTE software. E.g., it provides access to parameter descriptions to assist in properly filling out a configuration YAML. It's usage is described in slightly more detail below. Basic Usage Overview LUTE runs code as Task s that are managed by an Executor . The Executor provides modifications to the environment the Task runs in, as well as controls details of inter-process communication, reporting results to the eLog, etc. Combinations of specific Executor s and Task s are already provided, and are referred to as managed Task s. Managed Task s are submitted as a single unit. They can be run individually, or a series of independent steps can be submitted all at once in the form of a workflow, or directed acyclic graph ( DAG ). This latter option makes use of Airflow to manage the individual execution steps. Running analysis with LUTE is the process of submitting one or more managed Task s. This is generally a two step process. First, a configuration YAML file is prepared. This contains the parameterizations of all the Task s which you may run. Individual managed Task submission, or workflow ( DAG ) submission. These two steps are described below. Preparing a Configuration YAML All Task s are parameterized through a single configuration YAML file - even third party code which requires its own configuration files is managed through this YAML file. The basic structure is split into two documents, a brief header section which contains information that is applicable across all Task s, such as the experiment name, run numbers and the working directory, followed by per Task parameters: %YAML 1.3 --- title: \"Some title.\" experiment: \"MYEXP123\" # run: 12 # Does not need to be provided date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- TaskOne: param_a: 123 param_b: 456 param_c: sub_var: 3 sub_var2: 4 TaskTwo: new_param1: 3 new_param2: 4 # ... ... In the first document, the header, it is important that the work_dir is properly specified. This is the root directory from which Task outputs will be written, and the LUTE database will be stored. It may also be desirable to modify the task_timeout parameter which defines the time limit for individual Task jobs. By default it is set to 10 minutes, although this may not be sufficient for long running jobs. This value will be applied to all Task s so should account for the longest running job you expect. The actual analysis parameters are defined in the second document. As these vary from Task to Task , a full description will not be provided here. An actual template with real Task parameters is available in config/test.yaml . Your analysis POC can also help you set up and choose the correct Task s to include as a starting point. The template YAML file has further descriptions of what each parameter does and how to fill it out. You can also refer to the lute_help program described under the following sub-heading. Some things to consider and possible points of confusion: While we will be submitting managed Task s, the parameters are defined at the Task level. I.e. the managed Task and Task itself have different names, and the names in the YAML refer to the latter. This is because a single Task can be run using different Executor configurations, but using the same parameters. The list of managed Task s is in lute/managed_tasks.py . A table is also provided below for some routines of interest.. Managed Task The Task it Runs Task Description SmallDataProducer SubmitSMD Smalldata production CrystFELIndexer IndexCrystFEL Crystallographic indexing PartialatorMerger MergePartialator Crystallographic merging HKLComparer CompareHKL Crystallographic figures of merit HKLManipulator ManipulateHKL Crystallographic format conversions DimpleSolver DimpleSolve Crystallographic structure solution with molecular replacement PeakFinderPyAlgos FindPeaksPyAlgos Peak finding with PyAlgos algorithm. PeakFinderPsocake FindPeaksPsocake Peak finding with psocake algorithm. StreamFileConcatenator ConcatenateStreamFiles Stream file concatenation. How do I know what parameters are available, and what they do? A summary of Task parameters is available through the lute_help program. > utilities/lute_help -t [TaskName] Note, some parameters may say \"Unknown description\" - this either means they are using an old-style defintion that does not include parameter help, or they may have some internal use. In particular you will see this for lute_config on every Task , this parameter is filled in automatically and should be ignored. E.g. as an example: > utilities/lute_help -t IndexCrystFEL INFO:__main__:Fetching parameter information for IndexCrystFEL. IndexCrystFEL ------------- Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Required Parameters: -------------------- [...] All Parameters: ------------- [...] highres (number) Mark all pixels greater than `x` has bad. profile (boolean) - Default: False Display timing data to monitor performance. temp_dir (string) Specify a path for the temp files folder. wait_for_file (integer) - Default: 0 Wait at most `x` seconds for a file to be created. A value of -1 means wait forever. no_image_data (boolean) - Default: False Load only the metadata, no iamges. Can check indexability without high data requirements. [...] Running Managed Task s and Workflows (DAGs) After a YAML file has been filled in you can run a Task . There are multiple ways to submit a Task , but there are 3 that are most likely: Run a single managed Task interactively by running python ... Run a single managed Task as a batch job (e.g. on S3DF) via a SLURM submission submit_slurm.sh ... Run a DAG (workflow with multiple managed Task s). These will be covered in turn below; however, in general all methods will require two parameters: the path to a configuration YAML file, and the name of the managed Task or workflow you want to run. When submitting via SLURM or submitting an entire workflow there are additional parameters to control these processes. Running single managed Task s interactively The simplest submission method is just to run Python interactively. In most cases this is not practical for long-running analysis, but may be of use for short Task s or when debugging. From the root directory of the LUTE repository (or after installation) you can use the run_task.py script: > python -B [-O] run_task.py -t <ManagedTaskName> -c </path/to/config/yaml> The command-line arguments in square brackets [] are optional, while those in <> must be provided: -O is the flag controlling whether you run in debug or non-debug mode. By default, i.e. if you do NOT provide this flag you will run in debug mode which enables verbose printing. Passing -O will turn off debug to minimize output. -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. Submitting a single managed Task as a batch job On S3DF you can also submit individual managed Task s to run as batch jobs. To do so use launch_scripts/submit_slurm.sh > launch_scripts/submit_slurm.sh -t <ManagedTaskName> -c </path/to/config/yaml> [--debug] $SLURM_ARGS As before command-line arguments in square brackets [] are optional, while those in <> must be provided -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. --debug is the flag to control whether or not to run in debug mode. In addition to the LUTE-specific arguments, SLURM arguments must also be provided ( $SLURM_ARGS above). You can provide as many as you want; however you will need to at least provide: --partition=<partition/queue> - The queue to run on, in general for LCLS this is milano --account=lcls:<experiment> - The account to use for batch job accounting. You will likely also want to provide at a minimum: --ntasks=<...> to control the number of cores in allocated. In general, it is best to prefer the long-form of the SLURM-argument ( --arg=<...> ) in order to avoid potential clashes with present or future LUTE arguments. Workflow (DAG) submission Finally, you can submit a full workflow (e.g. SFX analysis, smalldata production and summary results, geometry optimization...). This can be done using a single script, submit_launch_airflow.sh , similarly to the SLURM submission above: > launch_scripts/submit_launch_airflow.sh /path/to/lute/launch_scripts/launch_airflow.py -c </path/to/yaml.yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS The submission process is slightly more complicated in this case. A more in-depth explanation is provided under \"Airflow Launch Steps\", in the advanced usage section below if interested. The parameters are as follows - as before command-line arguments in square brackets [] are optional, while those in <> must be provided: The first argument (must be first) is the full path to the launch_scripts/launch_airflow.py script located in whatever LUTE installation you are running. All other arguments can come afterwards in any order. -c </path/...> is the path to the configuration YAML to use. -w <dag_name> is the name of the DAG (workflow) to run. This replaces the task name provided when using the other two methods above. A DAG list is provided below. --debug controls whether to use debug mode (verbose printing) --test controls whether to use the test or production instance of Airflow to manage the DAG. The instances are running identical versions of Airflow, but the test instance may have \"test\" or more bleeding edge development DAGs. The $SLURM_ARGS must be provided in the same manner as when submitting an individual managed Task by hand to be run as batch job with the script above. Note that these parameters will be used as the starting point for the SLURM arguments of every managed Task in the DAG; however, individual steps in the DAG may have overrides built-in where appropriate to make sure that step is not submitted with potentially incompatible arguments. For example, a single threaded analysis Task may be capped to running on one core, even if in general everything should be running on 100 cores, per the SLURM argument provided. These caps are added during development and cannot be disabled through configuration changes in the YAML. DAG List find_peaks_index psocake_sfx_phasing pyalgos_sfx DAG Submission from the eLog You can use the script in the previous section to submit jobs through the eLog. To do so navigate to the Workflow > Definitions tab using the blue navigation bar at the top of the eLog. On this tab, in the top-right corner (underneath the help and zoom icons) you can click the + sign to add a new workflow. This will bring up a \"Workflow definition\" UI window. When filling out the eLog workflow definition the following fields are needed (all of them): Name : You can name the workflow anything you like. It should probably be something descriptive, e.g. if you are using LUTE to run smalldata_tools, you may call the workflow lute_smd . Executable : In this field you will put the full path to the submit_launch_airflow.sh script: /path/to/lute/launch_scripts/submit_launch_airflow.sh . Parameters : You will use the parameters as described above. Remember the first argument will be the full path to the launch_airflow.py script (this is NOT the same as the bash script used in the executable!): /full/path/to/lute/launch_scripts/launch_airflow.py -c <path/to/yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS Location : Be sure to set to S3DF . Trigger : You can have the workflow trigger automatically or manually. Which option to choose will depend on the type of workflow you are running. In general the options Manually triggered (which displays as MANUAL on the definitions page) and End of a run (which displays as END_OF_RUN on the definitions page) are safe options for ALL workflows. The latter will be automatically submitted for you when data acquisition has finished. If you are running a workflow with managed Task s that work as data is being acquired (e.g. SmallDataProducer ), you may also select Start of a run (which displays as START_OF_RUN on the definitions page). Upon clicking create you will see a new entry in the table on the definitions page. In order to run MANUAL workflows, or re-run automatic workflows, you must navigate to the Workflows > Control tab. For each acquisition run you will find a drop down menu under the Job column. To submit a workflow you select it from this drop down menu by the Name you provided when creating its definition. Advanced Usage Variable Substitution in YAML Files Using validator s, it is possible to define (generally, default) model parameters for a Task in terms of other parameters. It is also possible to use validated Pydantic model parameters to substitute values into a configuration file required to run a third party Task (e.g. some Task s may require their own JSON, TOML files, etc. to run properly). For more information on these types of substitutions, refer to the new_task.md documentation on Task creation. These types of substitutions, however, have a limitation in that they are not easily adapted at run time. They therefore address only a small number of the possible combinations in the dependencies between different input parameters. In order to support more complex relationships between parameters, variable substitutions can also be used in the configuration YAML itself. Using a syntax similar to Jinja templates, you can define values for YAML parameters in terms of other parameters or environment variables. The values are substituted before Pydantic attempts to validate the configuration. It is perhaps easiest to illustrate with an example. A test case is provided in config/test_var_subs.yaml and is reproduced here: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- OtherTask: useful_other_var: \"USE ME!\" NonExistentTask: test_sub: \"/path/to/{{ experiment }}/file_r{{ run:04d }}.input\" # Substitute `experiment` and `run` from header above test_env_sub: \"/path/to/{{ $EXPERIMENT }}/file.input\" # Substitute from the environment variable $EXPERIMENT test_nested: a: \"outfile_{{ run }}_one.out\" # Substitute `run` from header above b: c: \"outfile_{{ run }}_two.out\" # Also substitute `run` from header above d: \"{{ OtherTask.useful_other_var }}\" # Substitute `useful_other_var` from `OtherTask` test_fmt: \"{{ run:04d }}\" # Subsitute `run` and format as 0012 test_env_fmt: \"{{ $RUN:04d }}\" # Substitute environment variable $RUN and pad to 4 w/ zeros ... Input parameters in the config YAML can be substituted with either other input parameters or environment variables, with or without limited string formatting. All substitutions occur between double curly brackets: {{ VARIABLE_TO_SUBSTITUTE }} . Environment variables are indicated by $ in front of the variable name. Parameters from the header, i.e. the first YAML document (top section) containing the run , experiment , version fields, etc. can be substituted without any qualification. If you want to use the run parameter, you can substitute it using {{ run }} . All other parameters, i.e. from other Task s or within Task s, must use a qualified name. Nested levels are delimited using a . . E.g. consider a structure like: Task: param_set: a: 1 b: 2 c: 3 In order to use parameter c , you would use {{ Task.param_set.c }} as the substitution. Take care when using substitutions! This process will not try to guess for you. When a substitution is not available, e.g. due to misspelling, one of two things will happen: If it was an environment variable that does not exist, no substitution will be performed, although a message will be printed. I.e. you will be left with param: /my/failed/{{ $SUBSTITUTION }} as your parameter. This may or may not fail the model validation step, but is likely not what you intended. If it was an attempt at substituting another YAML parameter which does not exist, an exception will be thrown and the program will exit. Defining your own parameters The configuration file is not validated in its totality, only on a Task -by- Task basis, but it is read in its totality. E.g. when running MyTask only that portion of the configuration is validated even though the entire file has been read, and is available for substitutions. As a result, it is safe to introduce extra entries into the YAML file, as long as they are not entered under a specific Task 's configuration. This may be useful to create your own global substitutions, for example if there is a key variable that may be used across different Task s. E.g. Consider a case where you want to create a more generic configuration file where a single variable is used by multiple Task s. This single variable may be changed between experiments, for instance, but is likely static for the duration of a single set of analyses. In order to avoid a mistake when changing the configuration between experiments you can define this special variable (or variables) as a separate entry in the YAML, and make use of substitutions in each Task 's configuration. This way the variable only needs to be changed in one place. # Define our substitution. This is only for substitutiosns! MY_SPECIAL_SUB: \"EXPMT_DEPENDENT_VALUE\" # Can change here once per experiment! RunTask1: special_var: \"{{ MY_SPECIAL_SUB }}\" var_1: 1 var_2: \"a\" # ... RunTask2: special_var: \"{{ MY_SPECIAL_SUB }}\" var_3: \"abcd\" var_4: 123 # ... RunTask3: special_var: \"{{ MY_SPECIAL_SUB }}\" #... # ... and so on Gotchas! Order matters While in general you can use parameters that appear later in a YAML document to substitute for values of parameters that appear earlier, the substitutions themselves will be performed in order of appearance. It is therefore NOT possible to correctly use a later parameter as a substitution for an earlier one, if the later one itself depends on a substitution. The YAML document, however, can be rearranged without error. The order in the YAML document has no effect on execution order which is determined purely by the workflow definition. As mentioned above, the document is not validated in its entirety so rearrangements are allowed. For example consider the following situation which produces an incorrect substitution: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will incorrectly be \"{{ work_dir }}/additional_path/{{ $RUN }}\" # ... RunTaskTwo: # Remember `work_dir` and `run` come from the header document and don't need to # be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" ... This configuration can be rearranged to achieve the desired result: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskTwo: # Remember `work_dir` comes from the header document and doesn't need to be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will now be /sdf/data/lcls/ds/exp/experiment/scratch/additional_path/12 # ... ... On the otherhand, relationships such as these may point to inconsistencies in the dependencies between Task s which may warrant a refactor. Found unhashable key To avoid YAML parsing issues when using the substitution syntax, be sure to quote your substitutions. Before substitution is performed, a dictionary is first constructed by the pyyaml package which parses the document - it may fail to parse the document and raise an exception if the substitutions are not quoted. E.g. # USE THIS MyTask: var_sub: \"{{ other_var:04d }}\" # **DO NOT** USE THIS MyTask: var_sub: {{ other_var:04d }} During validation, Pydantic will by default cast variables if possible, because of this it is generally safe to use strings for substitutions. E.g. if your parameter is expecting an integer, and after substitution you pass \"2\" , Pydantic will cast this to the int 2 , and validation will succeed. As part of the substitution process limited type casting will also be handled if it is necessary for any formatting strings provided. E.g. \"{{ run:04d }}\" requires that run be an integer, so it will be treated as such in order to apply the formatting. Debug Environment Variables Special markers have been inserted at certain points in the execution flow for LUTE. These can be enabled by setting the environment variables detailed below. These are intended to allow developers to exit the program at certain points to investigate behaviour or a bug. For instance, when working on configuration parsing, an environment variable can be set which exits the program after passing this step. This allows you to run LUTE otherwise as normal (described above), without having to modify any additional code or insert your own early exits. Types of debug markers: LUTE_DEBUG_EXIT : Will exit the program at this point if the corresponding environment variable has been set. Developers can insert these markers as needed into their code to add new exit points, although as a rule of thumb they should be used sparingly, and generally only after major steps in the execution flow (e.g. after parsing, after beginning a task, after returning a result, etc.). In order to include a new marker in your code: from lute.execution.debug_utils import LUTE_DEBUG_EXIT def my_code() -> None: # ... LUTE_DEBUG_EXIT(\"MYENVVAR\", \"Additional message to print\") # If MYENVVAR is not set, the above function does nothing You can enable a marker by setting to 1, e.g. to enable the example marker above while running Tester : MYENVVAR=1 python -B run_task.py -t Tester -c config/test.yaml Currently used environment variables LUTE_DEBUG_EXIT_AT_YAML : Exits the program after reading in a YAML configuration file and performing variable substitutions, but BEFORE Pydantic validation. LUTE_DEBUG_BEFORE_TPP_EXEC : Exits the program after a ThirdPartyTask has prepared its submission command, but before exec is used to run it. Airflow Launch and DAG Execution Steps The Airflow launch process actually involves a number of steps, and is rather complicated. There are two wrapper steps prior to getting to the actual Airflow API communication. launch_scripts/submit_launch_airflow.sh is run. This script calls /sdf/group/lcls/ds/tools/lute_launcher with all the same parameters that it was called with. lute_launcher runs the launch_scripts/launch_airflow.py script which was provided as the first argument. This is the true launch script launch_airflow.py communicates with the Airflow API, requesting that a specific DAG be launched. It then continues to run, and gathers the individual logs and the exit status of each step of the DAG. Airflow will then enter a loop of communication where it asks the JID to submit each step of the requested DAG as batch job using launch_scripts/submit_slurm.sh . There are some specific reasons for this complexity: The use of submit_launch_airflow.sh as a thin-wrapper around lute_launcher is to allow the true Airflow launch script to be a long-lived job. This is for compatibility with the eLog and the ARP. When run from the eLog as a workflow, the job submission process must occur within 30 seconds due to a timeout built-in to the system. This is fine when submitting jobs to run on the batch-nodes, as the submission to the queue takes very little time. So here, submit_launch_airflow.sh serves as a thin script to have lute_launcher run as a batch job. It can then run as a long-lived job (for the duration of the entire DAG) collecting log files all in one place. This allows the log for each stage of the Airflow DAG to be inspected in a single file, and through the eLog browser interface. The use lute_launcher as a wrapper around launch_airflow.py is to manage authentication and credentials. The launch_airflow.py script requires loading credentials in order to authenticate against the Airflow API. For the average user this is not possible, unless the script is run from within the lute_launcher process.","title":"Quick Start"},{"location":"usage/#setup","text":"LUTE is publically available on GitHub . In order to run it, the first step is to clone the repository: # Navigate to the directory of your choice. git clone@github.com:slac-lcls/lute The repository directory structure is as follows: lute |--- config # Configuration YAML files (see below) and templates for third party config |--- docs # Documentation (including this page) |--- launch_scripts # Entry points for using SLURM and communicating with Airflow |--- lute # Code |--- run_task.py # Script to run an individual managed Task |--- ... |--- utilities # Help utility programs |--- workflows # This directory contains workflow definitions. It is synced elsewhere and not used directly. In general, most interactions with the software will be through scripts located in the launch_scripts directory. Some users (for certain use-cases) may also choose to run the run_task.py script directly - it's location has been highlighted within hierarchy. To begin with you will need a YAML file, templates for which are available in the config directory. The structure of the YAML file and how to use the various launch scripts are described in more detail below.","title":"Setup"},{"location":"usage/#a-note-on-utilties","text":"In the utilities directory there are two useful programs to provide assistance with using the software: utilities/dbview : LUTE stores all parameters for every analysis routine it runs (as well as results) in a database. This database is stored in the work_dir defined in the YAML file (see below). The dbview utility is a TUI application (Text-based user interface) which runs in the terminal. It allows you to navigate a LUTE database using the arrow keys, etc. Usage is: utilities/dbview -p <path/to/lute.db> . utilities/lute_help : This utility provides help and usage information for running LUTE software. E.g., it provides access to parameter descriptions to assist in properly filling out a configuration YAML. It's usage is described in slightly more detail below.","title":"A note on utilties"},{"location":"usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"usage/#overview","text":"LUTE runs code as Task s that are managed by an Executor . The Executor provides modifications to the environment the Task runs in, as well as controls details of inter-process communication, reporting results to the eLog, etc. Combinations of specific Executor s and Task s are already provided, and are referred to as managed Task s. Managed Task s are submitted as a single unit. They can be run individually, or a series of independent steps can be submitted all at once in the form of a workflow, or directed acyclic graph ( DAG ). This latter option makes use of Airflow to manage the individual execution steps. Running analysis with LUTE is the process of submitting one or more managed Task s. This is generally a two step process. First, a configuration YAML file is prepared. This contains the parameterizations of all the Task s which you may run. Individual managed Task submission, or workflow ( DAG ) submission. These two steps are described below.","title":"Overview"},{"location":"usage/#preparing-a-configuration-yaml","text":"All Task s are parameterized through a single configuration YAML file - even third party code which requires its own configuration files is managed through this YAML file. The basic structure is split into two documents, a brief header section which contains information that is applicable across all Task s, such as the experiment name, run numbers and the working directory, followed by per Task parameters: %YAML 1.3 --- title: \"Some title.\" experiment: \"MYEXP123\" # run: 12 # Does not need to be provided date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- TaskOne: param_a: 123 param_b: 456 param_c: sub_var: 3 sub_var2: 4 TaskTwo: new_param1: 3 new_param2: 4 # ... ... In the first document, the header, it is important that the work_dir is properly specified. This is the root directory from which Task outputs will be written, and the LUTE database will be stored. It may also be desirable to modify the task_timeout parameter which defines the time limit for individual Task jobs. By default it is set to 10 minutes, although this may not be sufficient for long running jobs. This value will be applied to all Task s so should account for the longest running job you expect. The actual analysis parameters are defined in the second document. As these vary from Task to Task , a full description will not be provided here. An actual template with real Task parameters is available in config/test.yaml . Your analysis POC can also help you set up and choose the correct Task s to include as a starting point. The template YAML file has further descriptions of what each parameter does and how to fill it out. You can also refer to the lute_help program described under the following sub-heading. Some things to consider and possible points of confusion: While we will be submitting managed Task s, the parameters are defined at the Task level. I.e. the managed Task and Task itself have different names, and the names in the YAML refer to the latter. This is because a single Task can be run using different Executor configurations, but using the same parameters. The list of managed Task s is in lute/managed_tasks.py . A table is also provided below for some routines of interest.. Managed Task The Task it Runs Task Description SmallDataProducer SubmitSMD Smalldata production CrystFELIndexer IndexCrystFEL Crystallographic indexing PartialatorMerger MergePartialator Crystallographic merging HKLComparer CompareHKL Crystallographic figures of merit HKLManipulator ManipulateHKL Crystallographic format conversions DimpleSolver DimpleSolve Crystallographic structure solution with molecular replacement PeakFinderPyAlgos FindPeaksPyAlgos Peak finding with PyAlgos algorithm. PeakFinderPsocake FindPeaksPsocake Peak finding with psocake algorithm. StreamFileConcatenator ConcatenateStreamFiles Stream file concatenation.","title":"Preparing a Configuration YAML"},{"location":"usage/#how-do-i-know-what-parameters-are-available-and-what-they-do","text":"A summary of Task parameters is available through the lute_help program. > utilities/lute_help -t [TaskName] Note, some parameters may say \"Unknown description\" - this either means they are using an old-style defintion that does not include parameter help, or they may have some internal use. In particular you will see this for lute_config on every Task , this parameter is filled in automatically and should be ignored. E.g. as an example: > utilities/lute_help -t IndexCrystFEL INFO:__main__:Fetching parameter information for IndexCrystFEL. IndexCrystFEL ------------- Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Required Parameters: -------------------- [...] All Parameters: ------------- [...] highres (number) Mark all pixels greater than `x` has bad. profile (boolean) - Default: False Display timing data to monitor performance. temp_dir (string) Specify a path for the temp files folder. wait_for_file (integer) - Default: 0 Wait at most `x` seconds for a file to be created. A value of -1 means wait forever. no_image_data (boolean) - Default: False Load only the metadata, no iamges. Can check indexability without high data requirements. [...]","title":"How do I know what parameters are available, and what they do?"},{"location":"usage/#running-managed-tasks-and-workflows-dags","text":"After a YAML file has been filled in you can run a Task . There are multiple ways to submit a Task , but there are 3 that are most likely: Run a single managed Task interactively by running python ... Run a single managed Task as a batch job (e.g. on S3DF) via a SLURM submission submit_slurm.sh ... Run a DAG (workflow with multiple managed Task s). These will be covered in turn below; however, in general all methods will require two parameters: the path to a configuration YAML file, and the name of the managed Task or workflow you want to run. When submitting via SLURM or submitting an entire workflow there are additional parameters to control these processes.","title":"Running Managed Tasks and Workflows (DAGs)"},{"location":"usage/#running-single-managed-tasks-interactively","text":"The simplest submission method is just to run Python interactively. In most cases this is not practical for long-running analysis, but may be of use for short Task s or when debugging. From the root directory of the LUTE repository (or after installation) you can use the run_task.py script: > python -B [-O] run_task.py -t <ManagedTaskName> -c </path/to/config/yaml> The command-line arguments in square brackets [] are optional, while those in <> must be provided: -O is the flag controlling whether you run in debug or non-debug mode. By default, i.e. if you do NOT provide this flag you will run in debug mode which enables verbose printing. Passing -O will turn off debug to minimize output. -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML.","title":"Running single managed Tasks interactively"},{"location":"usage/#submitting-a-single-managed-task-as-a-batch-job","text":"On S3DF you can also submit individual managed Task s to run as batch jobs. To do so use launch_scripts/submit_slurm.sh > launch_scripts/submit_slurm.sh -t <ManagedTaskName> -c </path/to/config/yaml> [--debug] $SLURM_ARGS As before command-line arguments in square brackets [] are optional, while those in <> must be provided -t <ManagedTaskName> is the name of the managed Task you want to run. -c </path/...> is the path to the configuration YAML. --debug is the flag to control whether or not to run in debug mode. In addition to the LUTE-specific arguments, SLURM arguments must also be provided ( $SLURM_ARGS above). You can provide as many as you want; however you will need to at least provide: --partition=<partition/queue> - The queue to run on, in general for LCLS this is milano --account=lcls:<experiment> - The account to use for batch job accounting. You will likely also want to provide at a minimum: --ntasks=<...> to control the number of cores in allocated. In general, it is best to prefer the long-form of the SLURM-argument ( --arg=<...> ) in order to avoid potential clashes with present or future LUTE arguments.","title":"Submitting a single managed Task as a batch job"},{"location":"usage/#workflow-dag-submission","text":"Finally, you can submit a full workflow (e.g. SFX analysis, smalldata production and summary results, geometry optimization...). This can be done using a single script, submit_launch_airflow.sh , similarly to the SLURM submission above: > launch_scripts/submit_launch_airflow.sh /path/to/lute/launch_scripts/launch_airflow.py -c </path/to/yaml.yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS The submission process is slightly more complicated in this case. A more in-depth explanation is provided under \"Airflow Launch Steps\", in the advanced usage section below if interested. The parameters are as follows - as before command-line arguments in square brackets [] are optional, while those in <> must be provided: The first argument (must be first) is the full path to the launch_scripts/launch_airflow.py script located in whatever LUTE installation you are running. All other arguments can come afterwards in any order. -c </path/...> is the path to the configuration YAML to use. -w <dag_name> is the name of the DAG (workflow) to run. This replaces the task name provided when using the other two methods above. A DAG list is provided below. --debug controls whether to use debug mode (verbose printing) --test controls whether to use the test or production instance of Airflow to manage the DAG. The instances are running identical versions of Airflow, but the test instance may have \"test\" or more bleeding edge development DAGs. The $SLURM_ARGS must be provided in the same manner as when submitting an individual managed Task by hand to be run as batch job with the script above. Note that these parameters will be used as the starting point for the SLURM arguments of every managed Task in the DAG; however, individual steps in the DAG may have overrides built-in where appropriate to make sure that step is not submitted with potentially incompatible arguments. For example, a single threaded analysis Task may be capped to running on one core, even if in general everything should be running on 100 cores, per the SLURM argument provided. These caps are added during development and cannot be disabled through configuration changes in the YAML. DAG List find_peaks_index psocake_sfx_phasing pyalgos_sfx","title":"Workflow (DAG) submission"},{"location":"usage/#dag-submission-from-the-elog","text":"You can use the script in the previous section to submit jobs through the eLog. To do so navigate to the Workflow > Definitions tab using the blue navigation bar at the top of the eLog. On this tab, in the top-right corner (underneath the help and zoom icons) you can click the + sign to add a new workflow. This will bring up a \"Workflow definition\" UI window. When filling out the eLog workflow definition the following fields are needed (all of them): Name : You can name the workflow anything you like. It should probably be something descriptive, e.g. if you are using LUTE to run smalldata_tools, you may call the workflow lute_smd . Executable : In this field you will put the full path to the submit_launch_airflow.sh script: /path/to/lute/launch_scripts/submit_launch_airflow.sh . Parameters : You will use the parameters as described above. Remember the first argument will be the full path to the launch_airflow.py script (this is NOT the same as the bash script used in the executable!): /full/path/to/lute/launch_scripts/launch_airflow.py -c <path/to/yaml> -w <dag_name> [--debug] [--test] $SLURM_ARGS Location : Be sure to set to S3DF . Trigger : You can have the workflow trigger automatically or manually. Which option to choose will depend on the type of workflow you are running. In general the options Manually triggered (which displays as MANUAL on the definitions page) and End of a run (which displays as END_OF_RUN on the definitions page) are safe options for ALL workflows. The latter will be automatically submitted for you when data acquisition has finished. If you are running a workflow with managed Task s that work as data is being acquired (e.g. SmallDataProducer ), you may also select Start of a run (which displays as START_OF_RUN on the definitions page). Upon clicking create you will see a new entry in the table on the definitions page. In order to run MANUAL workflows, or re-run automatic workflows, you must navigate to the Workflows > Control tab. For each acquisition run you will find a drop down menu under the Job column. To submit a workflow you select it from this drop down menu by the Name you provided when creating its definition.","title":"DAG Submission from the eLog"},{"location":"usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"usage/#variable-substitution-in-yaml-files","text":"Using validator s, it is possible to define (generally, default) model parameters for a Task in terms of other parameters. It is also possible to use validated Pydantic model parameters to substitute values into a configuration file required to run a third party Task (e.g. some Task s may require their own JSON, TOML files, etc. to run properly). For more information on these types of substitutions, refer to the new_task.md documentation on Task creation. These types of substitutions, however, have a limitation in that they are not easily adapted at run time. They therefore address only a small number of the possible combinations in the dependencies between different input parameters. In order to support more complex relationships between parameters, variable substitutions can also be used in the configuration YAML itself. Using a syntax similar to Jinja templates, you can define values for YAML parameters in terms of other parameters or environment variables. The values are substituted before Pydantic attempts to validate the configuration. It is perhaps easiest to illustrate with an example. A test case is provided in config/test_var_subs.yaml and is reproduced here: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/scratch/users/d/dorlhiac\" ... --- OtherTask: useful_other_var: \"USE ME!\" NonExistentTask: test_sub: \"/path/to/{{ experiment }}/file_r{{ run:04d }}.input\" # Substitute `experiment` and `run` from header above test_env_sub: \"/path/to/{{ $EXPERIMENT }}/file.input\" # Substitute from the environment variable $EXPERIMENT test_nested: a: \"outfile_{{ run }}_one.out\" # Substitute `run` from header above b: c: \"outfile_{{ run }}_two.out\" # Also substitute `run` from header above d: \"{{ OtherTask.useful_other_var }}\" # Substitute `useful_other_var` from `OtherTask` test_fmt: \"{{ run:04d }}\" # Subsitute `run` and format as 0012 test_env_fmt: \"{{ $RUN:04d }}\" # Substitute environment variable $RUN and pad to 4 w/ zeros ... Input parameters in the config YAML can be substituted with either other input parameters or environment variables, with or without limited string formatting. All substitutions occur between double curly brackets: {{ VARIABLE_TO_SUBSTITUTE }} . Environment variables are indicated by $ in front of the variable name. Parameters from the header, i.e. the first YAML document (top section) containing the run , experiment , version fields, etc. can be substituted without any qualification. If you want to use the run parameter, you can substitute it using {{ run }} . All other parameters, i.e. from other Task s or within Task s, must use a qualified name. Nested levels are delimited using a . . E.g. consider a structure like: Task: param_set: a: 1 b: 2 c: 3 In order to use parameter c , you would use {{ Task.param_set.c }} as the substitution. Take care when using substitutions! This process will not try to guess for you. When a substitution is not available, e.g. due to misspelling, one of two things will happen: If it was an environment variable that does not exist, no substitution will be performed, although a message will be printed. I.e. you will be left with param: /my/failed/{{ $SUBSTITUTION }} as your parameter. This may or may not fail the model validation step, but is likely not what you intended. If it was an attempt at substituting another YAML parameter which does not exist, an exception will be thrown and the program will exit. Defining your own parameters The configuration file is not validated in its totality, only on a Task -by- Task basis, but it is read in its totality. E.g. when running MyTask only that portion of the configuration is validated even though the entire file has been read, and is available for substitutions. As a result, it is safe to introduce extra entries into the YAML file, as long as they are not entered under a specific Task 's configuration. This may be useful to create your own global substitutions, for example if there is a key variable that may be used across different Task s. E.g. Consider a case where you want to create a more generic configuration file where a single variable is used by multiple Task s. This single variable may be changed between experiments, for instance, but is likely static for the duration of a single set of analyses. In order to avoid a mistake when changing the configuration between experiments you can define this special variable (or variables) as a separate entry in the YAML, and make use of substitutions in each Task 's configuration. This way the variable only needs to be changed in one place. # Define our substitution. This is only for substitutiosns! MY_SPECIAL_SUB: \"EXPMT_DEPENDENT_VALUE\" # Can change here once per experiment! RunTask1: special_var: \"{{ MY_SPECIAL_SUB }}\" var_1: 1 var_2: \"a\" # ... RunTask2: special_var: \"{{ MY_SPECIAL_SUB }}\" var_3: \"abcd\" var_4: 123 # ... RunTask3: special_var: \"{{ MY_SPECIAL_SUB }}\" #... # ... and so on","title":"Variable Substitution in YAML Files"},{"location":"usage/#gotchas","text":"Order matters While in general you can use parameters that appear later in a YAML document to substitute for values of parameters that appear earlier, the substitutions themselves will be performed in order of appearance. It is therefore NOT possible to correctly use a later parameter as a substitution for an earlier one, if the later one itself depends on a substitution. The YAML document, however, can be rearranged without error. The order in the YAML document has no effect on execution order which is determined purely by the workflow definition. As mentioned above, the document is not validated in its entirety so rearrangements are allowed. For example consider the following situation which produces an incorrect substitution: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will incorrectly be \"{{ work_dir }}/additional_path/{{ $RUN }}\" # ... RunTaskTwo: # Remember `work_dir` and `run` come from the header document and don't need to # be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" ... This configuration can be rearranged to achieve the desired result: %YAML 1.3 --- title: \"Configuration to Test YAML Substitution\" experiment: \"TestYAMLSubs\" run: 12 date: \"2024/05/01\" lute_version: 0.1 task_timeout: 600 work_dir: \"/sdf/data/lcls/ds/exp/experiment/scratch\" ... --- RunTaskTwo: # Remember `work_dir` comes from the header document and doesn't need to be qualified path: \"{{ work_dir }}/additional_path/{{ run }}\" RunTaskOne: input_dir: \"{{ RunTaskTwo.path }}\" # Will now be /sdf/data/lcls/ds/exp/experiment/scratch/additional_path/12 # ... ... On the otherhand, relationships such as these may point to inconsistencies in the dependencies between Task s which may warrant a refactor. Found unhashable key To avoid YAML parsing issues when using the substitution syntax, be sure to quote your substitutions. Before substitution is performed, a dictionary is first constructed by the pyyaml package which parses the document - it may fail to parse the document and raise an exception if the substitutions are not quoted. E.g. # USE THIS MyTask: var_sub: \"{{ other_var:04d }}\" # **DO NOT** USE THIS MyTask: var_sub: {{ other_var:04d }} During validation, Pydantic will by default cast variables if possible, because of this it is generally safe to use strings for substitutions. E.g. if your parameter is expecting an integer, and after substitution you pass \"2\" , Pydantic will cast this to the int 2 , and validation will succeed. As part of the substitution process limited type casting will also be handled if it is necessary for any formatting strings provided. E.g. \"{{ run:04d }}\" requires that run be an integer, so it will be treated as such in order to apply the formatting.","title":"Gotchas!"},{"location":"usage/#debug-environment-variables","text":"Special markers have been inserted at certain points in the execution flow for LUTE. These can be enabled by setting the environment variables detailed below. These are intended to allow developers to exit the program at certain points to investigate behaviour or a bug. For instance, when working on configuration parsing, an environment variable can be set which exits the program after passing this step. This allows you to run LUTE otherwise as normal (described above), without having to modify any additional code or insert your own early exits. Types of debug markers: LUTE_DEBUG_EXIT : Will exit the program at this point if the corresponding environment variable has been set. Developers can insert these markers as needed into their code to add new exit points, although as a rule of thumb they should be used sparingly, and generally only after major steps in the execution flow (e.g. after parsing, after beginning a task, after returning a result, etc.). In order to include a new marker in your code: from lute.execution.debug_utils import LUTE_DEBUG_EXIT def my_code() -> None: # ... LUTE_DEBUG_EXIT(\"MYENVVAR\", \"Additional message to print\") # If MYENVVAR is not set, the above function does nothing You can enable a marker by setting to 1, e.g. to enable the example marker above while running Tester : MYENVVAR=1 python -B run_task.py -t Tester -c config/test.yaml","title":"Debug Environment Variables"},{"location":"usage/#currently-used-environment-variables","text":"LUTE_DEBUG_EXIT_AT_YAML : Exits the program after reading in a YAML configuration file and performing variable substitutions, but BEFORE Pydantic validation. LUTE_DEBUG_BEFORE_TPP_EXEC : Exits the program after a ThirdPartyTask has prepared its submission command, but before exec is used to run it.","title":"Currently used environment variables"},{"location":"usage/#airflow-launch-and-dag-execution-steps","text":"The Airflow launch process actually involves a number of steps, and is rather complicated. There are two wrapper steps prior to getting to the actual Airflow API communication. launch_scripts/submit_launch_airflow.sh is run. This script calls /sdf/group/lcls/ds/tools/lute_launcher with all the same parameters that it was called with. lute_launcher runs the launch_scripts/launch_airflow.py script which was provided as the first argument. This is the true launch script launch_airflow.py communicates with the Airflow API, requesting that a specific DAG be launched. It then continues to run, and gathers the individual logs and the exit status of each step of the DAG. Airflow will then enter a loop of communication where it asks the JID to submit each step of the requested DAG as batch job using launch_scripts/submit_slurm.sh . There are some specific reasons for this complexity: The use of submit_launch_airflow.sh as a thin-wrapper around lute_launcher is to allow the true Airflow launch script to be a long-lived job. This is for compatibility with the eLog and the ARP. When run from the eLog as a workflow, the job submission process must occur within 30 seconds due to a timeout built-in to the system. This is fine when submitting jobs to run on the batch-nodes, as the submission to the queue takes very little time. So here, submit_launch_airflow.sh serves as a thin script to have lute_launcher run as a batch job. It can then run as a long-lived job (for the duration of the entire DAG) collecting log files all in one place. This allows the log for each stage of the Airflow DAG to be inspected in a single file, and through the eLog browser interface. The use lute_launcher as a wrapper around launch_airflow.py is to manage authentication and credentials. The launch_airflow.py script requires loading credentials in order to authenticate against the Airflow API. For the average user this is not possible, unless the script is run from within the lute_launcher process.","title":"Airflow Launch and DAG Execution Steps"},{"location":"adrs/","text":"Architecture Decision Records This directory contains a list of architecture and major feature decisions. Please refer to the madr_template.md for creating new ADRs. This template was adapted from the MADR template (MIT License). A table of ADRs is provided below. ADR No. Record Date Title Status 1 2023-11-06 All analysis Task s inherit from a base class Accepted 2 2023-11-06 Analysis Task submission and communication is performed via Executor s Accepted 3 2023-11-06 Executor s will run all Task s via subprocess Proposed 4 2023-11-06 Airflow Operator s and LUTE Executor s are separate entities. Proposed 5 2023-12-06 Task-Executor IPC is Managed by Communicator Objects Proposed 6 2024-02-12 Third-party Config Files Managed by Templates Rendered by ThirdPartyTask s Proposed 7 2024-02-12 Task Configuration is Stored in a Database Managed by Executor s Proposed 8 2024-03-18 Airflow credentials/authorization requires special launch program. Proposed 9 2024-04-15 Airflow launch script will run as long lived batch job. Proposed","title":"Architecture Decision Records"},{"location":"adrs/#architecture-decision-records","text":"This directory contains a list of architecture and major feature decisions. Please refer to the madr_template.md for creating new ADRs. This template was adapted from the MADR template (MIT License). A table of ADRs is provided below. ADR No. Record Date Title Status 1 2023-11-06 All analysis Task s inherit from a base class Accepted 2 2023-11-06 Analysis Task submission and communication is performed via Executor s Accepted 3 2023-11-06 Executor s will run all Task s via subprocess Proposed 4 2023-11-06 Airflow Operator s and LUTE Executor s are separate entities. Proposed 5 2023-12-06 Task-Executor IPC is Managed by Communicator Objects Proposed 6 2024-02-12 Third-party Config Files Managed by Templates Rendered by ThirdPartyTask s Proposed 7 2024-02-12 Task Configuration is Stored in a Database Managed by Executor s Proposed 8 2024-03-18 Airflow credentials/authorization requires special launch program. Proposed 9 2024-04-15 Airflow launch script will run as long lived batch job. Proposed","title":"Architecture Decision Records"},{"location":"adrs/MADR_LICENSE/","text":"Copyright 2022 ADR Github Organization Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MADR LICENSE"},{"location":"adrs/adr-1/","text":"[ADR-1] All Analysis Tasks Inherit from a Base Class Date: 2023-11-06 Status Accepted Context and Problem Statement Analysis programs of interest have varied APIs. Nonetheless, for the purposes of this software, a unified interface is desirable. Providing a unified interface can be simplified by inheritance from a base class for all analysis activites. Decision Decision Drivers The original btx tasks had heterogenous interfaces. This makes debugging challenging due to the need to look-up or remember different methods of task handling. The need to provide modular access to various types of software. A desire to reduce code redundancy for implementation decisions which affect ALL tasks. A need to provide access to/wrap third-party binaries. Considered Options Tasks as functions, with common interfaces provided through decorators, etc. Task code as functions wrapped by the execution code (cf. ADR-2) Consequences Simplified package structure. Ability to push feature updates to all Task s simultaneously. Potential complications due to an additional layer of abstraction. Compliance Data validation and type checking performed Common calling interface at higher levels relies on class structure (Execution layers, cf. ADR-2) Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-1] All Analysis Tasks Inherit from a Base Class"},{"location":"adrs/adr-1/#adr-1-all-analysis-tasks-inherit-from-a-base-class","text":"Date: 2023-11-06","title":"[ADR-1] All Analysis Tasks Inherit from a Base Class"},{"location":"adrs/adr-1/#status","text":"Accepted","title":"Status"},{"location":"adrs/adr-1/#context-and-problem-statement","text":"Analysis programs of interest have varied APIs. Nonetheless, for the purposes of this software, a unified interface is desirable. Providing a unified interface can be simplified by inheritance from a base class for all analysis activites.","title":"Context and Problem Statement"},{"location":"adrs/adr-1/#decision","text":"","title":"Decision"},{"location":"adrs/adr-1/#decision-drivers","text":"The original btx tasks had heterogenous interfaces. This makes debugging challenging due to the need to look-up or remember different methods of task handling. The need to provide modular access to various types of software. A desire to reduce code redundancy for implementation decisions which affect ALL tasks. A need to provide access to/wrap third-party binaries.","title":"Decision Drivers"},{"location":"adrs/adr-1/#considered-options","text":"Tasks as functions, with common interfaces provided through decorators, etc. Task code as functions wrapped by the execution code (cf. ADR-2)","title":"Considered Options"},{"location":"adrs/adr-1/#consequences","text":"Simplified package structure. Ability to push feature updates to all Task s simultaneously. Potential complications due to an additional layer of abstraction.","title":"Consequences"},{"location":"adrs/adr-1/#compliance","text":"Data validation and type checking performed Common calling interface at higher levels relies on class structure (Execution layers, cf. ADR-2)","title":"Compliance"},{"location":"adrs/adr-1/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-2/","text":"[ADR-2] Analysis Task Submission and Communication is Performed Via Executors Date: 2023-11-06 Status Accepted Context and Problem Statement Analysis code should be independent of the location and manner it is run. Additionally, communication is required after task submission to understand task context/state/results. This communication is best handled outside of the submitted job itself. This provides a mechanism for continued communication even in the case of task failure. A separate Executor (Controller) provides a mechanism that allows for communication and job submission to be independent of the task code itself. Therefore: An Executor will be submitted, which in turn submits the Task and manages communication activities. Decision Decision Drivers Removing the job submission and communication components from the Task code itself provides a separation of concerns allowing Task s to run indepently of execution environment. A separate Executor can prepare environment, submission requirements, etc. A desire to reduce code redundancy. Providing unified interfaces through Executor classes avoids maintaining that code independently for each task (cf. alternatives considered). Job submission strategies can be changed at the Executor level and immediately applied to all Task s. If communication APIs change, this does not affect Task code. Difficulties encountered due to edge-cases in the original btx tasks. E.g. task timeout leading to failure of a processing pipeline even if substantial work had been done and subsequent tasks could proceed. Varied methods of Task submission already exist in the original btx but the methods were not fully standardized. E.g. JobScheduler submission vs direct submission of the task. Considered Options Wrapping the execution, and communication, into the base Task class interface as pre/post analysis operations. Multiple Task subclasses for different execution environments. For communication specifically: Periodic asynchronous communication in the Task class. Consequences Task code independent of execution environment. Ability to maintain communication even in the event of Task failure. Potential complications due to an additional layer of abstraction. Compliance Airflow will submit Executor s as the \"Managed Task\" I.e. at the highlest API layer, Task s will not be submitted independently. Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-2] Analysis Task Submission and Communication is Performed Via Executors"},{"location":"adrs/adr-2/#adr-2-analysis-task-submission-and-communication-is-performed-via-executors","text":"Date: 2023-11-06","title":"[ADR-2] Analysis Task Submission and Communication is Performed Via Executors"},{"location":"adrs/adr-2/#status","text":"Accepted","title":"Status"},{"location":"adrs/adr-2/#context-and-problem-statement","text":"Analysis code should be independent of the location and manner it is run. Additionally, communication is required after task submission to understand task context/state/results. This communication is best handled outside of the submitted job itself. This provides a mechanism for continued communication even in the case of task failure. A separate Executor (Controller) provides a mechanism that allows for communication and job submission to be independent of the task code itself. Therefore: An Executor will be submitted, which in turn submits the Task and manages communication activities.","title":"Context and Problem Statement"},{"location":"adrs/adr-2/#decision","text":"","title":"Decision"},{"location":"adrs/adr-2/#decision-drivers","text":"Removing the job submission and communication components from the Task code itself provides a separation of concerns allowing Task s to run indepently of execution environment. A separate Executor can prepare environment, submission requirements, etc. A desire to reduce code redundancy. Providing unified interfaces through Executor classes avoids maintaining that code independently for each task (cf. alternatives considered). Job submission strategies can be changed at the Executor level and immediately applied to all Task s. If communication APIs change, this does not affect Task code. Difficulties encountered due to edge-cases in the original btx tasks. E.g. task timeout leading to failure of a processing pipeline even if substantial work had been done and subsequent tasks could proceed. Varied methods of Task submission already exist in the original btx but the methods were not fully standardized. E.g. JobScheduler submission vs direct submission of the task.","title":"Decision Drivers"},{"location":"adrs/adr-2/#considered-options","text":"Wrapping the execution, and communication, into the base Task class interface as pre/post analysis operations. Multiple Task subclasses for different execution environments. For communication specifically: Periodic asynchronous communication in the Task class.","title":"Considered Options"},{"location":"adrs/adr-2/#consequences","text":"Task code independent of execution environment. Ability to maintain communication even in the event of Task failure. Potential complications due to an additional layer of abstraction.","title":"Consequences"},{"location":"adrs/adr-2/#compliance","text":"Airflow will submit Executor s as the \"Managed Task\" I.e. at the highlest API layer, Task s will not be submitted independently.","title":"Compliance"},{"location":"adrs/adr-2/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-3/","text":"[ADR-3] Executor s will run all Task s via subprocess Date: 2023-11-06 Status Proposed Context and Problem Statement A mechanism is needed to submit Task s from within the Executor (cf. ADR-2) Ideally a single method can be used for all Task s, at all locations, but at the very least all Task s at a single location (e.g. S3DF, NERSC) Decision Decision Drivers Want to simplify the interface for Task submission, but have to submit both first-party and third-party code. Want to have execution/submission separated from the Task submission (cf. ADR-2) Need flexible method which can be used to run any task, and quickly adapted to new Tasks Considered Options Executor submits a separate SLURM job. This strategy was employed by JobScheduler for btx Challenging to maintain - non-trivial issues can arise, e.g. with MPI Use multiprocessing at the Python level. More complex to manage Provides more flexibility Different mechansims for third-party Task or first-party Tasks Consequences Communication must be via pipes or files Very challenging to share state between executor and task Generally want to limit this, but makes certain communciation tasks harder (passing results e.g.) Easier to run binary (i.e. third party) tasks Simple to implement. Need a separate method (e.g. a single script) which is submitted as a subprocess This script, e.g., will select the Task based on options provided by the Executor Compliance Implementation will be at base class level for the executors Metadata","title":"[ADR-3] Executors will run all Tasks via subprocess"},{"location":"adrs/adr-3/#adr-3-executors-will-run-all-tasks-via-subprocess","text":"Date: 2023-11-06","title":"[ADR-3] Executors will run all Tasks via subprocess"},{"location":"adrs/adr-3/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-3/#context-and-problem-statement","text":"A mechanism is needed to submit Task s from within the Executor (cf. ADR-2) Ideally a single method can be used for all Task s, at all locations, but at the very least all Task s at a single location (e.g. S3DF, NERSC)","title":"Context and Problem Statement"},{"location":"adrs/adr-3/#decision","text":"","title":"Decision"},{"location":"adrs/adr-3/#decision-drivers","text":"Want to simplify the interface for Task submission, but have to submit both first-party and third-party code. Want to have execution/submission separated from the Task submission (cf. ADR-2) Need flexible method which can be used to run any task, and quickly adapted to new Tasks","title":"Decision Drivers"},{"location":"adrs/adr-3/#considered-options","text":"Executor submits a separate SLURM job. This strategy was employed by JobScheduler for btx Challenging to maintain - non-trivial issues can arise, e.g. with MPI Use multiprocessing at the Python level. More complex to manage Provides more flexibility Different mechansims for third-party Task or first-party Tasks","title":"Considered Options"},{"location":"adrs/adr-3/#consequences","text":"Communication must be via pipes or files Very challenging to share state between executor and task Generally want to limit this, but makes certain communciation tasks harder (passing results e.g.) Easier to run binary (i.e. third party) tasks Simple to implement. Need a separate method (e.g. a single script) which is submitted as a subprocess This script, e.g., will select the Task based on options provided by the Executor","title":"Consequences"},{"location":"adrs/adr-3/#compliance","text":"Implementation will be at base class level for the executors","title":"Compliance"},{"location":"adrs/adr-3/#metadata","text":"","title":"Metadata"},{"location":"adrs/adr-4/","text":"[ADR-4] Airflow Operator s and LUTE Executor s are Separate Entities Date: 2023-11-06 Status Proposed Context and Problem Statement Airflow operators submit tasks by calling the JID API This is required since tasks running where Airflow is running would not have access to the data The current plan (cf. ADR-1 and ADR-2) requires submission of the Executor which in turn submits the Task Under this plan the Executor must be separated from the Airflow operator. Decision Decision Drivers * Considered Options * Consequences * Compliance Metadata","title":"[ADR-4] Airflow Operators and LUTE Executors are Separate Entities"},{"location":"adrs/adr-4/#adr-4-airflow-operators-and-lute-executors-are-separate-entities","text":"Date: 2023-11-06","title":"[ADR-4] Airflow Operators and LUTE Executors are Separate Entities"},{"location":"adrs/adr-4/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-4/#context-and-problem-statement","text":"Airflow operators submit tasks by calling the JID API This is required since tasks running where Airflow is running would not have access to the data The current plan (cf. ADR-1 and ADR-2) requires submission of the Executor which in turn submits the Task Under this plan the Executor must be separated from the Airflow operator.","title":"Context and Problem Statement"},{"location":"adrs/adr-4/#decision","text":"","title":"Decision"},{"location":"adrs/adr-4/#decision-drivers","text":"*","title":"Decision Drivers"},{"location":"adrs/adr-4/#considered-options","text":"*","title":"Considered Options"},{"location":"adrs/adr-4/#consequences","text":"*","title":"Consequences"},{"location":"adrs/adr-4/#compliance","text":"","title":"Compliance"},{"location":"adrs/adr-4/#metadata","text":"","title":"Metadata"},{"location":"adrs/adr-5/","text":"[ADR-5] Task-Executor IPC is Managed by Communicator Objects Date: 2023-12-06 Status Proposed Context and Problem Statement A form (or forms) of inter-process communication needs to be standardized between Task subprocesses and executors. Signals need to be sent potentially bidirectionally. Results need to be retrieved from the Task in a generic manner. Decision Communicator objects which maintain simple read and write mechanisms for Message objects. These latter can contain arbitrary Python objects. Task s do not interact directly with the communicator, but rather through specific instance methods which hide the communicator interfaces. Multiple Communicators can be used in parallel. The same Communicator objects are used identically at the Task and Executor layers - any changes to communication protocols are not transferred to the calling objects. Decision Drivers Task output needs to be routed to other layers of the software, but the Task s themselves should have no knowledge of where the output ends up. Ideally have the ability to send arbitrary objects (strings, arrays, objects, ...) Ideally not limited by size of the transferred object Communication should be hidden from callers - \"somewhat more declarative than imperative.\" Ability for protocols to be swapped out, or trialled without significant rewrites. Must handle uncontrolled output from \"Third-party\" software as well as \"in-house\" or \"first-party\" communication which is directly managed. Considered Options Singular specific options: Relying solely on pipes over stdout/stderr These are already controlled when the Executor opens the subprocess Unfortunately, the pipe buffer is limited, and processes may hang when the output is too large (~64k or lower depending on machine) Using a separate IPC method (e.g. Sockets) \"Binary\" or \"Third-party\" tasks would have no communication captured at all, and while signalling is not possible in the same way with these tasks, some output must still be captured and routed. Direct management of multiple communication methods E.g. use a combination of pipes and sockets, directly managed by the Task and Executor layers. Communicator Types Communicator : Abstract base class - defines interface PipeCommunicator : Manages communication through pipes ( stderr and stdout ) SocketCommunicator : Manages communication through Unix sockets Consequences Complexity due to management of (potentially) multiple communication methods Some of this compelxity is isolated, however, to a single object. From the Task and Executor side, IPC is greatly simplified Management is delegated to the Communicator Communication is \"pluggable\" -> not limited by the advantages and disadvantages of any single communication method or protocol Arbitrary objects can be sent and received Limits on size or type of object should not be an issue (e.g. large results output can be handled) Compliance Communication is handled in base classes. Communicator objects are non-public. Their interfaces (already limited) are handled by simple methods in the base classes of Task s and Executor s. The Communicator should have no need to be directly manipulated by callers (even less so by users) Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-5] Task-Executor IPC is Managed by Communicator Objects"},{"location":"adrs/adr-5/#adr-5-task-executor-ipc-is-managed-by-communicator-objects","text":"Date: 2023-12-06","title":"[ADR-5] Task-Executor IPC is Managed by Communicator Objects"},{"location":"adrs/adr-5/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-5/#context-and-problem-statement","text":"A form (or forms) of inter-process communication needs to be standardized between Task subprocesses and executors. Signals need to be sent potentially bidirectionally. Results need to be retrieved from the Task in a generic manner.","title":"Context and Problem Statement"},{"location":"adrs/adr-5/#decision","text":"Communicator objects which maintain simple read and write mechanisms for Message objects. These latter can contain arbitrary Python objects. Task s do not interact directly with the communicator, but rather through specific instance methods which hide the communicator interfaces. Multiple Communicators can be used in parallel. The same Communicator objects are used identically at the Task and Executor layers - any changes to communication protocols are not transferred to the calling objects.","title":"Decision"},{"location":"adrs/adr-5/#decision-drivers","text":"Task output needs to be routed to other layers of the software, but the Task s themselves should have no knowledge of where the output ends up. Ideally have the ability to send arbitrary objects (strings, arrays, objects, ...) Ideally not limited by size of the transferred object Communication should be hidden from callers - \"somewhat more declarative than imperative.\" Ability for protocols to be swapped out, or trialled without significant rewrites. Must handle uncontrolled output from \"Third-party\" software as well as \"in-house\" or \"first-party\" communication which is directly managed.","title":"Decision Drivers"},{"location":"adrs/adr-5/#considered-options","text":"Singular specific options: Relying solely on pipes over stdout/stderr These are already controlled when the Executor opens the subprocess Unfortunately, the pipe buffer is limited, and processes may hang when the output is too large (~64k or lower depending on machine) Using a separate IPC method (e.g. Sockets) \"Binary\" or \"Third-party\" tasks would have no communication captured at all, and while signalling is not possible in the same way with these tasks, some output must still be captured and routed. Direct management of multiple communication methods E.g. use a combination of pipes and sockets, directly managed by the Task and Executor layers.","title":"Considered Options"},{"location":"adrs/adr-5/#communicator-types","text":"Communicator : Abstract base class - defines interface PipeCommunicator : Manages communication through pipes ( stderr and stdout ) SocketCommunicator : Manages communication through Unix sockets","title":"Communicator Types"},{"location":"adrs/adr-5/#consequences","text":"Complexity due to management of (potentially) multiple communication methods Some of this compelxity is isolated, however, to a single object. From the Task and Executor side, IPC is greatly simplified Management is delegated to the Communicator Communication is \"pluggable\" -> not limited by the advantages and disadvantages of any single communication method or protocol Arbitrary objects can be sent and received Limits on size or type of object should not be an issue (e.g. large results output can be handled)","title":"Consequences"},{"location":"adrs/adr-5/#compliance","text":"Communication is handled in base classes. Communicator objects are non-public. Their interfaces (already limited) are handled by simple methods in the base classes of Task s and Executor s. The Communicator should have no need to be directly manipulated by callers (even less so by users)","title":"Compliance"},{"location":"adrs/adr-5/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-6/","text":"[ADR-6] Third-party Config Files Managed by Templates Rendered by ThirdPartyTask s Date: 2024-02-12 Status Proposed Context and Problem Statement While many third-party executables of interest to the LUTE platform can be fully configured via command-line arguments, some also require management of an additional config file. Config files may use a variety of languages and methods. E.g. YAML, TOML, JSON, or even direct management of Python scripts. From the perspective of a generic interface to manage these files this poses a challenge. Ideally all aspects of configuraiton could be managed from the single LUTE configuration file. Decision Templates will be used for the third party configuration files. A generic interface to heterogenous templates will be provided through a combination of pydantic models and the ThirdPartyTask implementation. The pydantic models will label extra arguments to ThirdPartyTask s as being TemplateParameters . I.e. any extra parameters are considered to be for a templated configuration file. The ThirdPartyTask will find the necessary template and render it if any extra parameters are found. This puts the burden of correct parsing on the template definition itself. Decision Drivers Need to be able to configure the necessary files from within the LUTE framework. Configuration files take many forms so need a generic interface to disparate file types. Want to maintain as simple a Task interface as possible - but due to the above, need a way of handling multiple output files. Text substiution provides a means to do this. Considered Options Separate configuration Task to be run before the main ThirdPartyTask . Generate the configuration file in its entirety from within the Task . This removes the simplicity in allowing all ThirdPartyTask s to be run as instances of a single class. Consequences Can configure and run third party tasks which require the use of a configuration file. Must manage templates in addition to the standard configuration parsing code. The templates themselves provide the specific \"programming\" for filling them in. I.e. the Python interface assumes that the template will properly handle the block of parameters it is sent. Due to the above, template errors can be fatal, and appropriate attention to template creation is necessary. Allowing for template parameters in the general configuration file requires accepting the possiblity of extra parameters not defined in the data validation (pydantic) models. Extra parameters are not validated in the same way as standard parameters. We have to assume the template will properly deal with them. Compliance Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-6] Third-party Config Files Managed by Templates Rendered by ThirdPartyTasks"},{"location":"adrs/adr-6/#adr-6-third-party-config-files-managed-by-templates-rendered-by-thirdpartytasks","text":"Date: 2024-02-12","title":"[ADR-6] Third-party Config Files Managed by Templates Rendered by ThirdPartyTasks"},{"location":"adrs/adr-6/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-6/#context-and-problem-statement","text":"While many third-party executables of interest to the LUTE platform can be fully configured via command-line arguments, some also require management of an additional config file. Config files may use a variety of languages and methods. E.g. YAML, TOML, JSON, or even direct management of Python scripts. From the perspective of a generic interface to manage these files this poses a challenge. Ideally all aspects of configuraiton could be managed from the single LUTE configuration file.","title":"Context and Problem Statement"},{"location":"adrs/adr-6/#decision","text":"Templates will be used for the third party configuration files. A generic interface to heterogenous templates will be provided through a combination of pydantic models and the ThirdPartyTask implementation. The pydantic models will label extra arguments to ThirdPartyTask s as being TemplateParameters . I.e. any extra parameters are considered to be for a templated configuration file. The ThirdPartyTask will find the necessary template and render it if any extra parameters are found. This puts the burden of correct parsing on the template definition itself.","title":"Decision"},{"location":"adrs/adr-6/#decision-drivers","text":"Need to be able to configure the necessary files from within the LUTE framework. Configuration files take many forms so need a generic interface to disparate file types. Want to maintain as simple a Task interface as possible - but due to the above, need a way of handling multiple output files. Text substiution provides a means to do this.","title":"Decision Drivers"},{"location":"adrs/adr-6/#considered-options","text":"Separate configuration Task to be run before the main ThirdPartyTask . Generate the configuration file in its entirety from within the Task . This removes the simplicity in allowing all ThirdPartyTask s to be run as instances of a single class.","title":"Considered Options"},{"location":"adrs/adr-6/#consequences","text":"Can configure and run third party tasks which require the use of a configuration file. Must manage templates in addition to the standard configuration parsing code. The templates themselves provide the specific \"programming\" for filling them in. I.e. the Python interface assumes that the template will properly handle the block of parameters it is sent. Due to the above, template errors can be fatal, and appropriate attention to template creation is necessary. Allowing for template parameters in the general configuration file requires accepting the possiblity of extra parameters not defined in the data validation (pydantic) models. Extra parameters are not validated in the same way as standard parameters. We have to assume the template will properly deal with them.","title":"Consequences"},{"location":"adrs/adr-6/#compliance","text":"","title":"Compliance"},{"location":"adrs/adr-6/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-7/","text":"[ADR-7] Task Configuration is Stored in a Database Managed by Executor s Date: 2024-02-12 Status Proposed Context and Problem Statement For metadata publishing reasons, need a mechanism to maintain a history of Task parameter configurations. Each Task 's code is designed to be independent of other Task 's aside from code shared by inheritance. Dependencies between Task s are intended to be defined only at the level of workflows. Nonetheless, some Task s may have implicit dependencies on others. E.g. one Task may use the output files of another, and so could benefit from having knowledge of where they were written. Decision Upon Task completion the managing Executor will write the AnalysisConfig object, including TaskParameters , results and generic configuration information to a database. Some entries from this database can be retrieved to provide default files for TaskParameter fields; however, the Task itself has no knowledge, and does not access to the database. Decision Drivers Want to reduce explicit dependencies between Task s while allowing information to be shared between them. Have Task -independent IO be managed solely at the Executor level. Considered Options Task s write the database. Task s pass information through other mechanisms, such as Airflow. Consequences Requires a database. Additional dependency, although at least one backend can be the standard sqlite which should make everything transferrable. Allows for information to be passed between Task s without any explicit code dependencies/linkages between them. The dependency is still mostly determined by the workflow definition. Default values can be provided by the database if needed. Compliance Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-7] Task Configuration is Stored in a Database Managed by Executors"},{"location":"adrs/adr-7/#adr-7-task-configuration-is-stored-in-a-database-managed-by-executors","text":"Date: 2024-02-12","title":"[ADR-7] Task Configuration is Stored in a Database Managed by Executors"},{"location":"adrs/adr-7/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-7/#context-and-problem-statement","text":"For metadata publishing reasons, need a mechanism to maintain a history of Task parameter configurations. Each Task 's code is designed to be independent of other Task 's aside from code shared by inheritance. Dependencies between Task s are intended to be defined only at the level of workflows. Nonetheless, some Task s may have implicit dependencies on others. E.g. one Task may use the output files of another, and so could benefit from having knowledge of where they were written.","title":"Context and Problem Statement"},{"location":"adrs/adr-7/#decision","text":"Upon Task completion the managing Executor will write the AnalysisConfig object, including TaskParameters , results and generic configuration information to a database. Some entries from this database can be retrieved to provide default files for TaskParameter fields; however, the Task itself has no knowledge, and does not access to the database.","title":"Decision"},{"location":"adrs/adr-7/#decision-drivers","text":"Want to reduce explicit dependencies between Task s while allowing information to be shared between them. Have Task -independent IO be managed solely at the Executor level.","title":"Decision Drivers"},{"location":"adrs/adr-7/#considered-options","text":"Task s write the database. Task s pass information through other mechanisms, such as Airflow.","title":"Considered Options"},{"location":"adrs/adr-7/#consequences","text":"Requires a database. Additional dependency, although at least one backend can be the standard sqlite which should make everything transferrable. Allows for information to be passed between Task s without any explicit code dependencies/linkages between them. The dependency is still mostly determined by the workflow definition. Default values can be provided by the database if needed.","title":"Consequences"},{"location":"adrs/adr-7/#compliance","text":"","title":"Compliance"},{"location":"adrs/adr-7/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-8/","text":"[ADR-8] Airflow credentials/authorization requires special launch program Date: 2024-03-18 Status Proposed Context and Problem Statement Airflow is used as the workflow manager. Airflow does not currently support multi-tenancy, and LDAP is not currently supported for authentication. Multiple users will be expected to run the software and thus need to authenticate against the Airflow API. We require a mechanism to control shared credentials for multiple users. The credentials are admin credentials, so we do not want unconstrained access to them. We want users to run workflows, for instance, but not to have free access to add and remove workflows. Decision A closed-source lute_launcher program will be used to run the Airflow launch scripts. This program accesses credentials with the correct permissions. Users should otherwise not have access to the credentials. This will help ensure the credentials can be used by everyone but only to run workflows and not perform restricted admin activities. Decision Drivers Need shared access to credentials for the purpose of launching jobs. Restricted access to credentials for administrative activities. Ease of use for users Authentication should be automatic - users can not be asked for passwords etc, for jobs that need to run automatically upon data acquisition Considered Options LDAP - this may be used in the future, but requires backend work outside of our control. We will revisit the implementation arising from this ADR in the future if LDAP is supported. * Consequences Complexity Compliance Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-8] Airflow credentials/authorization requires special launch program"},{"location":"adrs/adr-8/#adr-8-airflow-credentialsauthorization-requires-special-launch-program","text":"Date: 2024-03-18","title":"[ADR-8] Airflow credentials/authorization requires special launch program"},{"location":"adrs/adr-8/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-8/#context-and-problem-statement","text":"Airflow is used as the workflow manager. Airflow does not currently support multi-tenancy, and LDAP is not currently supported for authentication. Multiple users will be expected to run the software and thus need to authenticate against the Airflow API. We require a mechanism to control shared credentials for multiple users. The credentials are admin credentials, so we do not want unconstrained access to them. We want users to run workflows, for instance, but not to have free access to add and remove workflows.","title":"Context and Problem Statement"},{"location":"adrs/adr-8/#decision","text":"A closed-source lute_launcher program will be used to run the Airflow launch scripts. This program accesses credentials with the correct permissions. Users should otherwise not have access to the credentials. This will help ensure the credentials can be used by everyone but only to run workflows and not perform restricted admin activities.","title":"Decision"},{"location":"adrs/adr-8/#decision-drivers","text":"Need shared access to credentials for the purpose of launching jobs. Restricted access to credentials for administrative activities. Ease of use for users Authentication should be automatic - users can not be asked for passwords etc, for jobs that need to run automatically upon data acquisition","title":"Decision Drivers"},{"location":"adrs/adr-8/#considered-options","text":"LDAP - this may be used in the future, but requires backend work outside of our control. We will revisit the implementation arising from this ADR in the future if LDAP is supported. *","title":"Considered Options"},{"location":"adrs/adr-8/#consequences","text":"Complexity","title":"Consequences"},{"location":"adrs/adr-8/#compliance","text":"","title":"Compliance"},{"location":"adrs/adr-8/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/adr-9/","text":"[ADR-9] Airflow launch script will run as long lived batch job. Date: 2024-04-15 Status Proposed Context and Problem Statement Each Task will produce its own log file. Log files from jobs (i.e. DAGs/workflows) run by different users will be in different locations/directories. None of these log files will be accessible from the Web UI of the eLog unless they are available to the initial launch script which starts the workflow. Decision The Airflow launch script will be a long lived process, running for the duration of the entire DAG. It will provide basic status logging information, e.g. what Task s are running, if they succeed or failed. Additionally, at the end of each Task job, the launch job will collect the log file from that job and append it to its own log. As the Airflow launch script is an entry point used from the eLog, only its log file is available to users using that UI. By converting the launch script into a long-lived monitoring job it allows the log information to be easily accessible. In order to accomplish this, the launch script must be submitted as a batch job, in order to comply with the 30 second timeout imposed by jobs run by the ARP. This necessitates providing an additional wrapper script. Decision Drivers Log availability from the eLog. All logs available from a single location. Considered Options All jobs append to the same initial file, by specifying a log file. ( --open-mode=append for SLURM) Having a monitoring job provides the opportunity to include additional information. Consequences There needs to be an additional wrapper script: submit_launch_airflow.sh which submits the launch_airflow.py script (run by lute_launcher ) as a batch job. Jobs run by the ARP can not be long-lived - there is a 30 second timeout. The ARP was intended to submit batch jobs - it captures the log file from batch jobs, so running the job directly or submitting as a batch job is equivalent in terms of presenting information to the eLog UI. Another core is used to run the job. Overhead is now two cores - 1 for the monitoring job ( launch_airflow.py ) and 1 for the Executor process. Compliance Metadata This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"[ADR-9] Airflow launch script will run as long lived batch job."},{"location":"adrs/adr-9/#adr-9-airflow-launch-script-will-run-as-long-lived-batch-job","text":"Date: 2024-04-15","title":"[ADR-9] Airflow launch script will run as long lived batch job."},{"location":"adrs/adr-9/#status","text":"Proposed","title":"Status"},{"location":"adrs/adr-9/#context-and-problem-statement","text":"Each Task will produce its own log file. Log files from jobs (i.e. DAGs/workflows) run by different users will be in different locations/directories. None of these log files will be accessible from the Web UI of the eLog unless they are available to the initial launch script which starts the workflow.","title":"Context and Problem Statement"},{"location":"adrs/adr-9/#decision","text":"The Airflow launch script will be a long lived process, running for the duration of the entire DAG. It will provide basic status logging information, e.g. what Task s are running, if they succeed or failed. Additionally, at the end of each Task job, the launch job will collect the log file from that job and append it to its own log. As the Airflow launch script is an entry point used from the eLog, only its log file is available to users using that UI. By converting the launch script into a long-lived monitoring job it allows the log information to be easily accessible. In order to accomplish this, the launch script must be submitted as a batch job, in order to comply with the 30 second timeout imposed by jobs run by the ARP. This necessitates providing an additional wrapper script.","title":"Decision"},{"location":"adrs/adr-9/#decision-drivers","text":"Log availability from the eLog. All logs available from a single location.","title":"Decision Drivers"},{"location":"adrs/adr-9/#considered-options","text":"All jobs append to the same initial file, by specifying a log file. ( --open-mode=append for SLURM) Having a monitoring job provides the opportunity to include additional information.","title":"Considered Options"},{"location":"adrs/adr-9/#consequences","text":"There needs to be an additional wrapper script: submit_launch_airflow.sh which submits the launch_airflow.py script (run by lute_launcher ) as a batch job. Jobs run by the ARP can not be long-lived - there is a 30 second timeout. The ARP was intended to submit batch jobs - it captures the log file from batch jobs, so running the job directly or submitting as a batch job is equivalent in terms of presenting information to the eLog UI. Another core is used to run the job. Overhead is now two cores - 1 for the monitoring job ( launch_airflow.py ) and 1 for the Executor process.","title":"Consequences"},{"location":"adrs/adr-9/#compliance","text":"","title":"Compliance"},{"location":"adrs/adr-9/#metadata","text":"This ADR WILL be revisited during the post-mortem of the first prototype. Compliance section will be updated as prototype evolves.","title":"Metadata"},{"location":"adrs/madr_template/","text":"Title {ADR #X : Short description/title of feature/decision} Date: Status {Accepted | Proposed | Rejected | Deprecated | Superseded} {If this proposal supersedes another, please indicate so, e.g. \"Status: Accepted, supersedes [ADR-3]\"} {Likewise, if this proposal was superseded, e.g. \"Status: Superseded by [ADR-2]\"} Context and Problem Statement {Describe the problem context and why this decision has been made/feature implemented.} Decision {Describe how the solution was arrived at in the manner it was. You may use the sections below to help.} Decision Drivers {driver 1} {driver 2} Considered Options {option 1} {option 2} Consequences {Short description of anticipated consequences} * {Anticipated consequence 1} * {Anticipated consequence 2} Compliance {How will the decision/implementation be enforced. How will compliance be validated?} Metadata {Any additional information to include}","title":"Madr template"},{"location":"adrs/madr_template/#title","text":"{ADR #X : Short description/title of feature/decision} Date:","title":"Title"},{"location":"adrs/madr_template/#status","text":"{Accepted | Proposed | Rejected | Deprecated | Superseded} {If this proposal supersedes another, please indicate so, e.g. \"Status: Accepted, supersedes [ADR-3]\"} {Likewise, if this proposal was superseded, e.g. \"Status: Superseded by [ADR-2]\"}","title":"Status"},{"location":"adrs/madr_template/#context-and-problem-statement","text":"{Describe the problem context and why this decision has been made/feature implemented.}","title":"Context and Problem Statement"},{"location":"adrs/madr_template/#decision","text":"{Describe how the solution was arrived at in the manner it was. You may use the sections below to help.}","title":"Decision"},{"location":"adrs/madr_template/#decision-drivers","text":"{driver 1} {driver 2}","title":"Decision Drivers"},{"location":"adrs/madr_template/#considered-options","text":"{option 1} {option 2}","title":"Considered Options"},{"location":"adrs/madr_template/#consequences","text":"{Short description of anticipated consequences} * {Anticipated consequence 1} * {Anticipated consequence 2}","title":"Consequences"},{"location":"adrs/madr_template/#compliance","text":"{How will the decision/implementation be enforced. How will compliance be validated?}","title":"Compliance"},{"location":"adrs/madr_template/#metadata","text":"{Any additional information to include}","title":"Metadata"},{"location":"design/database/","text":"LUTE Configuration Database Specification Date: 2024-02-12 VERSION: v0.1 Basic Outline The backend database will be sqlite, using the standard Python library. A high-level API is provided, so if needed, the backend database can be changed without affecting Executor level code. One LUTE database is created per working directory for this iteration of the database. Note that this database is independent of any database used by a workflow manager (e.g. Airflow) to manage task execution order. Each database has the following tables: 1 table for Executor configuration 1 table for general task configuration (i.e., lute.io.config.AnalysisHeader ) 1 table PER Task Executor and general configuration is shared between Task tables by pointing/linking to the entry ids in the above two tables. Multiple experiments can reside in the same table, although in practice this is unlikely to occur in production as the working directory will most likely change between experiments. gen_cfg table The general configuration table contains entries which may be shared between multiple Task s. The format of the table is: id title experiment run date lute_version task_timeout 2 \"My experiment desc\" \"EXPx00000 1 YYYY/MM/DD 0.1 6000 These parameters are extracted from the TaskParameters object. Each of those contains an AnalysisHeader object stored in the lute_config variable. For a given experimental run, this value will be shared across any Task s that are executed. Column descriptions Column Description id ID of the entry in this table. title Arbitrary description/title of the purpose of analysis. E.g. what kind of experiment is being conducted experiment LCLS Experiment. Can be a placeholder if debugging, etc. run LCLS Acquisition run. Can be a placeholder if debugging, testing, etc. date Date the configuration file was first setup. lute_version Version of the codebase being used to execute Task s. task_timeout The maximum amount of time in seconds that a Task can run before being cancelled. exec_cfg table The Executor table contains information on the environment provided to the Executor for Task execution, the polling interval used for IPC between the Task and Executor and information on the communicator protocols used for IPC. This information can be shared between Task s or between experimental runs, but not necessarily every Task of a given run will use exactly the same Executor configuration and environment. id env poll_interval communicator_desc 2 \"VAR1=val1;VAR2=val2\" 0.1 \"PipeCommunicator...;SocketCommunicator...\" Column descriptions Column Description id ID of the entry in this table. env Execution environment used by the Executor and by proxy any Tasks submitted by an Executor matching this entry. Environment is stored as a string with variables delimited by \";\" poll_interval Polling interval used for Task monitoring. communicator_desc Description of the Communicators used. NOTE : The env column currently only stores variables related to SLURM or LUTE itself. Task tables For every Task a table of the following format will be created. The exact number of columns will depend on the specific Task , as the number of parameters can vary between them, and each parameter gets its own column. Within a table, multiple experiments and runs can coexist. The experiment and run are not recorded directly. Instead the first two columns point to the id of entries in the general configuration and Executor tables respectively. The general configuration table entry will contain the experiment and run information. id timestamp gen_cfg_id exec_cfg_id P1 P2 ... Pn result.task_status result.summary result.payload result.impl_schemas valid_flag 2 \"YYYY-MM-DD HH:MM:SS\" 1 1 1 2 ... 3 \"COMPLETED\" \"Summary\" \"XYZ\" \"schema1;schema3;\" 1 3 \"YYYY-MM-DD HH:MM:SS\" 1 1 3 1 ... 4 \"FAILED\" \"Summary\" \"XYZ\" \"schema1;schema3;\" 0 Column descriptions Column Description id ID of the entry in this table. CURRENT_TIMESTAMP Full timestamp for the entry. gen_cfg_id ID of the entry in the general config table that applies to this Task entry. That table has, e.g., experiment and run number. exec_cfg_id The ID of the entry in the Executor table which applies to this Task entry. P1 - Pn The specific parameters of the Task . The P{1..n} are replaced by the actual parameter names. task_status Reported exit status of the Task . Note that the output may still be labeled invalid by the valid_flag (see below). summary Short text summary of the Task result. This is provided by the Task , or sometimes the Executor . payload Full description of result from the Task . If the object is incompatible with the database, will instead be a pointer to where it can be found. impl_schemas A string of semi-colon separated schema(s) implemented by the Task . Schemas describe conceptually the type output the Task produces. valid_flag A boolean flag for whether the result is valid. May be 0 (False) if e.g., data is missing, or corrupt, or reported status is failed. NOTE: The payload is distinct from the output files. Payloads are an optional summary of the results provided by the Task . E.g. this may include graphical descriptions of results (plots, figures, etc.). The output files themselves, if they exist, will most likely be pointed to be an output file parameter in one of the columns P{1...n} API This API is intended to be used at the Executor level, with some calls intended to provide default values for Pydantic models. Utilities for reading and inspecting the database outside of normal Task execution are addressed in the following subheader. Write record_analysis_db(cfg: DescribedAnalysis) -> None : Writes the configuration to the backend database. ... ... Read read_latest_db_entry(db_dir: str, task_name: str, param: str) -> Any : Retrieve the most recent entry from a database for a specific Task. ... ... Utilities Scripts invalidate_entry : Marks a database entry as invalid. Common reason to use this is if data has been deleted, or found to be corrupted. ... TUI and GUI dbview : TUI for database inspection. Read only. ...","title":"Database"},{"location":"design/database/#lute-configuration-database-specification","text":"Date: 2024-02-12 VERSION: v0.1","title":"LUTE Configuration Database Specification"},{"location":"design/database/#basic-outline","text":"The backend database will be sqlite, using the standard Python library. A high-level API is provided, so if needed, the backend database can be changed without affecting Executor level code. One LUTE database is created per working directory for this iteration of the database. Note that this database is independent of any database used by a workflow manager (e.g. Airflow) to manage task execution order. Each database has the following tables: 1 table for Executor configuration 1 table for general task configuration (i.e., lute.io.config.AnalysisHeader ) 1 table PER Task Executor and general configuration is shared between Task tables by pointing/linking to the entry ids in the above two tables. Multiple experiments can reside in the same table, although in practice this is unlikely to occur in production as the working directory will most likely change between experiments.","title":"Basic Outline"},{"location":"design/database/#gen_cfg-table","text":"The general configuration table contains entries which may be shared between multiple Task s. The format of the table is: id title experiment run date lute_version task_timeout 2 \"My experiment desc\" \"EXPx00000 1 YYYY/MM/DD 0.1 6000 These parameters are extracted from the TaskParameters object. Each of those contains an AnalysisHeader object stored in the lute_config variable. For a given experimental run, this value will be shared across any Task s that are executed.","title":"gen_cfg table"},{"location":"design/database/#column-descriptions","text":"Column Description id ID of the entry in this table. title Arbitrary description/title of the purpose of analysis. E.g. what kind of experiment is being conducted experiment LCLS Experiment. Can be a placeholder if debugging, etc. run LCLS Acquisition run. Can be a placeholder if debugging, testing, etc. date Date the configuration file was first setup. lute_version Version of the codebase being used to execute Task s. task_timeout The maximum amount of time in seconds that a Task can run before being cancelled.","title":"Column descriptions"},{"location":"design/database/#exec_cfg-table","text":"The Executor table contains information on the environment provided to the Executor for Task execution, the polling interval used for IPC between the Task and Executor and information on the communicator protocols used for IPC. This information can be shared between Task s or between experimental runs, but not necessarily every Task of a given run will use exactly the same Executor configuration and environment. id env poll_interval communicator_desc 2 \"VAR1=val1;VAR2=val2\" 0.1 \"PipeCommunicator...;SocketCommunicator...\"","title":"exec_cfg table"},{"location":"design/database/#column-descriptions_1","text":"Column Description id ID of the entry in this table. env Execution environment used by the Executor and by proxy any Tasks submitted by an Executor matching this entry. Environment is stored as a string with variables delimited by \";\" poll_interval Polling interval used for Task monitoring. communicator_desc Description of the Communicators used. NOTE : The env column currently only stores variables related to SLURM or LUTE itself.","title":"Column descriptions"},{"location":"design/database/#task-tables","text":"For every Task a table of the following format will be created. The exact number of columns will depend on the specific Task , as the number of parameters can vary between them, and each parameter gets its own column. Within a table, multiple experiments and runs can coexist. The experiment and run are not recorded directly. Instead the first two columns point to the id of entries in the general configuration and Executor tables respectively. The general configuration table entry will contain the experiment and run information. id timestamp gen_cfg_id exec_cfg_id P1 P2 ... Pn result.task_status result.summary result.payload result.impl_schemas valid_flag 2 \"YYYY-MM-DD HH:MM:SS\" 1 1 1 2 ... 3 \"COMPLETED\" \"Summary\" \"XYZ\" \"schema1;schema3;\" 1 3 \"YYYY-MM-DD HH:MM:SS\" 1 1 3 1 ... 4 \"FAILED\" \"Summary\" \"XYZ\" \"schema1;schema3;\" 0","title":"Task tables"},{"location":"design/database/#column-descriptions_2","text":"Column Description id ID of the entry in this table. CURRENT_TIMESTAMP Full timestamp for the entry. gen_cfg_id ID of the entry in the general config table that applies to this Task entry. That table has, e.g., experiment and run number. exec_cfg_id The ID of the entry in the Executor table which applies to this Task entry. P1 - Pn The specific parameters of the Task . The P{1..n} are replaced by the actual parameter names. task_status Reported exit status of the Task . Note that the output may still be labeled invalid by the valid_flag (see below). summary Short text summary of the Task result. This is provided by the Task , or sometimes the Executor . payload Full description of result from the Task . If the object is incompatible with the database, will instead be a pointer to where it can be found. impl_schemas A string of semi-colon separated schema(s) implemented by the Task . Schemas describe conceptually the type output the Task produces. valid_flag A boolean flag for whether the result is valid. May be 0 (False) if e.g., data is missing, or corrupt, or reported status is failed. NOTE: The payload is distinct from the output files. Payloads are an optional summary of the results provided by the Task . E.g. this may include graphical descriptions of results (plots, figures, etc.). The output files themselves, if they exist, will most likely be pointed to be an output file parameter in one of the columns P{1...n}","title":"Column descriptions"},{"location":"design/database/#api","text":"This API is intended to be used at the Executor level, with some calls intended to provide default values for Pydantic models. Utilities for reading and inspecting the database outside of normal Task execution are addressed in the following subheader.","title":"API"},{"location":"design/database/#write","text":"record_analysis_db(cfg: DescribedAnalysis) -> None : Writes the configuration to the backend database. ... ...","title":"Write"},{"location":"design/database/#read","text":"read_latest_db_entry(db_dir: str, task_name: str, param: str) -> Any : Retrieve the most recent entry from a database for a specific Task. ... ...","title":"Read"},{"location":"design/database/#utilities","text":"","title":"Utilities"},{"location":"design/database/#scripts","text":"invalidate_entry : Marks a database entry as invalid. Common reason to use this is if data has been deleted, or found to be corrupted. ...","title":"Scripts"},{"location":"design/database/#tui-and-gui","text":"dbview : TUI for database inspection. Read only. ...","title":"TUI and GUI"},{"location":"source/managed_tasks/","text":"LUTE Managed Tasks. Executor-managed Tasks with specific environment specifications are defined here. BinaryErrTester = Executor('TestBinaryErr') module-attribute Runs a test of a third-party task that fails. BinaryTester: Executor = Executor('TestBinary') module-attribute Runs a basic test of a multi-threaded third-party Task. CrystFELIndexer: Executor = Executor('IndexCrystFEL') module-attribute Runs crystallographic indexing using CrystFEL. DimpleSolver: Executor = Executor('DimpleSolve') module-attribute Solves a crystallographic structure using molecular replacement. HKLComparer: Executor = Executor('CompareHKL') module-attribute Runs analysis on merge results for statistics/figures of merit.. HKLManipulator: Executor = Executor('ManipulateHKL') module-attribute Performs format conversions (among other things) of merge results. PartialatorMerger: Executor = Executor('MergePartialator') module-attribute Runs crystallographic merging using CrystFEL's partialator. PeakFinderPsocake: Executor = Executor('FindPeaksPsocake') module-attribute Performs Bragg peak finding using psocake - DEPRECATED . PeakFinderPyAlgos: MPIExecutor = MPIExecutor('FindPeaksPyAlgos') module-attribute Performs Bragg peak finding using the PyAlgos algorithm. ReadTester: Executor = Executor('TestReadOutput') module-attribute Runs a test to confirm database reading. SHELXCRunner: Executor = Executor('RunSHELXC') module-attribute Runs CCP4 SHELXC - needed for crystallographic phasing. SmallDataProducer: Executor = Executor('SubmitSMD') module-attribute Runs the production of a smalldata HDF5 file. SocketTester: Executor = Executor('TestSocket') module-attribute Runs a test of socket-based communication. StreamFileConcatenator: Executor = Executor('ConcatenateStreamFiles') module-attribute Concatenates results from crystallographic indexing of multiple runs. Tester: Executor = Executor('Test') module-attribute Runs a basic test of a first-party Task. WriteTester: Executor = Executor('TestWriteOutput') module-attribute Runs a test to confirm database writing.","title":"managed_tasks"},{"location":"source/managed_tasks/#managed_tasks.BinaryErrTester","text":"Runs a test of a third-party task that fails.","title":"BinaryErrTester"},{"location":"source/managed_tasks/#managed_tasks.BinaryTester","text":"Runs a basic test of a multi-threaded third-party Task.","title":"BinaryTester"},{"location":"source/managed_tasks/#managed_tasks.CrystFELIndexer","text":"Runs crystallographic indexing using CrystFEL.","title":"CrystFELIndexer"},{"location":"source/managed_tasks/#managed_tasks.DimpleSolver","text":"Solves a crystallographic structure using molecular replacement.","title":"DimpleSolver"},{"location":"source/managed_tasks/#managed_tasks.HKLComparer","text":"Runs analysis on merge results for statistics/figures of merit..","title":"HKLComparer"},{"location":"source/managed_tasks/#managed_tasks.HKLManipulator","text":"Performs format conversions (among other things) of merge results.","title":"HKLManipulator"},{"location":"source/managed_tasks/#managed_tasks.PartialatorMerger","text":"Runs crystallographic merging using CrystFEL's partialator.","title":"PartialatorMerger"},{"location":"source/managed_tasks/#managed_tasks.PeakFinderPsocake","text":"Performs Bragg peak finding using psocake - DEPRECATED .","title":"PeakFinderPsocake"},{"location":"source/managed_tasks/#managed_tasks.PeakFinderPyAlgos","text":"Performs Bragg peak finding using the PyAlgos algorithm.","title":"PeakFinderPyAlgos"},{"location":"source/managed_tasks/#managed_tasks.ReadTester","text":"Runs a test to confirm database reading.","title":"ReadTester"},{"location":"source/managed_tasks/#managed_tasks.SHELXCRunner","text":"Runs CCP4 SHELXC - needed for crystallographic phasing.","title":"SHELXCRunner"},{"location":"source/managed_tasks/#managed_tasks.SmallDataProducer","text":"Runs the production of a smalldata HDF5 file.","title":"SmallDataProducer"},{"location":"source/managed_tasks/#managed_tasks.SocketTester","text":"Runs a test of socket-based communication.","title":"SocketTester"},{"location":"source/managed_tasks/#managed_tasks.StreamFileConcatenator","text":"Concatenates results from crystallographic indexing of multiple runs.","title":"StreamFileConcatenator"},{"location":"source/managed_tasks/#managed_tasks.Tester","text":"Runs a basic test of a first-party Task.","title":"Tester"},{"location":"source/managed_tasks/#managed_tasks.WriteTester","text":"Runs a test to confirm database writing.","title":"WriteTester"},{"location":"source/execution/debug_utils/","text":"Functions to assist in debugging execution of LUTE. Functions: Name Description LUTE_DEBUG_EXIT str, str_dump: Optional[str]): Exits the program if the provided env_var is set. Optionally, also prints a message if provided. Raises: ValidationError \u2013 Error raised by pydantic during data validation. (From Pydantic)","title":"debug_utils"},{"location":"source/execution/executor/","text":"Base classes and functions for handling Task execution. Executors run a Task as a subprocess and handle all communication with other services, e.g., the eLog. They accept specific handlers to override default stream parsing. Event handlers/hooks are implemented as standalone functions which can be added to an Executor. Classes: Name Description AnalysisConfig Data class for holding a managed Task's configuration. BaseExecutor Abstract base class from which all Executors are derived. Executor Default Executor implementing all basic functionality and IPC. BinaryExecutor Can execute any arbitrary binary/command as a managed task within the framework provided by LUTE. Exceptions BaseExecutor Bases: ABC ABC to manage Task execution and communication with user services. When running in a workflow, \"tasks\" (not the class instances) are submitted as Executors . The Executor manages environment setup, the actual Task submission, and communication regarding Task results and status with third party services like the eLog. Attributes: Methods: Name Description add_hook str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks Populate the event hooks with the default functions. update_environment Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task Run the task as a subprocess. Source code in lute/execution/executor.py class BaseExecutor(ABC): \"\"\"ABC to manage Task execution and communication with user services. When running in a workflow, \"tasks\" (not the class instances) are submitted as `Executors`. The Executor manages environment setup, the actual Task submission, and communication regarding Task results and status with third party services like the eLog. Attributes: Methods: add_hook(event: str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks() -> None: Populate the event hooks with the default functions. update_environment(env: Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task(): Run the task as a subprocess. \"\"\" class Hooks: \"\"\"A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. \"\"\" def no_pickle_mode(self: Self, msg: Message): ... def task_started(self: Self, msg: Message): ... def task_failed(self: Self, msg: Message): ... def task_stopped(self: Self, msg: Message): ... def task_done(self: Self, msg: Message): ... def task_cancelled(self: Self, msg: Message): ... def task_result(self: Self, msg: Message): ... def __init__( self, task_name: str, communicators: List[Communicator], poll_interval: float = 0.05, ) -> None: \"\"\"The Executor will manage the subprocess in which `task_name` is run. Args: task_name (str): The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators (List[Communicator]): A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval (float): Time to wait between reading/writing to the managed subprocess. In seconds. \"\"\" result: TaskResult = TaskResult( task_name=task_name, task_status=TaskStatus.PENDING, summary=\"\", payload=\"\" ) task_parameters: Optional[TaskParameters] = None task_env: Dict[str, str] = os.environ.copy() self._communicators: List[Communicator] = communicators communicator_desc: List[str] = [] for comm in self._communicators: comm.stage_communicator() communicator_desc.append(str(comm)) self._analysis_desc: DescribedAnalysis = DescribedAnalysis( task_result=result, task_parameters=task_parameters, task_env=task_env, poll_interval=poll_interval, communicator_desc=communicator_desc, ) def add_hook(self, event: str, hook: Callable[[Self, Message], None]) -> None: \"\"\"Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Args: event (str): The event for which the hook will be called. hook (Callable[[None], None]) The function to be called during each occurrence of the event. \"\"\" if event.upper() in LUTE_SIGNALS: setattr(self.Hooks, event.lower(), hook) @abstractmethod def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" ... def update_environment( self, env: Dict[str, str], update_path: str = \"prepend\" ) -> None: \"\"\"Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Args: env (Dict[str, str]): A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path (str): If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. \"\"\" if \"PATH\" in env: sep: str = os.pathsep if update_path == \"prepend\": env[\"PATH\"] = ( f\"{env['PATH']}{sep}{self._analysis_desc.task_env['PATH']}\" ) elif update_path == \"append\": env[\"PATH\"] = ( f\"{self._analysis_desc.task_env['PATH']}{sep}{env['PATH']}\" ) elif update_path == \"overwrite\": pass else: raise ValueError( ( f\"{update_path} is not a valid option for `update_path`!\" \" Options are: prepend, append, overwrite.\" ) ) os.environ.update(env) self._analysis_desc.task_env.update(env) def shell_source(self, env: str) -> None: \"\"\"Source a script. Unlike `update_environment` this method sources a new file. Args: env (str): Path to the script to source. \"\"\" import sys if not os.path.exists(env): logger.info(f\"Cannot source environment from {env}!\") return script: str = ( f\"set -a\\n\" f'source \"{env}\" >/dev/null\\n' f'{sys.executable} -c \"import os; print(dict(os.environ))\"\\n' ) logger.info(f\"Sourcing file {env}\") o, e = subprocess.Popen( [\"bash\", \"-c\", script], stdout=subprocess.PIPE ).communicate() new_environment: Dict[str, str] = eval(o) self._analysis_desc.task_env = new_environment def _pre_task(self) -> None: \"\"\"Any actions to be performed before task submission. This method may or may not be used by subclasses. It may be useful for logging etc. \"\"\" ... def _submit_task(self, cmd: str) -> subprocess.Popen: proc: subprocess.Popen = subprocess.Popen( cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=self._analysis_desc.task_env, ) os.set_blocking(proc.stdout.fileno(), False) os.set_blocking(proc.stderr.fileno(), False) return proc @abstractmethod def _task_loop(self, proc: subprocess.Popen) -> None: \"\"\"Actions to perform while the Task is running. This function is run in the body of a loop until the Task signals that its finished. \"\"\" ... @abstractmethod def _finalize_task(self, proc: subprocess.Popen) -> None: \"\"\"Any actions to be performed after the Task has ended. Examples include a final clearing of the pipes, retrieving results, reporting to third party services, etc. \"\"\" ... def _submit_cmd(self, executable_path: str, params: str) -> str: \"\"\"Return a formatted command for launching Task subprocess. May be overridden by subclasses. Args: executable_path (str): Path to the LUTE subprocess script. params (str): String of formatted command-line arguments. Returns: cmd (str): Appropriately formatted command for this Executor. \"\"\" cmd: str = \"\" if __debug__: cmd = f\"python -B {executable_path} {params}\" else: cmd = f\"python -OB {executable_path} {params}\" return cmd def execute_task(self) -> None: \"\"\"Run the requested Task as a subprocess.\"\"\" lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") if lute_path is None: logger.debug(\"Absolute path to subprocess_task.py not found.\") lute_path = os.path.abspath(f\"{os.path.dirname(__file__)}/../..\") self.update_environment({\"LUTE_PATH\": lute_path}) executable_path: str = f\"{lute_path}/subprocess_task.py\" config_path: str = self._analysis_desc.task_env[\"LUTE_CONFIGPATH\"] params: str = f\"-c {config_path} -t {self._analysis_desc.task_result.task_name}\" cmd: str = self._submit_cmd(executable_path, params) proc: subprocess.Popen = self._submit_task(cmd) while self._task_is_running(proc): self._task_loop(proc) time.sleep(self._analysis_desc.poll_interval) os.set_blocking(proc.stdout.fileno(), True) os.set_blocking(proc.stderr.fileno(), True) self._finalize_task(proc) proc.stdout.close() proc.stderr.close() proc.wait() if ret := proc.returncode: logger.info(f\"Task failed with return code: {ret}\") self._analysis_desc.task_result.task_status = TaskStatus.FAILED self.Hooks.task_failed(self, msg=Message()) elif self._analysis_desc.task_result.task_status == TaskStatus.RUNNING: # Ret code is 0, no exception was thrown, task forgot to set status self._analysis_desc.task_result.task_status = TaskStatus.COMPLETED logger.debug(f\"Task did not change from RUNNING status. Assume COMPLETED.\") self.Hooks.task_done(self, msg=Message()) self._store_configuration() for comm in self._communicators: comm.clear_communicator() if self._analysis_desc.task_result.task_status == TaskStatus.FAILED: logger.info(\"Exiting after Task failure. Result recorded.\") sys.exit(-1) self.process_results() def _store_configuration(self) -> None: \"\"\"Store configuration and results in the LUTE database.\"\"\" record_analysis_db(copy.deepcopy(self._analysis_desc)) def _task_is_running(self, proc: subprocess.Popen) -> bool: \"\"\"Whether a subprocess is running. Args: proc (subprocess.Popen): The subprocess to determine the run status of. Returns: bool: Is the subprocess task running. \"\"\" # Add additional conditions - don't want to exit main loop # if only stopped task_status: TaskStatus = self._analysis_desc.task_result.task_status is_running: bool = task_status != TaskStatus.COMPLETED is_running &= task_status != TaskStatus.CANCELLED is_running &= task_status != TaskStatus.TIMEDOUT return proc.poll() is None and is_running def _stop(self, proc: subprocess.Popen) -> None: \"\"\"Stop the Task subprocess.\"\"\" os.kill(proc.pid, signal.SIGTSTP) self._analysis_desc.task_result.task_status = TaskStatus.STOPPED def _continue(self, proc: subprocess.Popen) -> None: \"\"\"Resume a stopped Task subprocess.\"\"\" os.kill(proc.pid, signal.SIGCONT) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING def _set_result_from_parameters(self) -> None: \"\"\"Use TaskParameters object to set TaskResult fields. A result may be defined in terms of specific parameters. This is most useful for ThirdPartyTasks which would not otherwise have an easy way of reporting what the TaskResult is. There are two options for specifying results from parameters: 1. A single parameter (Field) of the model has an attribute `is_result`. This is a bool indicating that this parameter points to a result. E.g. a parameter `output` may set `is_result=True`. 2. The `TaskParameters.Config` has a `result_from_params` attribute. This is an appropriate option if the result is determinable for the Task, but it is not easily defined by a single parameter. The TaskParameters.Config.result_from_param can be set by a custom validator, e.g. to combine the values of multiple parameters into a single result. E.g. an `out_dir` and `out_file` parameter used together specify the result. Currently only string specifiers are supported. A TaskParameters object specifies that it contains information about the result by setting a single config option: TaskParameters.Config.set_result=True In general, this method should only be called when the above condition is met, however, there are minimal checks in it as well. \"\"\" # This method shouldn't be called unless appropriate # But we will add extra guards here if self._analysis_desc.task_parameters is None: logger.debug( \"Cannot set result from TaskParameters. TaskParameters is None!\" ) return if ( not hasattr(self._analysis_desc.task_parameters.Config, \"set_result\") or not self._analysis_desc.task_parameters.Config.set_result ): logger.debug( \"Cannot set result from TaskParameters. `set_result` not specified!\" ) return # First try to set from result_from_params (faster) if self._analysis_desc.task_parameters.Config.result_from_params is not None: result_from_params: str = ( self._analysis_desc.task_parameters.Config.result_from_params ) logger.info(f\"TaskResult specified as {result_from_params}.\") self._analysis_desc.task_result.payload = result_from_params else: # Iterate parameters to find the one that is the result schema: Dict[str, Any] = self._analysis_desc.task_parameters.schema() for param, value in self._analysis_desc.task_parameters.dict().items(): param_attrs: Dict[str, Any] = schema[\"properties\"][param] if \"is_result\" in param_attrs: is_result: bool = param_attrs[\"is_result\"] if isinstance(is_result, bool) and is_result: logger.info(f\"TaskResult specified as {value}.\") self._analysis_desc.task_result.payload = value else: logger.debug( ( f\"{param} specified as result! But specifier is of \" f\"wrong type: {type(is_result)}!\" ) ) break # We should only have 1 result-like parameter! # If we get this far and haven't changed the payload we should complain if self._analysis_desc.task_result.payload == \"\": task_name: str = self._analysis_desc.task_result.task_name logger.debug( ( f\"{task_name} specified result be set from {task_name}Parameters,\" \" but no result provided! Check model definition!\" ) ) # Now check for impl_schemas and pass to result.impl_schemas # Currently unused impl_schemas: Optional[str] = ( self._analysis_desc.task_parameters.Config.impl_schemas ) self._analysis_desc.task_result.impl_schemas = impl_schemas # If we set_result but didn't get schema information we should complain if self._analysis_desc.task_result.impl_schemas is None: task_name: str = self._analysis_desc.task_result.task_name logger.debug( ( f\"{task_name} specified result be set from {task_name}Parameters,\" \" but no schema provided! Check model definition!\" ) ) def process_results(self) -> None: \"\"\"Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. \"\"\" self._process_results() @abstractmethod def _process_results(self) -> None: ... Hooks A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. Source code in lute/execution/executor.py class Hooks: \"\"\"A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. \"\"\" def no_pickle_mode(self: Self, msg: Message): ... def task_started(self: Self, msg: Message): ... def task_failed(self: Self, msg: Message): ... def task_stopped(self: Self, msg: Message): ... def task_done(self: Self, msg: Message): ... def task_cancelled(self: Self, msg: Message): ... def task_result(self: Self, msg: Message): ... __init__(task_name, communicators, poll_interval=0.05) The Executor will manage the subprocess in which task_name is run. Parameters: task_name ( str ) \u2013 The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators ( List [ Communicator ] ) \u2013 A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval ( float , default: 0.05 ) \u2013 Time to wait between reading/writing to the managed subprocess. In seconds. Source code in lute/execution/executor.py def __init__( self, task_name: str, communicators: List[Communicator], poll_interval: float = 0.05, ) -> None: \"\"\"The Executor will manage the subprocess in which `task_name` is run. Args: task_name (str): The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators (List[Communicator]): A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval (float): Time to wait between reading/writing to the managed subprocess. In seconds. \"\"\" result: TaskResult = TaskResult( task_name=task_name, task_status=TaskStatus.PENDING, summary=\"\", payload=\"\" ) task_parameters: Optional[TaskParameters] = None task_env: Dict[str, str] = os.environ.copy() self._communicators: List[Communicator] = communicators communicator_desc: List[str] = [] for comm in self._communicators: comm.stage_communicator() communicator_desc.append(str(comm)) self._analysis_desc: DescribedAnalysis = DescribedAnalysis( task_result=result, task_parameters=task_parameters, task_env=task_env, poll_interval=poll_interval, communicator_desc=communicator_desc, ) add_default_hooks() abstractmethod Populate the set of default event hooks. Source code in lute/execution/executor.py @abstractmethod def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" ... add_hook(event, hook) Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Parameters: event ( str ) \u2013 The event for which the hook will be called. Source code in lute/execution/executor.py def add_hook(self, event: str, hook: Callable[[Self, Message], None]) -> None: \"\"\"Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Args: event (str): The event for which the hook will be called. hook (Callable[[None], None]) The function to be called during each occurrence of the event. \"\"\" if event.upper() in LUTE_SIGNALS: setattr(self.Hooks, event.lower(), hook) execute_task() Run the requested Task as a subprocess. Source code in lute/execution/executor.py def execute_task(self) -> None: \"\"\"Run the requested Task as a subprocess.\"\"\" lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") if lute_path is None: logger.debug(\"Absolute path to subprocess_task.py not found.\") lute_path = os.path.abspath(f\"{os.path.dirname(__file__)}/../..\") self.update_environment({\"LUTE_PATH\": lute_path}) executable_path: str = f\"{lute_path}/subprocess_task.py\" config_path: str = self._analysis_desc.task_env[\"LUTE_CONFIGPATH\"] params: str = f\"-c {config_path} -t {self._analysis_desc.task_result.task_name}\" cmd: str = self._submit_cmd(executable_path, params) proc: subprocess.Popen = self._submit_task(cmd) while self._task_is_running(proc): self._task_loop(proc) time.sleep(self._analysis_desc.poll_interval) os.set_blocking(proc.stdout.fileno(), True) os.set_blocking(proc.stderr.fileno(), True) self._finalize_task(proc) proc.stdout.close() proc.stderr.close() proc.wait() if ret := proc.returncode: logger.info(f\"Task failed with return code: {ret}\") self._analysis_desc.task_result.task_status = TaskStatus.FAILED self.Hooks.task_failed(self, msg=Message()) elif self._analysis_desc.task_result.task_status == TaskStatus.RUNNING: # Ret code is 0, no exception was thrown, task forgot to set status self._analysis_desc.task_result.task_status = TaskStatus.COMPLETED logger.debug(f\"Task did not change from RUNNING status. Assume COMPLETED.\") self.Hooks.task_done(self, msg=Message()) self._store_configuration() for comm in self._communicators: comm.clear_communicator() if self._analysis_desc.task_result.task_status == TaskStatus.FAILED: logger.info(\"Exiting after Task failure. Result recorded.\") sys.exit(-1) self.process_results() process_results() Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. Source code in lute/execution/executor.py def process_results(self) -> None: \"\"\"Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. \"\"\" self._process_results() shell_source(env) Source a script. Unlike update_environment this method sources a new file. Parameters: env ( str ) \u2013 Path to the script to source. Source code in lute/execution/executor.py def shell_source(self, env: str) -> None: \"\"\"Source a script. Unlike `update_environment` this method sources a new file. Args: env (str): Path to the script to source. \"\"\" import sys if not os.path.exists(env): logger.info(f\"Cannot source environment from {env}!\") return script: str = ( f\"set -a\\n\" f'source \"{env}\" >/dev/null\\n' f'{sys.executable} -c \"import os; print(dict(os.environ))\"\\n' ) logger.info(f\"Sourcing file {env}\") o, e = subprocess.Popen( [\"bash\", \"-c\", script], stdout=subprocess.PIPE ).communicate() new_environment: Dict[str, str] = eval(o) self._analysis_desc.task_env = new_environment update_environment(env, update_path='prepend') Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Parameters: env ( Dict [ str , str ] ) \u2013 A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path ( str , default: 'prepend' ) \u2013 If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. Source code in lute/execution/executor.py def update_environment( self, env: Dict[str, str], update_path: str = \"prepend\" ) -> None: \"\"\"Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Args: env (Dict[str, str]): A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path (str): If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. \"\"\" if \"PATH\" in env: sep: str = os.pathsep if update_path == \"prepend\": env[\"PATH\"] = ( f\"{env['PATH']}{sep}{self._analysis_desc.task_env['PATH']}\" ) elif update_path == \"append\": env[\"PATH\"] = ( f\"{self._analysis_desc.task_env['PATH']}{sep}{env['PATH']}\" ) elif update_path == \"overwrite\": pass else: raise ValueError( ( f\"{update_path} is not a valid option for `update_path`!\" \" Options are: prepend, append, overwrite.\" ) ) os.environ.update(env) self._analysis_desc.task_env.update(env) Communicator Bases: ABC Source code in lute/execution/ipc.py class Communicator(ABC): def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... def __str__(self): name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] return f\"{name}: {self.desc}\" def __repr__(self): return self.__str__() def __enter__(self) -> Self: return self def __exit__(self) -> None: ... def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__() __init__(party=Party.TASK, use_pickle=True) Abstract Base Class for IPC Communicator objects. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using pickle prior to sending it. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" clear_communicator() Alternative exit method outside of context manager. Source code in lute/execution/ipc.py def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__() read(proc) abstractmethod Method for reading data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... stage_communicator() Alternative method for staging outside of context manager. Source code in lute/execution/ipc.py def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() write(msg) abstractmethod Method for sending data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... Executor Bases: BaseExecutor Basic implementation of an Executor which manages simple IPC with Task. Attributes: Methods: Name Description add_hook str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks Populate the event hooks with the default functions. update_environment Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task Run the task as a subprocess. Source code in lute/execution/executor.py class Executor(BaseExecutor): \"\"\"Basic implementation of an Executor which manages simple IPC with Task. Attributes: Methods: add_hook(event: str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks() -> None: Populate the event hooks with the default functions. update_environment(env: Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task(): Run the task as a subprocess. \"\"\" def __init__( self, task_name: str, communicators: List[Communicator] = [ PipeCommunicator(Party.EXECUTOR), SocketCommunicator(Party.EXECUTOR), ], poll_interval: float = 0.05, ) -> None: super().__init__( task_name=task_name, communicators=communicators, poll_interval=poll_interval, ) self.add_default_hooks() def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" def no_pickle_mode(self: Executor, msg: Message): for idx, communicator in enumerate(self._communicators): if isinstance(communicator, PipeCommunicator): self._communicators[idx] = PipeCommunicator( Party.EXECUTOR, use_pickle=False ) self.add_hook(\"no_pickle_mode\", no_pickle_mode) def task_started(self: Executor, msg: Message): if isinstance(msg.contents, TaskParameters): self._analysis_desc.task_parameters = msg.contents # Maybe just run this no matter what? Rely on the other guards? # Perhaps just check if ThirdPartyParameters? # if isinstance(self._analysis_desc.task_parameters, ThirdPartyParameters): if hasattr(self._analysis_desc.task_parameters.Config, \"set_result\"): # Third party Tasks may mark a parameter as the result # If so, setup the result now. self._set_result_from_parameters() logger.info( f\"Executor: {self._analysis_desc.task_result.task_name} started\" ) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"RUNNING\", } post_elog_run_status(elog_data) self.add_hook(\"task_started\", task_started) def task_failed(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"FAILED\", } post_elog_run_status(elog_data) self.add_hook(\"task_failed\", task_failed) def task_stopped(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"STOPPED\", } post_elog_run_status(elog_data) self.add_hook(\"task_stopped\", task_stopped) def task_done(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_done\", task_done) def task_cancelled(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"CANCELLED\", } post_elog_run_status(elog_data) self.add_hook(\"task_cancelled\", task_cancelled) def task_result(self: Executor, msg: Message): if isinstance(msg.contents, TaskResult): self._analysis_desc.task_result = msg.contents logger.info(self._analysis_desc.task_result.summary) logger.info(self._analysis_desc.task_result.task_status) elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_result\", task_result) def _task_loop(self, proc: subprocess.Popen) -> None: \"\"\"Actions to perform while the Task is running. This function is run in the body of a loop until the Task signals that its finished. \"\"\" for communicator in self._communicators: msg: Message = communicator.read(proc) if msg.signal is not None and msg.signal.upper() in LUTE_SIGNALS: hook: Callable[[Executor, Message], None] = getattr( self.Hooks, msg.signal.lower() ) hook(self, msg) if msg.contents is not None: if isinstance(msg.contents, str) and msg.contents != \"\": logger.info(msg.contents) elif not isinstance(msg.contents, str): logger.info(msg.contents) def _finalize_task(self, proc: subprocess.Popen) -> None: \"\"\"Any actions to be performed after the Task has ended. Examples include a final clearing of the pipes, retrieving results, reporting to third party services, etc. \"\"\" self._task_loop(proc) # Perform a final read. def _process_results(self) -> None: \"\"\"Performs result processing. Actions include: - For `ElogSummaryPlots`, will save the summary plot to the appropriate directory for display in the eLog. \"\"\" task_result: TaskResult = self._analysis_desc.task_result self._process_result_payload(task_result.payload) self._process_result_summary(task_result.summary) def _process_result_payload(self, payload: Any) -> None: if self._analysis_desc.task_parameters is None: logger.debug(\"Please run Task before using this method!\") return if isinstance(payload, ElogSummaryPlots): # ElogSummaryPlots has figures and a display name # display name also serves as a path. expmt: str = self._analysis_desc.task_parameters.lute_config.experiment base_path: str = f\"/sdf/data/lcls/ds/{expmt[:3]}/{expmt}/stats/summary\" full_path: str = f\"{base_path}/{payload.display_name}\" if not os.path.isdir(full_path): os.makedirs(full_path) # Preferred plots are pn.Tabs objects which save directly as html # Only supported plot type that has \"save\" method - do not want to # import plot modules here to do type checks. if hasattr(payload.figures, \"save\"): payload.figures.save(f\"{full_path}/report.html\") else: ... elif isinstance(payload, str): # May be a path to a file... schemas: Optional[str] = self._analysis_desc.task_result.impl_schemas # Should also check `impl_schemas` to determine what to do with path def _process_result_summary(self, summary: str) -> None: ... add_default_hooks() Populate the set of default event hooks. Source code in lute/execution/executor.py def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" def no_pickle_mode(self: Executor, msg: Message): for idx, communicator in enumerate(self._communicators): if isinstance(communicator, PipeCommunicator): self._communicators[idx] = PipeCommunicator( Party.EXECUTOR, use_pickle=False ) self.add_hook(\"no_pickle_mode\", no_pickle_mode) def task_started(self: Executor, msg: Message): if isinstance(msg.contents, TaskParameters): self._analysis_desc.task_parameters = msg.contents # Maybe just run this no matter what? Rely on the other guards? # Perhaps just check if ThirdPartyParameters? # if isinstance(self._analysis_desc.task_parameters, ThirdPartyParameters): if hasattr(self._analysis_desc.task_parameters.Config, \"set_result\"): # Third party Tasks may mark a parameter as the result # If so, setup the result now. self._set_result_from_parameters() logger.info( f\"Executor: {self._analysis_desc.task_result.task_name} started\" ) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"RUNNING\", } post_elog_run_status(elog_data) self.add_hook(\"task_started\", task_started) def task_failed(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"FAILED\", } post_elog_run_status(elog_data) self.add_hook(\"task_failed\", task_failed) def task_stopped(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"STOPPED\", } post_elog_run_status(elog_data) self.add_hook(\"task_stopped\", task_stopped) def task_done(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_done\", task_done) def task_cancelled(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"CANCELLED\", } post_elog_run_status(elog_data) self.add_hook(\"task_cancelled\", task_cancelled) def task_result(self: Executor, msg: Message): if isinstance(msg.contents, TaskResult): self._analysis_desc.task_result = msg.contents logger.info(self._analysis_desc.task_result.summary) logger.info(self._analysis_desc.task_result.task_status) elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_result\", task_result) MPIExecutor Bases: Executor Runs first-party Tasks that require MPI. This Executor is otherwise identical to the standard Executor, except it uses mpirun for Task submission. Currently this Executor assumes a job has been submitted using SLURM as a first step. It will determine the number of MPI ranks based on the resources requested. As a fallback, it will try to determine the number of local cores available for cases where a job has not been submitted via SLURM. On S3DF, the second determination mechanism should accurately match the environment variable provided by SLURM indicating resources allocated. This Executor will submit the Task to run with a number of processes equal to the total number of cores available minus 1. A single core is reserved for the Executor itself. Note that currently this means that you must submit on 3 cores or more, since MPI requires a minimum of 2 ranks, and the number of ranks is determined from the cores dedicated to Task execution. Methods: Name Description _submit_cmd Run the task as a subprocess using mpirun . Source code in lute/execution/executor.py class MPIExecutor(Executor): \"\"\"Runs first-party Tasks that require MPI. This Executor is otherwise identical to the standard Executor, except it uses `mpirun` for `Task` submission. Currently this Executor assumes a job has been submitted using SLURM as a first step. It will determine the number of MPI ranks based on the resources requested. As a fallback, it will try to determine the number of local cores available for cases where a job has not been submitted via SLURM. On S3DF, the second determination mechanism should accurately match the environment variable provided by SLURM indicating resources allocated. This Executor will submit the Task to run with a number of processes equal to the total number of cores available minus 1. A single core is reserved for the Executor itself. Note that currently this means that you must submit on 3 cores or more, since MPI requires a minimum of 2 ranks, and the number of ranks is determined from the cores dedicated to Task execution. Methods: _submit_cmd: Run the task as a subprocess using `mpirun`. \"\"\" def _submit_cmd(self, executable_path: str, params: str) -> str: \"\"\"Override submission command to use `mpirun` Args: executable_path (str): Path to the LUTE subprocess script. params (str): String of formatted command-line arguments. Returns: cmd (str): Appropriately formatted command for this Executor. \"\"\" py_cmd: str = \"\" nprocs: int = max( int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1 ) mpi_cmd: str = f\"mpirun -np {nprocs}\" if __debug__: py_cmd = f\"python -B -u -m mpi4py.run {executable_path} {params}\" else: py_cmd = f\"python -OB -u -m mpi4py.run {executable_path} {params}\" cmd: str = f\"{mpi_cmd} {py_cmd}\" return cmd Party Bases: Enum Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. Source code in lute/execution/ipc.py class Party(Enum): \"\"\"Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. \"\"\" TASK = 0 \"\"\" The Task (client) side. \"\"\" EXECUTOR = 1 \"\"\" The Executor (server) side. \"\"\" EXECUTOR = 1 class-attribute instance-attribute The Executor (server) side. TASK = 0 class-attribute instance-attribute The Task (client) side. PipeCommunicator Bases: Communicator Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the Task will be writing while the Executor will be reading. stderr is used for sending signals. Source code in lute/execution/ipc.py class PipeCommunicator(Communicator): \"\"\"Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the `Task` will be writing while the `Executor` will be reading. `stderr` is used for sending signals. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) def _safe_unpickle_decode(self, maybe_mixed: bytes) -> Optional[str]: \"\"\"This method is used to unpickle and/or decode a bytes object. It attempts to handle cases where contents can be mixed, i.e., part of the message must be decoded and the other part unpickled. It handles only two-way splits. If there are more complex arrangements such as: <pickled>:<unpickled>:<pickled> etc, it will give up. The simpler two way splits are unlikely to occur in normal usage. They may arise when debugging if, e.g., `print` statements are mixed with the usage of the `_report_to_executor` method. Note that this method works because ONLY text data is assumed to be sent via the pipes. The method needs to be revised to handle non-text data if the `Task` is modified to also send that via PipeCommunicator. The use of pickle is supported to provide for this option if it is necessary. It may be deprecated in the future. Be careful when making changes. This method has seemingly redundant checks because unpickling will not throw an error if a full object can be retrieved. That is, the library will ignore extraneous bytes. This method attempts to retrieve that information if the pickled data comes first in the stream. Args: maybe_mixed (bytes): A bytes object which could require unpickling, decoding, or both. Returns: contents (Optional[str]): The unpickled/decoded contents if possible. Otherwise, None. \"\"\" contents: Optional[str] try: contents = pickle.loads(maybe_mixed) repickled: bytes = pickle.dumps(contents) if len(repickled) < len(maybe_mixed): # Successful unpickling, but pickle stops even if there are more bytes try: additional_data: str = maybe_mixed[len(repickled) :].decode() contents = f\"{contents}{additional_data}\" except UnicodeDecodeError: # Can't decode the bytes left by pickle, so they are lost missing_bytes: int = len(maybe_mixed) - len(repickled) logger.debug( f\"PipeCommunicator has truncated message. Unable to retrieve {missing_bytes} bytes.\" ) except (pickle.UnpicklingError, ValueError, EOFError) as err: # Pickle may also throw a ValueError, e.g. this bytes: b\"Found! \\n\" # Pickle may also throw an EOFError, eg. this bytes: b\"F0\\n\" try: contents = maybe_mixed.decode() except UnicodeDecodeError as err2: try: contents = maybe_mixed[: err2.start].decode() contents = f\"{contents}{pickle.loads(maybe_mixed[err2.start:])}\" except Exception as err3: logger.debug( f\"PipeCommunicator unable to decode/parse data! {err3}\" ) contents = None return contents def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents) __init__(party=Party.TASK, use_pickle=True) IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" read(proc) Read from stdout and stderr. Parameters: proc ( Popen ) \u2013 The process to read from. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) write(msg) Write to stdout and stderr. The signal component is sent to stderr while the contents of the Message are sent to stdout . Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents) SocketCommunicator Bases: Communicator Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable LUTE_SOCKET=/path/to/socket This class assumes proper permissions and that this above environment variable has been defined. The Task is configured as what would commonly be referred to as the client , while the Executor is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. Source code in lute/execution/ipc.py class SocketCommunicator(Communicator): \"\"\"Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable: `LUTE_SOCKET=/path/to/socket` This class assumes proper permissions and that this above environment variable has been defined. The `Task` is configured as what would commonly be referred to as the `client`, while the `Executor` is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. \"\"\" READ_TIMEOUT: float = 0.01 \"\"\" Maximum time to wait to retrieve data. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg) def _create_socket(self) -> socket.socket: \"\"\"Returns a socket object. Returns: data_socket (socket.socket): Unix socket object. \"\"\" socket_path: str try: socket_path = os.environ[\"LUTE_SOCKET\"] except KeyError as err: import uuid import tempfile # Define a path,up and add to environment # Executor-side always created first, Task will use the same one socket_path = f\"{tempfile.gettempdir()}/lute_{uuid.uuid4().hex}.sock\" os.environ[\"LUTE_SOCKET\"] = socket_path logger.debug(f\"SocketCommunicator defines socket_path: {socket_path}\") data_socket: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) if self._party == Party.EXECUTOR: if os.path.exists(socket_path): os.unlink(socket_path) data_socket.bind(socket_path) data_socket.listen(1) elif self._party == Party.TASK: data_socket.connect(socket_path) return data_socket def _write_socket(self, msg: Message) -> None: \"\"\"Sends data over a socket from the 'client' (Task) side. Communicator objects on the Task-side are fleeting, so a socket is opened, data is sent, and then the connection and socket are cleaned up. \"\"\" self._data_socket.sendall(pickle.dumps(msg)) self._clean_up() def _clean_up(self) -> None: \"\"\"Clean up connections.\"\"\" # Check the object exists in case the Communicator is cleaned up before # opening any connections if hasattr(self, \"_data_socket\"): socket_path: str = self._data_socket.getsockname() self._data_socket.close() if self._party == Party.EXECUTOR: os.unlink(socket_path) @property def socket_path(self) -> str: socket_path: str = self._data_socket.getsockname() return socket_path def __exit__(self): self._clean_up() READ_TIMEOUT: float = 0.01 class-attribute instance-attribute Maximum time to wait to retrieve data. __init__(party=Party.TASK, use_pickle=True) IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to use pickle. Always True currently, passing False does not change behaviour. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) read(proc) Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Parameters: proc ( Popen ) \u2013 The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg write(msg) Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg)","title":"executor"},{"location":"source/execution/executor/#execution.executor--exceptions","text":"","title":"Exceptions"},{"location":"source/execution/executor/#execution.executor.BaseExecutor","text":"Bases: ABC ABC to manage Task execution and communication with user services. When running in a workflow, \"tasks\" (not the class instances) are submitted as Executors . The Executor manages environment setup, the actual Task submission, and communication regarding Task results and status with third party services like the eLog. Attributes: Methods: Name Description add_hook str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks Populate the event hooks with the default functions. update_environment Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task Run the task as a subprocess. Source code in lute/execution/executor.py class BaseExecutor(ABC): \"\"\"ABC to manage Task execution and communication with user services. When running in a workflow, \"tasks\" (not the class instances) are submitted as `Executors`. The Executor manages environment setup, the actual Task submission, and communication regarding Task results and status with third party services like the eLog. Attributes: Methods: add_hook(event: str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks() -> None: Populate the event hooks with the default functions. update_environment(env: Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task(): Run the task as a subprocess. \"\"\" class Hooks: \"\"\"A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. \"\"\" def no_pickle_mode(self: Self, msg: Message): ... def task_started(self: Self, msg: Message): ... def task_failed(self: Self, msg: Message): ... def task_stopped(self: Self, msg: Message): ... def task_done(self: Self, msg: Message): ... def task_cancelled(self: Self, msg: Message): ... def task_result(self: Self, msg: Message): ... def __init__( self, task_name: str, communicators: List[Communicator], poll_interval: float = 0.05, ) -> None: \"\"\"The Executor will manage the subprocess in which `task_name` is run. Args: task_name (str): The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators (List[Communicator]): A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval (float): Time to wait between reading/writing to the managed subprocess. In seconds. \"\"\" result: TaskResult = TaskResult( task_name=task_name, task_status=TaskStatus.PENDING, summary=\"\", payload=\"\" ) task_parameters: Optional[TaskParameters] = None task_env: Dict[str, str] = os.environ.copy() self._communicators: List[Communicator] = communicators communicator_desc: List[str] = [] for comm in self._communicators: comm.stage_communicator() communicator_desc.append(str(comm)) self._analysis_desc: DescribedAnalysis = DescribedAnalysis( task_result=result, task_parameters=task_parameters, task_env=task_env, poll_interval=poll_interval, communicator_desc=communicator_desc, ) def add_hook(self, event: str, hook: Callable[[Self, Message], None]) -> None: \"\"\"Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Args: event (str): The event for which the hook will be called. hook (Callable[[None], None]) The function to be called during each occurrence of the event. \"\"\" if event.upper() in LUTE_SIGNALS: setattr(self.Hooks, event.lower(), hook) @abstractmethod def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" ... def update_environment( self, env: Dict[str, str], update_path: str = \"prepend\" ) -> None: \"\"\"Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Args: env (Dict[str, str]): A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path (str): If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. \"\"\" if \"PATH\" in env: sep: str = os.pathsep if update_path == \"prepend\": env[\"PATH\"] = ( f\"{env['PATH']}{sep}{self._analysis_desc.task_env['PATH']}\" ) elif update_path == \"append\": env[\"PATH\"] = ( f\"{self._analysis_desc.task_env['PATH']}{sep}{env['PATH']}\" ) elif update_path == \"overwrite\": pass else: raise ValueError( ( f\"{update_path} is not a valid option for `update_path`!\" \" Options are: prepend, append, overwrite.\" ) ) os.environ.update(env) self._analysis_desc.task_env.update(env) def shell_source(self, env: str) -> None: \"\"\"Source a script. Unlike `update_environment` this method sources a new file. Args: env (str): Path to the script to source. \"\"\" import sys if not os.path.exists(env): logger.info(f\"Cannot source environment from {env}!\") return script: str = ( f\"set -a\\n\" f'source \"{env}\" >/dev/null\\n' f'{sys.executable} -c \"import os; print(dict(os.environ))\"\\n' ) logger.info(f\"Sourcing file {env}\") o, e = subprocess.Popen( [\"bash\", \"-c\", script], stdout=subprocess.PIPE ).communicate() new_environment: Dict[str, str] = eval(o) self._analysis_desc.task_env = new_environment def _pre_task(self) -> None: \"\"\"Any actions to be performed before task submission. This method may or may not be used by subclasses. It may be useful for logging etc. \"\"\" ... def _submit_task(self, cmd: str) -> subprocess.Popen: proc: subprocess.Popen = subprocess.Popen( cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=self._analysis_desc.task_env, ) os.set_blocking(proc.stdout.fileno(), False) os.set_blocking(proc.stderr.fileno(), False) return proc @abstractmethod def _task_loop(self, proc: subprocess.Popen) -> None: \"\"\"Actions to perform while the Task is running. This function is run in the body of a loop until the Task signals that its finished. \"\"\" ... @abstractmethod def _finalize_task(self, proc: subprocess.Popen) -> None: \"\"\"Any actions to be performed after the Task has ended. Examples include a final clearing of the pipes, retrieving results, reporting to third party services, etc. \"\"\" ... def _submit_cmd(self, executable_path: str, params: str) -> str: \"\"\"Return a formatted command for launching Task subprocess. May be overridden by subclasses. Args: executable_path (str): Path to the LUTE subprocess script. params (str): String of formatted command-line arguments. Returns: cmd (str): Appropriately formatted command for this Executor. \"\"\" cmd: str = \"\" if __debug__: cmd = f\"python -B {executable_path} {params}\" else: cmd = f\"python -OB {executable_path} {params}\" return cmd def execute_task(self) -> None: \"\"\"Run the requested Task as a subprocess.\"\"\" lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") if lute_path is None: logger.debug(\"Absolute path to subprocess_task.py not found.\") lute_path = os.path.abspath(f\"{os.path.dirname(__file__)}/../..\") self.update_environment({\"LUTE_PATH\": lute_path}) executable_path: str = f\"{lute_path}/subprocess_task.py\" config_path: str = self._analysis_desc.task_env[\"LUTE_CONFIGPATH\"] params: str = f\"-c {config_path} -t {self._analysis_desc.task_result.task_name}\" cmd: str = self._submit_cmd(executable_path, params) proc: subprocess.Popen = self._submit_task(cmd) while self._task_is_running(proc): self._task_loop(proc) time.sleep(self._analysis_desc.poll_interval) os.set_blocking(proc.stdout.fileno(), True) os.set_blocking(proc.stderr.fileno(), True) self._finalize_task(proc) proc.stdout.close() proc.stderr.close() proc.wait() if ret := proc.returncode: logger.info(f\"Task failed with return code: {ret}\") self._analysis_desc.task_result.task_status = TaskStatus.FAILED self.Hooks.task_failed(self, msg=Message()) elif self._analysis_desc.task_result.task_status == TaskStatus.RUNNING: # Ret code is 0, no exception was thrown, task forgot to set status self._analysis_desc.task_result.task_status = TaskStatus.COMPLETED logger.debug(f\"Task did not change from RUNNING status. Assume COMPLETED.\") self.Hooks.task_done(self, msg=Message()) self._store_configuration() for comm in self._communicators: comm.clear_communicator() if self._analysis_desc.task_result.task_status == TaskStatus.FAILED: logger.info(\"Exiting after Task failure. Result recorded.\") sys.exit(-1) self.process_results() def _store_configuration(self) -> None: \"\"\"Store configuration and results in the LUTE database.\"\"\" record_analysis_db(copy.deepcopy(self._analysis_desc)) def _task_is_running(self, proc: subprocess.Popen) -> bool: \"\"\"Whether a subprocess is running. Args: proc (subprocess.Popen): The subprocess to determine the run status of. Returns: bool: Is the subprocess task running. \"\"\" # Add additional conditions - don't want to exit main loop # if only stopped task_status: TaskStatus = self._analysis_desc.task_result.task_status is_running: bool = task_status != TaskStatus.COMPLETED is_running &= task_status != TaskStatus.CANCELLED is_running &= task_status != TaskStatus.TIMEDOUT return proc.poll() is None and is_running def _stop(self, proc: subprocess.Popen) -> None: \"\"\"Stop the Task subprocess.\"\"\" os.kill(proc.pid, signal.SIGTSTP) self._analysis_desc.task_result.task_status = TaskStatus.STOPPED def _continue(self, proc: subprocess.Popen) -> None: \"\"\"Resume a stopped Task subprocess.\"\"\" os.kill(proc.pid, signal.SIGCONT) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING def _set_result_from_parameters(self) -> None: \"\"\"Use TaskParameters object to set TaskResult fields. A result may be defined in terms of specific parameters. This is most useful for ThirdPartyTasks which would not otherwise have an easy way of reporting what the TaskResult is. There are two options for specifying results from parameters: 1. A single parameter (Field) of the model has an attribute `is_result`. This is a bool indicating that this parameter points to a result. E.g. a parameter `output` may set `is_result=True`. 2. The `TaskParameters.Config` has a `result_from_params` attribute. This is an appropriate option if the result is determinable for the Task, but it is not easily defined by a single parameter. The TaskParameters.Config.result_from_param can be set by a custom validator, e.g. to combine the values of multiple parameters into a single result. E.g. an `out_dir` and `out_file` parameter used together specify the result. Currently only string specifiers are supported. A TaskParameters object specifies that it contains information about the result by setting a single config option: TaskParameters.Config.set_result=True In general, this method should only be called when the above condition is met, however, there are minimal checks in it as well. \"\"\" # This method shouldn't be called unless appropriate # But we will add extra guards here if self._analysis_desc.task_parameters is None: logger.debug( \"Cannot set result from TaskParameters. TaskParameters is None!\" ) return if ( not hasattr(self._analysis_desc.task_parameters.Config, \"set_result\") or not self._analysis_desc.task_parameters.Config.set_result ): logger.debug( \"Cannot set result from TaskParameters. `set_result` not specified!\" ) return # First try to set from result_from_params (faster) if self._analysis_desc.task_parameters.Config.result_from_params is not None: result_from_params: str = ( self._analysis_desc.task_parameters.Config.result_from_params ) logger.info(f\"TaskResult specified as {result_from_params}.\") self._analysis_desc.task_result.payload = result_from_params else: # Iterate parameters to find the one that is the result schema: Dict[str, Any] = self._analysis_desc.task_parameters.schema() for param, value in self._analysis_desc.task_parameters.dict().items(): param_attrs: Dict[str, Any] = schema[\"properties\"][param] if \"is_result\" in param_attrs: is_result: bool = param_attrs[\"is_result\"] if isinstance(is_result, bool) and is_result: logger.info(f\"TaskResult specified as {value}.\") self._analysis_desc.task_result.payload = value else: logger.debug( ( f\"{param} specified as result! But specifier is of \" f\"wrong type: {type(is_result)}!\" ) ) break # We should only have 1 result-like parameter! # If we get this far and haven't changed the payload we should complain if self._analysis_desc.task_result.payload == \"\": task_name: str = self._analysis_desc.task_result.task_name logger.debug( ( f\"{task_name} specified result be set from {task_name}Parameters,\" \" but no result provided! Check model definition!\" ) ) # Now check for impl_schemas and pass to result.impl_schemas # Currently unused impl_schemas: Optional[str] = ( self._analysis_desc.task_parameters.Config.impl_schemas ) self._analysis_desc.task_result.impl_schemas = impl_schemas # If we set_result but didn't get schema information we should complain if self._analysis_desc.task_result.impl_schemas is None: task_name: str = self._analysis_desc.task_result.task_name logger.debug( ( f\"{task_name} specified result be set from {task_name}Parameters,\" \" but no schema provided! Check model definition!\" ) ) def process_results(self) -> None: \"\"\"Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. \"\"\" self._process_results() @abstractmethod def _process_results(self) -> None: ...","title":"BaseExecutor"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.Hooks","text":"A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. Source code in lute/execution/executor.py class Hooks: \"\"\"A container class for the Executor's event hooks. There is a corresponding function (hook) for each event/signal. Each function takes two parameters - a reference to the Executor (self) and a reference to the Message (msg) which includes the corresponding signal. \"\"\" def no_pickle_mode(self: Self, msg: Message): ... def task_started(self: Self, msg: Message): ... def task_failed(self: Self, msg: Message): ... def task_stopped(self: Self, msg: Message): ... def task_done(self: Self, msg: Message): ... def task_cancelled(self: Self, msg: Message): ... def task_result(self: Self, msg: Message): ...","title":"Hooks"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.__init__","text":"The Executor will manage the subprocess in which task_name is run. Parameters: task_name ( str ) \u2013 The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators ( List [ Communicator ] ) \u2013 A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval ( float , default: 0.05 ) \u2013 Time to wait between reading/writing to the managed subprocess. In seconds. Source code in lute/execution/executor.py def __init__( self, task_name: str, communicators: List[Communicator], poll_interval: float = 0.05, ) -> None: \"\"\"The Executor will manage the subprocess in which `task_name` is run. Args: task_name (str): The name of the Task to be submitted. Must match the Task's class name exactly. The parameter specification must also be in a properly named model to be identified. communicators (List[Communicator]): A list of one or more communicators which manage information flow to/from the Task. Subclasses may have different defaults, and new functionality can be introduced by composing Executors with communicators. poll_interval (float): Time to wait between reading/writing to the managed subprocess. In seconds. \"\"\" result: TaskResult = TaskResult( task_name=task_name, task_status=TaskStatus.PENDING, summary=\"\", payload=\"\" ) task_parameters: Optional[TaskParameters] = None task_env: Dict[str, str] = os.environ.copy() self._communicators: List[Communicator] = communicators communicator_desc: List[str] = [] for comm in self._communicators: comm.stage_communicator() communicator_desc.append(str(comm)) self._analysis_desc: DescribedAnalysis = DescribedAnalysis( task_result=result, task_parameters=task_parameters, task_env=task_env, poll_interval=poll_interval, communicator_desc=communicator_desc, )","title":"__init__"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.add_default_hooks","text":"Populate the set of default event hooks. Source code in lute/execution/executor.py @abstractmethod def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" ...","title":"add_default_hooks"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.add_hook","text":"Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Parameters: event ( str ) \u2013 The event for which the hook will be called. Source code in lute/execution/executor.py def add_hook(self, event: str, hook: Callable[[Self, Message], None]) -> None: \"\"\"Add a new hook. Each hook is a function called any time the Executor receives a signal for a particular event, e.g. Task starts, Task ends, etc. Calling this method will remove any hook that currently exists for the event. I.e. only one hook can be called per event at a time. Creating hooks for events which do not exist is not allowed. Args: event (str): The event for which the hook will be called. hook (Callable[[None], None]) The function to be called during each occurrence of the event. \"\"\" if event.upper() in LUTE_SIGNALS: setattr(self.Hooks, event.lower(), hook)","title":"add_hook"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.execute_task","text":"Run the requested Task as a subprocess. Source code in lute/execution/executor.py def execute_task(self) -> None: \"\"\"Run the requested Task as a subprocess.\"\"\" lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") if lute_path is None: logger.debug(\"Absolute path to subprocess_task.py not found.\") lute_path = os.path.abspath(f\"{os.path.dirname(__file__)}/../..\") self.update_environment({\"LUTE_PATH\": lute_path}) executable_path: str = f\"{lute_path}/subprocess_task.py\" config_path: str = self._analysis_desc.task_env[\"LUTE_CONFIGPATH\"] params: str = f\"-c {config_path} -t {self._analysis_desc.task_result.task_name}\" cmd: str = self._submit_cmd(executable_path, params) proc: subprocess.Popen = self._submit_task(cmd) while self._task_is_running(proc): self._task_loop(proc) time.sleep(self._analysis_desc.poll_interval) os.set_blocking(proc.stdout.fileno(), True) os.set_blocking(proc.stderr.fileno(), True) self._finalize_task(proc) proc.stdout.close() proc.stderr.close() proc.wait() if ret := proc.returncode: logger.info(f\"Task failed with return code: {ret}\") self._analysis_desc.task_result.task_status = TaskStatus.FAILED self.Hooks.task_failed(self, msg=Message()) elif self._analysis_desc.task_result.task_status == TaskStatus.RUNNING: # Ret code is 0, no exception was thrown, task forgot to set status self._analysis_desc.task_result.task_status = TaskStatus.COMPLETED logger.debug(f\"Task did not change from RUNNING status. Assume COMPLETED.\") self.Hooks.task_done(self, msg=Message()) self._store_configuration() for comm in self._communicators: comm.clear_communicator() if self._analysis_desc.task_result.task_status == TaskStatus.FAILED: logger.info(\"Exiting after Task failure. Result recorded.\") sys.exit(-1) self.process_results()","title":"execute_task"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.process_results","text":"Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. Source code in lute/execution/executor.py def process_results(self) -> None: \"\"\"Perform any necessary steps to process TaskResults object. Processing will depend on subclass. Examples of steps include, moving files, converting file formats, compiling plots/figures into an HTML file, etc. \"\"\" self._process_results()","title":"process_results"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.shell_source","text":"Source a script. Unlike update_environment this method sources a new file. Parameters: env ( str ) \u2013 Path to the script to source. Source code in lute/execution/executor.py def shell_source(self, env: str) -> None: \"\"\"Source a script. Unlike `update_environment` this method sources a new file. Args: env (str): Path to the script to source. \"\"\" import sys if not os.path.exists(env): logger.info(f\"Cannot source environment from {env}!\") return script: str = ( f\"set -a\\n\" f'source \"{env}\" >/dev/null\\n' f'{sys.executable} -c \"import os; print(dict(os.environ))\"\\n' ) logger.info(f\"Sourcing file {env}\") o, e = subprocess.Popen( [\"bash\", \"-c\", script], stdout=subprocess.PIPE ).communicate() new_environment: Dict[str, str] = eval(o) self._analysis_desc.task_env = new_environment","title":"shell_source"},{"location":"source/execution/executor/#execution.executor.BaseExecutor.update_environment","text":"Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Parameters: env ( Dict [ str , str ] ) \u2013 A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path ( str , default: 'prepend' ) \u2013 If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. Source code in lute/execution/executor.py def update_environment( self, env: Dict[str, str], update_path: str = \"prepend\" ) -> None: \"\"\"Update the stored set of environment variables. These are passed to the subprocess to setup its environment. Args: env (Dict[str, str]): A dictionary of \"VAR\":\"VALUE\" pairs of environment variables to be added to the subprocess environment. If any variables already exist, the new variables will overwrite them (except PATH, see below). update_path (str): If PATH is present in the new set of variables, this argument determines how the old PATH is dealt with. There are three options: * \"prepend\" : The new PATH values are prepended to the old ones. * \"append\" : The new PATH values are appended to the old ones. * \"overwrite\" : The old PATH is overwritten by the new one. \"prepend\" is the default option. If PATH is not present in the current environment, the new PATH is used without modification. \"\"\" if \"PATH\" in env: sep: str = os.pathsep if update_path == \"prepend\": env[\"PATH\"] = ( f\"{env['PATH']}{sep}{self._analysis_desc.task_env['PATH']}\" ) elif update_path == \"append\": env[\"PATH\"] = ( f\"{self._analysis_desc.task_env['PATH']}{sep}{env['PATH']}\" ) elif update_path == \"overwrite\": pass else: raise ValueError( ( f\"{update_path} is not a valid option for `update_path`!\" \" Options are: prepend, append, overwrite.\" ) ) os.environ.update(env) self._analysis_desc.task_env.update(env)","title":"update_environment"},{"location":"source/execution/executor/#execution.executor.Communicator","text":"Bases: ABC Source code in lute/execution/ipc.py class Communicator(ABC): def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... def __str__(self): name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] return f\"{name}: {self.desc}\" def __repr__(self): return self.__str__() def __enter__(self) -> Self: return self def __exit__(self) -> None: ... def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__()","title":"Communicator"},{"location":"source/execution/executor/#execution.executor.Communicator.__init__","text":"Abstract Base Class for IPC Communicator objects. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using pickle prior to sending it. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\"","title":"__init__"},{"location":"source/execution/executor/#execution.executor.Communicator.clear_communicator","text":"Alternative exit method outside of context manager. Source code in lute/execution/ipc.py def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__()","title":"clear_communicator"},{"location":"source/execution/executor/#execution.executor.Communicator.read","text":"Method for reading data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ...","title":"read"},{"location":"source/execution/executor/#execution.executor.Communicator.stage_communicator","text":"Alternative method for staging outside of context manager. Source code in lute/execution/ipc.py def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__()","title":"stage_communicator"},{"location":"source/execution/executor/#execution.executor.Communicator.write","text":"Method for sending data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ...","title":"write"},{"location":"source/execution/executor/#execution.executor.Executor","text":"Bases: BaseExecutor Basic implementation of an Executor which manages simple IPC with Task. Attributes: Methods: Name Description add_hook str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks Populate the event hooks with the default functions. update_environment Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task Run the task as a subprocess. Source code in lute/execution/executor.py class Executor(BaseExecutor): \"\"\"Basic implementation of an Executor which manages simple IPC with Task. Attributes: Methods: add_hook(event: str, hook: Callable[[None], None]) -> None: Create a new hook to be called each time a specific event occurs. add_default_hooks() -> None: Populate the event hooks with the default functions. update_environment(env: Dict[str, str], update_path: str): Update the environment that is passed to the Task subprocess. execute_task(): Run the task as a subprocess. \"\"\" def __init__( self, task_name: str, communicators: List[Communicator] = [ PipeCommunicator(Party.EXECUTOR), SocketCommunicator(Party.EXECUTOR), ], poll_interval: float = 0.05, ) -> None: super().__init__( task_name=task_name, communicators=communicators, poll_interval=poll_interval, ) self.add_default_hooks() def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" def no_pickle_mode(self: Executor, msg: Message): for idx, communicator in enumerate(self._communicators): if isinstance(communicator, PipeCommunicator): self._communicators[idx] = PipeCommunicator( Party.EXECUTOR, use_pickle=False ) self.add_hook(\"no_pickle_mode\", no_pickle_mode) def task_started(self: Executor, msg: Message): if isinstance(msg.contents, TaskParameters): self._analysis_desc.task_parameters = msg.contents # Maybe just run this no matter what? Rely on the other guards? # Perhaps just check if ThirdPartyParameters? # if isinstance(self._analysis_desc.task_parameters, ThirdPartyParameters): if hasattr(self._analysis_desc.task_parameters.Config, \"set_result\"): # Third party Tasks may mark a parameter as the result # If so, setup the result now. self._set_result_from_parameters() logger.info( f\"Executor: {self._analysis_desc.task_result.task_name} started\" ) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"RUNNING\", } post_elog_run_status(elog_data) self.add_hook(\"task_started\", task_started) def task_failed(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"FAILED\", } post_elog_run_status(elog_data) self.add_hook(\"task_failed\", task_failed) def task_stopped(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"STOPPED\", } post_elog_run_status(elog_data) self.add_hook(\"task_stopped\", task_stopped) def task_done(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_done\", task_done) def task_cancelled(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"CANCELLED\", } post_elog_run_status(elog_data) self.add_hook(\"task_cancelled\", task_cancelled) def task_result(self: Executor, msg: Message): if isinstance(msg.contents, TaskResult): self._analysis_desc.task_result = msg.contents logger.info(self._analysis_desc.task_result.summary) logger.info(self._analysis_desc.task_result.task_status) elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_result\", task_result) def _task_loop(self, proc: subprocess.Popen) -> None: \"\"\"Actions to perform while the Task is running. This function is run in the body of a loop until the Task signals that its finished. \"\"\" for communicator in self._communicators: msg: Message = communicator.read(proc) if msg.signal is not None and msg.signal.upper() in LUTE_SIGNALS: hook: Callable[[Executor, Message], None] = getattr( self.Hooks, msg.signal.lower() ) hook(self, msg) if msg.contents is not None: if isinstance(msg.contents, str) and msg.contents != \"\": logger.info(msg.contents) elif not isinstance(msg.contents, str): logger.info(msg.contents) def _finalize_task(self, proc: subprocess.Popen) -> None: \"\"\"Any actions to be performed after the Task has ended. Examples include a final clearing of the pipes, retrieving results, reporting to third party services, etc. \"\"\" self._task_loop(proc) # Perform a final read. def _process_results(self) -> None: \"\"\"Performs result processing. Actions include: - For `ElogSummaryPlots`, will save the summary plot to the appropriate directory for display in the eLog. \"\"\" task_result: TaskResult = self._analysis_desc.task_result self._process_result_payload(task_result.payload) self._process_result_summary(task_result.summary) def _process_result_payload(self, payload: Any) -> None: if self._analysis_desc.task_parameters is None: logger.debug(\"Please run Task before using this method!\") return if isinstance(payload, ElogSummaryPlots): # ElogSummaryPlots has figures and a display name # display name also serves as a path. expmt: str = self._analysis_desc.task_parameters.lute_config.experiment base_path: str = f\"/sdf/data/lcls/ds/{expmt[:3]}/{expmt}/stats/summary\" full_path: str = f\"{base_path}/{payload.display_name}\" if not os.path.isdir(full_path): os.makedirs(full_path) # Preferred plots are pn.Tabs objects which save directly as html # Only supported plot type that has \"save\" method - do not want to # import plot modules here to do type checks. if hasattr(payload.figures, \"save\"): payload.figures.save(f\"{full_path}/report.html\") else: ... elif isinstance(payload, str): # May be a path to a file... schemas: Optional[str] = self._analysis_desc.task_result.impl_schemas # Should also check `impl_schemas` to determine what to do with path def _process_result_summary(self, summary: str) -> None: ...","title":"Executor"},{"location":"source/execution/executor/#execution.executor.Executor.add_default_hooks","text":"Populate the set of default event hooks. Source code in lute/execution/executor.py def add_default_hooks(self) -> None: \"\"\"Populate the set of default event hooks.\"\"\" def no_pickle_mode(self: Executor, msg: Message): for idx, communicator in enumerate(self._communicators): if isinstance(communicator, PipeCommunicator): self._communicators[idx] = PipeCommunicator( Party.EXECUTOR, use_pickle=False ) self.add_hook(\"no_pickle_mode\", no_pickle_mode) def task_started(self: Executor, msg: Message): if isinstance(msg.contents, TaskParameters): self._analysis_desc.task_parameters = msg.contents # Maybe just run this no matter what? Rely on the other guards? # Perhaps just check if ThirdPartyParameters? # if isinstance(self._analysis_desc.task_parameters, ThirdPartyParameters): if hasattr(self._analysis_desc.task_parameters.Config, \"set_result\"): # Third party Tasks may mark a parameter as the result # If so, setup the result now. self._set_result_from_parameters() logger.info( f\"Executor: {self._analysis_desc.task_result.task_name} started\" ) self._analysis_desc.task_result.task_status = TaskStatus.RUNNING elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"RUNNING\", } post_elog_run_status(elog_data) self.add_hook(\"task_started\", task_started) def task_failed(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"FAILED\", } post_elog_run_status(elog_data) self.add_hook(\"task_failed\", task_failed) def task_stopped(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"STOPPED\", } post_elog_run_status(elog_data) self.add_hook(\"task_stopped\", task_stopped) def task_done(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_done\", task_done) def task_cancelled(self: Executor, msg: Message): elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"CANCELLED\", } post_elog_run_status(elog_data) self.add_hook(\"task_cancelled\", task_cancelled) def task_result(self: Executor, msg: Message): if isinstance(msg.contents, TaskResult): self._analysis_desc.task_result = msg.contents logger.info(self._analysis_desc.task_result.summary) logger.info(self._analysis_desc.task_result.task_status) elog_data: Dict[str, str] = { f\"{self._analysis_desc.task_result.task_name} status\": \"COMPLETED\", } post_elog_run_status(elog_data) self.add_hook(\"task_result\", task_result)","title":"add_default_hooks"},{"location":"source/execution/executor/#execution.executor.MPIExecutor","text":"Bases: Executor Runs first-party Tasks that require MPI. This Executor is otherwise identical to the standard Executor, except it uses mpirun for Task submission. Currently this Executor assumes a job has been submitted using SLURM as a first step. It will determine the number of MPI ranks based on the resources requested. As a fallback, it will try to determine the number of local cores available for cases where a job has not been submitted via SLURM. On S3DF, the second determination mechanism should accurately match the environment variable provided by SLURM indicating resources allocated. This Executor will submit the Task to run with a number of processes equal to the total number of cores available minus 1. A single core is reserved for the Executor itself. Note that currently this means that you must submit on 3 cores or more, since MPI requires a minimum of 2 ranks, and the number of ranks is determined from the cores dedicated to Task execution. Methods: Name Description _submit_cmd Run the task as a subprocess using mpirun . Source code in lute/execution/executor.py class MPIExecutor(Executor): \"\"\"Runs first-party Tasks that require MPI. This Executor is otherwise identical to the standard Executor, except it uses `mpirun` for `Task` submission. Currently this Executor assumes a job has been submitted using SLURM as a first step. It will determine the number of MPI ranks based on the resources requested. As a fallback, it will try to determine the number of local cores available for cases where a job has not been submitted via SLURM. On S3DF, the second determination mechanism should accurately match the environment variable provided by SLURM indicating resources allocated. This Executor will submit the Task to run with a number of processes equal to the total number of cores available minus 1. A single core is reserved for the Executor itself. Note that currently this means that you must submit on 3 cores or more, since MPI requires a minimum of 2 ranks, and the number of ranks is determined from the cores dedicated to Task execution. Methods: _submit_cmd: Run the task as a subprocess using `mpirun`. \"\"\" def _submit_cmd(self, executable_path: str, params: str) -> str: \"\"\"Override submission command to use `mpirun` Args: executable_path (str): Path to the LUTE subprocess script. params (str): String of formatted command-line arguments. Returns: cmd (str): Appropriately formatted command for this Executor. \"\"\" py_cmd: str = \"\" nprocs: int = max( int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1 ) mpi_cmd: str = f\"mpirun -np {nprocs}\" if __debug__: py_cmd = f\"python -B -u -m mpi4py.run {executable_path} {params}\" else: py_cmd = f\"python -OB -u -m mpi4py.run {executable_path} {params}\" cmd: str = f\"{mpi_cmd} {py_cmd}\" return cmd","title":"MPIExecutor"},{"location":"source/execution/executor/#execution.executor.Party","text":"Bases: Enum Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. Source code in lute/execution/ipc.py class Party(Enum): \"\"\"Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. \"\"\" TASK = 0 \"\"\" The Task (client) side. \"\"\" EXECUTOR = 1 \"\"\" The Executor (server) side. \"\"\"","title":"Party"},{"location":"source/execution/executor/#execution.executor.Party.EXECUTOR","text":"The Executor (server) side.","title":"EXECUTOR"},{"location":"source/execution/executor/#execution.executor.Party.TASK","text":"The Task (client) side.","title":"TASK"},{"location":"source/execution/executor/#execution.executor.PipeCommunicator","text":"Bases: Communicator Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the Task will be writing while the Executor will be reading. stderr is used for sending signals. Source code in lute/execution/ipc.py class PipeCommunicator(Communicator): \"\"\"Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the `Task` will be writing while the `Executor` will be reading. `stderr` is used for sending signals. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) def _safe_unpickle_decode(self, maybe_mixed: bytes) -> Optional[str]: \"\"\"This method is used to unpickle and/or decode a bytes object. It attempts to handle cases where contents can be mixed, i.e., part of the message must be decoded and the other part unpickled. It handles only two-way splits. If there are more complex arrangements such as: <pickled>:<unpickled>:<pickled> etc, it will give up. The simpler two way splits are unlikely to occur in normal usage. They may arise when debugging if, e.g., `print` statements are mixed with the usage of the `_report_to_executor` method. Note that this method works because ONLY text data is assumed to be sent via the pipes. The method needs to be revised to handle non-text data if the `Task` is modified to also send that via PipeCommunicator. The use of pickle is supported to provide for this option if it is necessary. It may be deprecated in the future. Be careful when making changes. This method has seemingly redundant checks because unpickling will not throw an error if a full object can be retrieved. That is, the library will ignore extraneous bytes. This method attempts to retrieve that information if the pickled data comes first in the stream. Args: maybe_mixed (bytes): A bytes object which could require unpickling, decoding, or both. Returns: contents (Optional[str]): The unpickled/decoded contents if possible. Otherwise, None. \"\"\" contents: Optional[str] try: contents = pickle.loads(maybe_mixed) repickled: bytes = pickle.dumps(contents) if len(repickled) < len(maybe_mixed): # Successful unpickling, but pickle stops even if there are more bytes try: additional_data: str = maybe_mixed[len(repickled) :].decode() contents = f\"{contents}{additional_data}\" except UnicodeDecodeError: # Can't decode the bytes left by pickle, so they are lost missing_bytes: int = len(maybe_mixed) - len(repickled) logger.debug( f\"PipeCommunicator has truncated message. Unable to retrieve {missing_bytes} bytes.\" ) except (pickle.UnpicklingError, ValueError, EOFError) as err: # Pickle may also throw a ValueError, e.g. this bytes: b\"Found! \\n\" # Pickle may also throw an EOFError, eg. this bytes: b\"F0\\n\" try: contents = maybe_mixed.decode() except UnicodeDecodeError as err2: try: contents = maybe_mixed[: err2.start].decode() contents = f\"{contents}{pickle.loads(maybe_mixed[err2.start:])}\" except Exception as err3: logger.debug( f\"PipeCommunicator unable to decode/parse data! {err3}\" ) contents = None return contents def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents)","title":"PipeCommunicator"},{"location":"source/execution/executor/#execution.executor.PipeCommunicator.__init__","text":"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\"","title":"__init__"},{"location":"source/execution/executor/#execution.executor.PipeCommunicator.read","text":"Read from stdout and stderr. Parameters: proc ( Popen ) \u2013 The process to read from. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal)","title":"read"},{"location":"source/execution/executor/#execution.executor.PipeCommunicator.write","text":"Write to stdout and stderr. The signal component is sent to stderr while the contents of the Message are sent to stdout . Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents)","title":"write"},{"location":"source/execution/executor/#execution.executor.SocketCommunicator","text":"Bases: Communicator Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable LUTE_SOCKET=/path/to/socket This class assumes proper permissions and that this above environment variable has been defined. The Task is configured as what would commonly be referred to as the client , while the Executor is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. Source code in lute/execution/ipc.py class SocketCommunicator(Communicator): \"\"\"Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable: `LUTE_SOCKET=/path/to/socket` This class assumes proper permissions and that this above environment variable has been defined. The `Task` is configured as what would commonly be referred to as the `client`, while the `Executor` is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. \"\"\" READ_TIMEOUT: float = 0.01 \"\"\" Maximum time to wait to retrieve data. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg) def _create_socket(self) -> socket.socket: \"\"\"Returns a socket object. Returns: data_socket (socket.socket): Unix socket object. \"\"\" socket_path: str try: socket_path = os.environ[\"LUTE_SOCKET\"] except KeyError as err: import uuid import tempfile # Define a path,up and add to environment # Executor-side always created first, Task will use the same one socket_path = f\"{tempfile.gettempdir()}/lute_{uuid.uuid4().hex}.sock\" os.environ[\"LUTE_SOCKET\"] = socket_path logger.debug(f\"SocketCommunicator defines socket_path: {socket_path}\") data_socket: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) if self._party == Party.EXECUTOR: if os.path.exists(socket_path): os.unlink(socket_path) data_socket.bind(socket_path) data_socket.listen(1) elif self._party == Party.TASK: data_socket.connect(socket_path) return data_socket def _write_socket(self, msg: Message) -> None: \"\"\"Sends data over a socket from the 'client' (Task) side. Communicator objects on the Task-side are fleeting, so a socket is opened, data is sent, and then the connection and socket are cleaned up. \"\"\" self._data_socket.sendall(pickle.dumps(msg)) self._clean_up() def _clean_up(self) -> None: \"\"\"Clean up connections.\"\"\" # Check the object exists in case the Communicator is cleaned up before # opening any connections if hasattr(self, \"_data_socket\"): socket_path: str = self._data_socket.getsockname() self._data_socket.close() if self._party == Party.EXECUTOR: os.unlink(socket_path) @property def socket_path(self) -> str: socket_path: str = self._data_socket.getsockname() return socket_path def __exit__(self): self._clean_up()","title":"SocketCommunicator"},{"location":"source/execution/executor/#execution.executor.SocketCommunicator.READ_TIMEOUT","text":"Maximum time to wait to retrieve data.","title":"READ_TIMEOUT"},{"location":"source/execution/executor/#execution.executor.SocketCommunicator.__init__","text":"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to use pickle. Always True currently, passing False does not change behaviour. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0)","title":"__init__"},{"location":"source/execution/executor/#execution.executor.SocketCommunicator.read","text":"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Parameters: proc ( Popen ) \u2013 The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg","title":"read"},{"location":"source/execution/executor/#execution.executor.SocketCommunicator.write","text":"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg)","title":"write"},{"location":"source/execution/ipc/","text":"Classes and utilities for communication between Executors and subprocesses. Communicators manage message passing and parsing between subprocesses. They maintain a limited public interface of \"read\" and \"write\" operations. Behind this interface the methods of communication vary from serialization across pipes to Unix sockets, etc. All communicators pass a single object called a \"Message\" which contains an arbitrary \"contents\" field as well as an optional \"signal\" field. Classes: Communicator Bases: ABC Source code in lute/execution/ipc.py class Communicator(ABC): def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... def __str__(self): name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] return f\"{name}: {self.desc}\" def __repr__(self): return self.__str__() def __enter__(self) -> Self: return self def __exit__(self) -> None: ... def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__() __init__(party=Party.TASK, use_pickle=True) Abstract Base Class for IPC Communicator objects. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using pickle prior to sending it. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" clear_communicator() Alternative exit method outside of context manager. Source code in lute/execution/ipc.py def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__() read(proc) abstractmethod Method for reading data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... stage_communicator() Alternative method for staging outside of context manager. Source code in lute/execution/ipc.py def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() write(msg) abstractmethod Method for sending data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... Party Bases: Enum Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. Source code in lute/execution/ipc.py class Party(Enum): \"\"\"Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. \"\"\" TASK = 0 \"\"\" The Task (client) side. \"\"\" EXECUTOR = 1 \"\"\" The Executor (server) side. \"\"\" EXECUTOR = 1 class-attribute instance-attribute The Executor (server) side. TASK = 0 class-attribute instance-attribute The Task (client) side. PipeCommunicator Bases: Communicator Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the Task will be writing while the Executor will be reading. stderr is used for sending signals. Source code in lute/execution/ipc.py class PipeCommunicator(Communicator): \"\"\"Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the `Task` will be writing while the `Executor` will be reading. `stderr` is used for sending signals. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) def _safe_unpickle_decode(self, maybe_mixed: bytes) -> Optional[str]: \"\"\"This method is used to unpickle and/or decode a bytes object. It attempts to handle cases where contents can be mixed, i.e., part of the message must be decoded and the other part unpickled. It handles only two-way splits. If there are more complex arrangements such as: <pickled>:<unpickled>:<pickled> etc, it will give up. The simpler two way splits are unlikely to occur in normal usage. They may arise when debugging if, e.g., `print` statements are mixed with the usage of the `_report_to_executor` method. Note that this method works because ONLY text data is assumed to be sent via the pipes. The method needs to be revised to handle non-text data if the `Task` is modified to also send that via PipeCommunicator. The use of pickle is supported to provide for this option if it is necessary. It may be deprecated in the future. Be careful when making changes. This method has seemingly redundant checks because unpickling will not throw an error if a full object can be retrieved. That is, the library will ignore extraneous bytes. This method attempts to retrieve that information if the pickled data comes first in the stream. Args: maybe_mixed (bytes): A bytes object which could require unpickling, decoding, or both. Returns: contents (Optional[str]): The unpickled/decoded contents if possible. Otherwise, None. \"\"\" contents: Optional[str] try: contents = pickle.loads(maybe_mixed) repickled: bytes = pickle.dumps(contents) if len(repickled) < len(maybe_mixed): # Successful unpickling, but pickle stops even if there are more bytes try: additional_data: str = maybe_mixed[len(repickled) :].decode() contents = f\"{contents}{additional_data}\" except UnicodeDecodeError: # Can't decode the bytes left by pickle, so they are lost missing_bytes: int = len(maybe_mixed) - len(repickled) logger.debug( f\"PipeCommunicator has truncated message. Unable to retrieve {missing_bytes} bytes.\" ) except (pickle.UnpicklingError, ValueError, EOFError) as err: # Pickle may also throw a ValueError, e.g. this bytes: b\"Found! \\n\" # Pickle may also throw an EOFError, eg. this bytes: b\"F0\\n\" try: contents = maybe_mixed.decode() except UnicodeDecodeError as err2: try: contents = maybe_mixed[: err2.start].decode() contents = f\"{contents}{pickle.loads(maybe_mixed[err2.start:])}\" except Exception as err3: logger.debug( f\"PipeCommunicator unable to decode/parse data! {err3}\" ) contents = None return contents def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents) __init__(party=Party.TASK, use_pickle=True) IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" read(proc) Read from stdout and stderr. Parameters: proc ( Popen ) \u2013 The process to read from. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) write(msg) Write to stdout and stderr. The signal component is sent to stderr while the contents of the Message are sent to stdout . Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents) SocketCommunicator Bases: Communicator Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable LUTE_SOCKET=/path/to/socket This class assumes proper permissions and that this above environment variable has been defined. The Task is configured as what would commonly be referred to as the client , while the Executor is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. Source code in lute/execution/ipc.py class SocketCommunicator(Communicator): \"\"\"Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable: `LUTE_SOCKET=/path/to/socket` This class assumes proper permissions and that this above environment variable has been defined. The `Task` is configured as what would commonly be referred to as the `client`, while the `Executor` is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. \"\"\" READ_TIMEOUT: float = 0.01 \"\"\" Maximum time to wait to retrieve data. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg) def _create_socket(self) -> socket.socket: \"\"\"Returns a socket object. Returns: data_socket (socket.socket): Unix socket object. \"\"\" socket_path: str try: socket_path = os.environ[\"LUTE_SOCKET\"] except KeyError as err: import uuid import tempfile # Define a path,up and add to environment # Executor-side always created first, Task will use the same one socket_path = f\"{tempfile.gettempdir()}/lute_{uuid.uuid4().hex}.sock\" os.environ[\"LUTE_SOCKET\"] = socket_path logger.debug(f\"SocketCommunicator defines socket_path: {socket_path}\") data_socket: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) if self._party == Party.EXECUTOR: if os.path.exists(socket_path): os.unlink(socket_path) data_socket.bind(socket_path) data_socket.listen(1) elif self._party == Party.TASK: data_socket.connect(socket_path) return data_socket def _write_socket(self, msg: Message) -> None: \"\"\"Sends data over a socket from the 'client' (Task) side. Communicator objects on the Task-side are fleeting, so a socket is opened, data is sent, and then the connection and socket are cleaned up. \"\"\" self._data_socket.sendall(pickle.dumps(msg)) self._clean_up() def _clean_up(self) -> None: \"\"\"Clean up connections.\"\"\" # Check the object exists in case the Communicator is cleaned up before # opening any connections if hasattr(self, \"_data_socket\"): socket_path: str = self._data_socket.getsockname() self._data_socket.close() if self._party == Party.EXECUTOR: os.unlink(socket_path) @property def socket_path(self) -> str: socket_path: str = self._data_socket.getsockname() return socket_path def __exit__(self): self._clean_up() READ_TIMEOUT: float = 0.01 class-attribute instance-attribute Maximum time to wait to retrieve data. __init__(party=Party.TASK, use_pickle=True) IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to use pickle. Always True currently, passing False does not change behaviour. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) read(proc) Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Parameters: proc ( Popen ) \u2013 The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg write(msg) Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg)","title":"ipc"},{"location":"source/execution/ipc/#execution.ipc.Communicator","text":"Bases: ABC Source code in lute/execution/ipc.py class Communicator(ABC): def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\" @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ... @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ... def __str__(self): name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] return f\"{name}: {self.desc}\" def __repr__(self): return self.__str__() def __enter__(self) -> Self: return self def __exit__(self) -> None: ... def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__() def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__()","title":"Communicator"},{"location":"source/execution/ipc/#execution.ipc.Communicator.__init__","text":"Abstract Base Class for IPC Communicator objects. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using pickle prior to sending it. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"Abstract Base Class for IPC Communicator objects. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using pickle prior to sending it. \"\"\" self._party = party self._use_pickle = use_pickle self.desc = \"Communicator abstract base class.\"","title":"__init__"},{"location":"source/execution/ipc/#execution.ipc.Communicator.clear_communicator","text":"Alternative exit method outside of context manager. Source code in lute/execution/ipc.py def clear_communicator(self): \"\"\"Alternative exit method outside of context manager.\"\"\" self.__exit__()","title":"clear_communicator"},{"location":"source/execution/ipc/#execution.ipc.Communicator.read","text":"Method for reading data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def read(self, proc: subprocess.Popen) -> Message: \"\"\"Method for reading data through the communication mechanism.\"\"\" ...","title":"read"},{"location":"source/execution/ipc/#execution.ipc.Communicator.stage_communicator","text":"Alternative method for staging outside of context manager. Source code in lute/execution/ipc.py def stage_communicator(self): \"\"\"Alternative method for staging outside of context manager.\"\"\" self.__enter__()","title":"stage_communicator"},{"location":"source/execution/ipc/#execution.ipc.Communicator.write","text":"Method for sending data through the communication mechanism. Source code in lute/execution/ipc.py @abstractmethod def write(self, msg: Message) -> None: \"\"\"Method for sending data through the communication mechanism.\"\"\" ...","title":"write"},{"location":"source/execution/ipc/#execution.ipc.Party","text":"Bases: Enum Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. Source code in lute/execution/ipc.py class Party(Enum): \"\"\"Identifier for which party (side/end) is using a communicator. For some types of communication streams there may be different interfaces depending on which side of the communicator you are on. This enum is used by the communicator to determine which interface to use. \"\"\" TASK = 0 \"\"\" The Task (client) side. \"\"\" EXECUTOR = 1 \"\"\" The Executor (server) side. \"\"\"","title":"Party"},{"location":"source/execution/ipc/#execution.ipc.Party.EXECUTOR","text":"The Executor (server) side.","title":"EXECUTOR"},{"location":"source/execution/ipc/#execution.ipc.Party.TASK","text":"The Task (client) side.","title":"TASK"},{"location":"source/execution/ipc/#execution.ipc.PipeCommunicator","text":"Bases: Communicator Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the Task will be writing while the Executor will be reading. stderr is used for sending signals. Source code in lute/execution/ipc.py class PipeCommunicator(Communicator): \"\"\"Provides communication through pipes over stderr/stdout. The implementation of this communicator has reading and writing ocurring on stderr and stdout. In general the `Task` will be writing while the `Executor` will be reading. `stderr` is used for sending signals. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\" def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal) def _safe_unpickle_decode(self, maybe_mixed: bytes) -> Optional[str]: \"\"\"This method is used to unpickle and/or decode a bytes object. It attempts to handle cases where contents can be mixed, i.e., part of the message must be decoded and the other part unpickled. It handles only two-way splits. If there are more complex arrangements such as: <pickled>:<unpickled>:<pickled> etc, it will give up. The simpler two way splits are unlikely to occur in normal usage. They may arise when debugging if, e.g., `print` statements are mixed with the usage of the `_report_to_executor` method. Note that this method works because ONLY text data is assumed to be sent via the pipes. The method needs to be revised to handle non-text data if the `Task` is modified to also send that via PipeCommunicator. The use of pickle is supported to provide for this option if it is necessary. It may be deprecated in the future. Be careful when making changes. This method has seemingly redundant checks because unpickling will not throw an error if a full object can be retrieved. That is, the library will ignore extraneous bytes. This method attempts to retrieve that information if the pickled data comes first in the stream. Args: maybe_mixed (bytes): A bytes object which could require unpickling, decoding, or both. Returns: contents (Optional[str]): The unpickled/decoded contents if possible. Otherwise, None. \"\"\" contents: Optional[str] try: contents = pickle.loads(maybe_mixed) repickled: bytes = pickle.dumps(contents) if len(repickled) < len(maybe_mixed): # Successful unpickling, but pickle stops even if there are more bytes try: additional_data: str = maybe_mixed[len(repickled) :].decode() contents = f\"{contents}{additional_data}\" except UnicodeDecodeError: # Can't decode the bytes left by pickle, so they are lost missing_bytes: int = len(maybe_mixed) - len(repickled) logger.debug( f\"PipeCommunicator has truncated message. Unable to retrieve {missing_bytes} bytes.\" ) except (pickle.UnpicklingError, ValueError, EOFError) as err: # Pickle may also throw a ValueError, e.g. this bytes: b\"Found! \\n\" # Pickle may also throw an EOFError, eg. this bytes: b\"F0\\n\" try: contents = maybe_mixed.decode() except UnicodeDecodeError as err2: try: contents = maybe_mixed[: err2.start].decode() contents = f\"{contents}{pickle.loads(maybe_mixed[err2.start:])}\" except Exception as err3: logger.debug( f\"PipeCommunicator unable to decode/parse data! {err3}\" ) contents = None return contents def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents)","title":"PipeCommunicator"},{"location":"source/execution/ipc/#execution.ipc.PipeCommunicator.__init__","text":"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC through pipes. Arbitrary objects may be transmitted using pickle to serialize the data. If pickle is not used Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to serialize data using Pickle prior to sending it. If False, data is assumed to be text whi \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc = \"Communicates through stderr and stdout using pickle.\"","title":"__init__"},{"location":"source/execution/ipc/#execution.ipc.PipeCommunicator.read","text":"Read from stdout and stderr. Parameters: proc ( Popen ) \u2013 The process to read from. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read from stdout and stderr. Args: proc (subprocess.Popen): The process to read from. Returns: msg (Message): The message read, containing contents and signal. \"\"\" signal: Optional[str] contents: Optional[str] raw_signal: bytes = proc.stderr.read() raw_contents: bytes = proc.stdout.read() if raw_signal is not None: signal = raw_signal.decode() else: signal = raw_signal if raw_contents: if self._use_pickle: try: contents = pickle.loads(raw_contents) except (pickle.UnpicklingError, ValueError, EOFError) as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=False\") self._use_pickle = False contents = self._safe_unpickle_decode(raw_contents) else: try: contents = raw_contents.decode() except UnicodeDecodeError as err: logger.debug(\"PipeCommunicator (Executor) - Set _use_pickle=True\") self._use_pickle = True contents = self._safe_unpickle_decode(raw_contents) else: contents = None if signal and signal not in LUTE_SIGNALS: # Some tasks write on stderr # If the signal channel has \"non-signal\" info, add it to # contents if not contents: contents = f\"({signal})\" else: contents = f\"{contents} ({signal})\" signal = None return Message(contents=contents, signal=signal)","title":"read"},{"location":"source/execution/ipc/#execution.ipc.PipeCommunicator.write","text":"Write to stdout and stderr. The signal component is sent to stderr while the contents of the Message are sent to stdout . Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Write to stdout and stderr. The signal component is sent to `stderr` while the contents of the Message are sent to `stdout`. Args: msg (Message): The Message to send. \"\"\" if self._use_pickle: signal: bytes if msg.signal: signal = msg.signal.encode() else: signal = b\"\" contents: bytes = pickle.dumps(msg.contents) sys.stderr.buffer.write(signal) sys.stdout.buffer.write(contents) sys.stderr.buffer.flush() sys.stdout.buffer.flush() else: raw_signal: str if msg.signal: raw_signal = msg.signal else: raw_signal = \"\" raw_contents: str if isinstance(msg.contents, str): raw_contents = msg.contents elif msg.contents is None: raw_contents = \"\" else: raise ValueError( f\"Cannot send msg contents of type: {type(msg.contents)} when not using pickle!\" ) sys.stderr.write(raw_signal) sys.stdout.write(raw_contents)","title":"write"},{"location":"source/execution/ipc/#execution.ipc.SocketCommunicator","text":"Bases: Communicator Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable LUTE_SOCKET=/path/to/socket This class assumes proper permissions and that this above environment variable has been defined. The Task is configured as what would commonly be referred to as the client , while the Executor is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. Source code in lute/execution/ipc.py class SocketCommunicator(Communicator): \"\"\"Provides communication over Unix sockets. The path to the Unix socket is defined by the environment variable: `LUTE_SOCKET=/path/to/socket` This class assumes proper permissions and that this above environment variable has been defined. The `Task` is configured as what would commonly be referred to as the `client`, while the `Executor` is configured as the server. The Executor continuosly monitors for connections and appends any Messages that are received to a queue. Read requests retrieve Messages from the queue. Task-side Communicators are fleeting so they open a connection, send data, and immediately close and clean up. \"\"\" READ_TIMEOUT: float = 0.01 \"\"\" Maximum time to wait to retrieve data. \"\"\" def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0) def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg) def _create_socket(self) -> socket.socket: \"\"\"Returns a socket object. Returns: data_socket (socket.socket): Unix socket object. \"\"\" socket_path: str try: socket_path = os.environ[\"LUTE_SOCKET\"] except KeyError as err: import uuid import tempfile # Define a path,up and add to environment # Executor-side always created first, Task will use the same one socket_path = f\"{tempfile.gettempdir()}/lute_{uuid.uuid4().hex}.sock\" os.environ[\"LUTE_SOCKET\"] = socket_path logger.debug(f\"SocketCommunicator defines socket_path: {socket_path}\") data_socket: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) if self._party == Party.EXECUTOR: if os.path.exists(socket_path): os.unlink(socket_path) data_socket.bind(socket_path) data_socket.listen(1) elif self._party == Party.TASK: data_socket.connect(socket_path) return data_socket def _write_socket(self, msg: Message) -> None: \"\"\"Sends data over a socket from the 'client' (Task) side. Communicator objects on the Task-side are fleeting, so a socket is opened, data is sent, and then the connection and socket are cleaned up. \"\"\" self._data_socket.sendall(pickle.dumps(msg)) self._clean_up() def _clean_up(self) -> None: \"\"\"Clean up connections.\"\"\" # Check the object exists in case the Communicator is cleaned up before # opening any connections if hasattr(self, \"_data_socket\"): socket_path: str = self._data_socket.getsockname() self._data_socket.close() if self._party == Party.EXECUTOR: os.unlink(socket_path) @property def socket_path(self) -> str: socket_path: str = self._data_socket.getsockname() return socket_path def __exit__(self): self._clean_up()","title":"SocketCommunicator"},{"location":"source/execution/ipc/#execution.ipc.SocketCommunicator.READ_TIMEOUT","text":"Maximum time to wait to retrieve data.","title":"READ_TIMEOUT"},{"location":"source/execution/ipc/#execution.ipc.SocketCommunicator.__init__","text":"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Parameters: party ( Party , default: TASK ) \u2013 Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle ( bool , default: True ) \u2013 Whether to use pickle. Always True currently, passing False does not change behaviour. Source code in lute/execution/ipc.py def __init__(self, party: Party = Party.TASK, use_pickle: bool = True) -> None: \"\"\"IPC over a Unix socket. Unlike with the PipeCommunicator, pickle is always used to send data through the socket. Args: party (Party): Which object (side/process) the Communicator is managing IPC for. I.e., is this the \"Task\" or \"Executor\" side. use_pickle (bool): Whether to use pickle. Always True currently, passing False does not change behaviour. \"\"\" super().__init__(party=party, use_pickle=use_pickle) self.desc: str = \"Communicates through a Unix socket.\" self._data_socket: socket.socket = self._create_socket() self._data_socket.setblocking(0)","title":"__init__"},{"location":"source/execution/ipc/#execution.ipc.SocketCommunicator.read","text":"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Parameters: proc ( Popen ) \u2013 The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg ( Message ) \u2013 The message read, containing contents and signal. Source code in lute/execution/ipc.py def read(self, proc: subprocess.Popen) -> Message: \"\"\"Read data from a socket. Socket(s) are continuously monitored, and read from when new data is available. Args: proc (subprocess.Popen): The process to read from. Provided for compatibility with other Communicator subtypes. Is ignored. Returns: msg (Message): The message read, containing contents and signal. \"\"\" has_data, _, has_error = select.select( [self._data_socket], [], [self._data_socket], SocketCommunicator.READ_TIMEOUT, ) msg: Message if has_data: connection, _ = has_data[0].accept() full_data: bytes = b\"\" while True: data: bytes = connection.recv(1024) if data: full_data += data else: break msg = pickle.loads(full_data) if full_data else Message() connection.close() else: msg = Message() return msg","title":"read"},{"location":"source/execution/ipc/#execution.ipc.SocketCommunicator.write","text":"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Parameters: msg ( Message ) \u2013 The Message to send. Source code in lute/execution/ipc.py def write(self, msg: Message) -> None: \"\"\"Send a single Message. The entire Message (signal and contents) is serialized and sent through a connection over Unix socket. Args: msg (Message): The Message to send. \"\"\" self._write_socket(msg)","title":"write"},{"location":"source/io/_sqlite/","text":"Backend SQLite database utilites. Functions should be used only by the higher-level database module.","title":"_sqlite"},{"location":"source/io/config/","text":"Machinary for the IO of configuration YAML files and their validation. Functions: Name Description parse_config str, config_path: str) -> TaskParameters: Parse a configuration file and return a TaskParameters object of validated parameters for a specific Task. Raises an exception if the provided configuration does not match the expected model. Raises: ValidationError \u2013 Error raised by pydantic during data validation. (From Pydantic) AnalysisHeader Bases: BaseModel Header information for LUTE analysis runs. Source code in lute/io/models/base.py class AnalysisHeader(BaseModel): \"\"\"Header information for LUTE analysis runs.\"\"\" title: str = Field( \"LUTE Task Configuration\", description=\"Description of the configuration or experiment.\", ) experiment: str = Field(\"\", description=\"Experiment.\") run: Union[str, int] = Field(\"\", description=\"Data acquisition run.\") date: str = Field(\"1970/01/01\", description=\"Start date of analysis.\") lute_version: Union[float, str] = Field( 0.1, description=\"Version of LUTE used for analysis.\" ) task_timeout: PositiveInt = Field( 600, description=( \"Time in seconds until a task times out. Should be slightly shorter\" \" than job timeout if using a job manager (e.g. SLURM).\" ), ) work_dir: str = Field(\"\", description=\"Main working directory for LUTE.\") @validator(\"work_dir\", always=True) def validate_work_dir(cls, directory: str, values: Dict[str, Any]) -> str: work_dir: str if directory == \"\": std_work_dir = ( f\"/sdf/data/lcls/ds/{values['experiment'][:3]}/\" f\"{values['experiment']}/scratch\" ) work_dir = std_work_dir else: work_dir = directory # Check existence and permissions if not os.path.exists(work_dir): raise ValueError(f\"Working Directory: {work_dir} does not exist!\") if not os.access(work_dir, os.W_OK): # Need write access for database, files etc. raise ValueError(f\"Not write access for working directory: {work_dir}!\") return work_dir @validator(\"run\", always=True) def validate_run( cls, run: Union[str, int], values: Dict[str, Any] ) -> Union[str, int]: if run == \"\": # From Airflow RUN_NUM should have Format \"RUN_DATETIME\" - Num is first part run_time: str = os.environ.get(\"RUN_NUM\", \"\") if run_time != \"\": return int(run_time.split(\"_\")[0]) return run @validator(\"experiment\", always=True) def validate_experiment(cls, experiment: str, values: Dict[str, Any]) -> str: if experiment == \"\": arp_exp: str = os.environ.get(\"EXPERIMENT\", \"EXPX00000\") return arp_exp return experiment CompareHKLParameters Bases: ThirdPartyParameters Parameters for CrystFEL's compare_hkl for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class CompareHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `compare_hkl` for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/compare_hkl\", description=\"CrystFEL's reflection comparison binary.\", flag_type=\"\", ) in_files: Optional[str] = Field( \"\", description=\"Path to input HKLs. Space-separated list of 2. Use output of partialator e.g.\", flag_type=\"\", ) ## Need mechanism to set is_result=True ... symmetry: str = Field(\"\", description=\"Point group symmetry.\", flag_type=\"--\") cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) fom: str = Field( \"Rsplit\", description=\"Specify figure of merit to calculate.\", flag_type=\"--\" ) nshells: int = Field(10, description=\"Use n resolution shells.\", flag_type=\"--\") # NEED A NEW CASE FOR THIS -> Boolean flag, no arg, one hyphen... # fix_unity: bool = Field( # False, # description=\"Fix scale factors to unity.\", # flag_type=\"-\", # rename_param=\"u\", # ) shell_file: str = Field( \"\", description=\"Write the statistics in resolution shells to a file.\", flag_type=\"--\", rename_param=\"shell-file\", is_result=True, ) ignore_negs: bool = Field( False, description=\"Ignore reflections with negative reflections.\", flag_type=\"--\", rename_param=\"ignore-negs\", ) zero_negs: bool = Field( False, description=\"Set negative intensities to 0.\", flag_type=\"--\", rename_param=\"zero-negs\", ) sigma_cutoff: Optional[Union[float, int, str]] = Field( # \"-infinity\", description=\"Discard reflections with I/sigma(I) < n. -infinity means no cutoff.\", flag_type=\"--\", rename_param=\"sigma-cutoff\", ) rmin: Optional[float] = Field( description=\"Low resolution cutoff of 1/d (m-1). Use this or --lowres NOT both.\", flag_type=\"--\", ) lowres: Optional[float] = Field( descirption=\"Low resolution cutoff in Angstroms. Use this or --rmin NOT both.\", flag_type=\"--\", ) rmax: Optional[float] = Field( description=\"High resolution cutoff in 1/d (m-1). Use this or --highres NOT both.\", flag_type=\"--\", ) highres: Optional[float] = Field( description=\"High resolution cutoff in Angstroms. Use this or --rmax NOT both.\", flag_type=\"--\", ) @validator(\"in_files\", always=True) def validate_in_files(cls, in_files: str, values: Dict[str, Any]) -> str: if in_files == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: hkls: str = f\"{partialator_file}1 {partialator_file}2\" return hkls return in_files @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file @validator(\"symmetry\", always=True) def validate_symmetry(cls, symmetry: str, values: Dict[str, Any]) -> str: if symmetry == \"\": partialator_sym: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"symmetry\" ) if partialator_sym: return partialator_sym return symmetry @validator(\"shell_file\", always=True) def validate_shell_file(cls, shell_file: str, values: Dict[str, Any]) -> str: if shell_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: shells_out: str = partialator_file.split(\".\")[0] shells_out = f\"{shells_out}_{values['fom']}_n{values['nshells']}.dat\" return shells_out return shell_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. ConcatenateStreamFilesParameters Bases: TaskParameters Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. Source code in lute/io/models/sfx_index.py class ConcatenateStreamFilesParameters(TaskParameters): \"\"\"Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" in_file: str = Field( \"\", description=\"Root of directory tree storing stream files to merge.\", ) tag: Optional[str] = Field( \"\", description=\"Tag identifying the stream files to merge.\", ) out_file: str = Field( \"\", description=\"Path to merged output stream file.\", is_result=True ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_dir: str = str(Path(stream_file).parent) return stream_dir return in_file @validator(\"tag\", always=True) def validate_tag(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_tag: str = Path(stream_file).name.split(\"_\")[0] return stream_tag return tag @validator(\"out_file\", always=True) def validate_out_file(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_out_file: str = str( Path(values[\"in_file\"]).parent / f\"{values['tag'].stream}\" ) return stream_out_file return tag Config Bases: Config Source code in lute/io/models/sfx_index.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. DimpleSolveParameters Bases: ThirdPartyParameters Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ Source code in lute/io/models/sfx_solve.py class DimpleSolveParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/dimple\", description=\"CCP4 Dimple for solving structures with MR.\", flag_type=\"\", ) # Positional requirements - all required. in_file: str = Field( \"\", description=\"Path to input mtz.\", flag_type=\"\", ) pdb: str = Field(\"\", description=\"Path to a PDB.\", flag_type=\"\") out_dir: str = Field(\"\", description=\"Output DIRECTORY.\", flag_type=\"\") # Most used options mr_thresh: PositiveFloat = Field( 0.4, description=\"Threshold for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-when-r\", ) slow: Optional[bool] = Field( False, description=\"Perform more refinement.\", flag_type=\"--\" ) # Other options (IO) hklout: str = Field( \"final.mtz\", description=\"Output mtz file name.\", flag_type=\"--\" ) xyzout: str = Field( \"final.pdb\", description=\"Output PDB file name.\", flag_type=\"--\" ) icolumn: Optional[str] = Field( # \"IMEAN\", description=\"Name for the I column.\", flag_type=\"--\", ) sigicolumn: Optional[str] = Field( # \"SIG<ICOL>\", description=\"Name for the Sig<I> column.\", flag_type=\"--\", ) fcolumn: Optional[str] = Field( # \"F\", description=\"Name for the F column.\", flag_type=\"--\", ) sigfcolumn: Optional[str] = Field( # \"F\", description=\"Name for the Sig<F> column.\", flag_type=\"--\", ) libin: Optional[str] = Field( description=\"Ligand descriptions for refmac (LIBIN).\", flag_type=\"--\" ) refmac_key: Optional[str] = Field( description=\"Extra Refmac keywords to use in refinement.\", flag_type=\"--\", rename_param=\"refmac-key\", ) free_r_flags: Optional[str] = Field( description=\"Path to a mtz file with freeR flags.\", flag_type=\"--\", rename_param=\"free-r-flags\", ) freecolumn: Optional[Union[int, float]] = Field( # 0, description=\"Refree column with an optional value.\", flag_type=\"--\", ) img_format: Optional[str] = Field( description=\"Format of generated images. (png, jpeg, none).\", flag_type=\"-\", rename_param=\"f\", ) white_bg: bool = Field( False, description=\"Use a white background in Coot and in images.\", flag_type=\"--\", rename_param=\"white-bg\", ) no_cleanup: bool = Field( False, description=\"Retain intermediate files.\", flag_type=\"--\", rename_param=\"no-cleanup\", ) # Calculations no_blob_search: bool = Field( False, description=\"Do not search for unmodelled blobs.\", flag_type=\"--\", rename_param=\"no-blob-search\", ) anode: bool = Field( False, description=\"Use SHELX/AnoDe to find peaks in the anomalous map.\" ) # Run customization no_hetatm: bool = Field( False, description=\"Remove heteroatoms from the given model.\", flag_type=\"--\", rename_param=\"no-hetatm\", ) rigid_cycles: Optional[PositiveInt] = Field( # 10, description=\"Number of cycles of rigid-body refinement to perform.\", flag_type=\"--\", rename_param=\"rigid-cycles\", ) jelly: Optional[PositiveInt] = Field( # 4, description=\"Number of cycles of jelly-body refinement to perform.\", flag_type=\"--\", ) restr_cycles: Optional[PositiveInt] = Field( # 8, description=\"Number of cycles of refmac final refinement to perform.\", flag_type=\"--\", rename_param=\"restr-cycles\", ) lim_resolution: Optional[PositiveFloat] = Field( description=\"Limit the final resolution.\", flag_type=\"--\", rename_param=\"reso\" ) weight: Optional[str] = Field( # \"auto-weight\", description=\"The refmac matrix weight.\", flag_type=\"--\", ) mr_prog: Optional[str] = Field( # \"phaser\", description=\"Molecular replacement program. phaser or molrep.\", flag_type=\"--\", rename_param=\"mr-prog\", ) mr_num: Optional[Union[str, int]] = Field( # \"auto\", description=\"Number of molecules to use for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-num\", ) mr_reso: Optional[PositiveFloat] = Field( # 3.25, description=\"High resolution for molecular replacement. If >10 interpreted as eLLG.\", flag_type=\"--\", rename_param=\"mr-reso\", ) itof_prog: Optional[str] = Field( description=\"Program to calculate amplitudes. truncate, or ctruncate.\", flag_type=\"--\", rename_param=\"ItoF-prog\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return get_hkl_file return in_file @validator(\"out_dir\", always=True) def validate_out_dir(cls, out_dir: str, values: Dict[str, Any]) -> str: if out_dir == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return os.path.dirname(get_hkl_file) return out_dir FindOverlapXSSParameters Bases: TaskParameters TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. Source code in lute/io/models/smd.py class FindOverlapXSSParameters(TaskParameters): \"\"\"TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. \"\"\" class ExpConfig(BaseModel): det_name: str ipm_var: str scan_var: Union[str, List[str]] class Thresholds(BaseModel): min_Iscat: Union[int, float] min_ipm: Union[int, float] class AnalysisFlags(BaseModel): use_pyfai: bool = True use_asymls: bool = False exp_config: ExpConfig thresholds: Thresholds analysis_flags: AnalysisFlags FindPeaksPsocakeParameters Bases: ThirdPartyParameters Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPsocakeParameters(ThirdPartyParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" class SZParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description=\"SZ compression algorithm (qoz, sz3)\" ) binSize: int = Field(2, description=\"SZ compression's bin size paramater\") roiWindowSize: int = Field( 2, description=\"SZ compression's ROI window size paramater\" ) absError: float = Field(10, descriptionp=\"Maximum absolute error value\") executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) mca: str = Field( \"btl ^openib\", description=\"Mca option for the MPI executable\", flag_type=\"--\" ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) p_arg2: str = Field( \"findPeaksSZ.py\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\", ) d: str = Field(description=\"Detector name\", flag_type=\"-\") e: str = Field(\"\", description=\"Experiment name\", flag_type=\"-\") r: int = Field(-1, description=\"Run number\", flag_type=\"-\") outDir: str = Field( description=\"Output directory where .cxi will be saved\", flag_type=\"--\" ) algorithm: int = Field(1, description=\"PyAlgos algorithm to use\", flag_type=\"--\") alg_npix_min: float = Field( 1.0, description=\"PyAlgos algorithm's npix_min parameter\", flag_type=\"--\" ) alg_npix_max: float = Field( 45.0, description=\"PyAlgos algorithm's npix_max parameter\", flag_type=\"--\" ) alg_amax_thr: float = Field( 250.0, description=\"PyAlgos algorithm's amax_thr parameter\", flag_type=\"--\" ) alg_atot_thr: float = Field( 330.0, description=\"PyAlgos algorithm's atot_thr parameter\", flag_type=\"--\" ) alg_son_min: float = Field( 10.0, description=\"PyAlgos algorithm's son_min parameter\", flag_type=\"--\" ) alg1_thr_low: float = Field( 80.0, description=\"PyAlgos algorithm's thr_low parameter\", flag_type=\"--\" ) alg1_thr_high: float = Field( 270.0, description=\"PyAlgos algorithm's thr_high parameter\", flag_type=\"--\" ) alg1_rank: int = Field( 3, description=\"PyAlgos algorithm's rank parameter\", flag_type=\"--\" ) alg1_radius: int = Field( 3, description=\"PyAlgos algorithm's radius parameter\", flag_type=\"--\" ) alg1_dr: int = Field( 1, description=\"PyAlgos algorithm's dr parameter\", flag_type=\"--\" ) psanaMask_on: str = Field( \"True\", description=\"Whether psana's mask should be used\", flag_type=\"--\" ) psanaMask_calib: str = Field( \"True\", description=\"Psana mask's calib parameter\", flag_type=\"--\" ) psanaMask_status: str = Field( \"True\", description=\"Psana mask's status parameter\", flag_type=\"--\" ) psanaMask_edges: str = Field( \"True\", description=\"Psana mask's edges parameter\", flag_type=\"--\" ) psanaMask_central: str = Field( \"True\", description=\"Psana mask's central parameter\", flag_type=\"--\" ) psanaMask_unbond: str = Field( \"True\", description=\"Psana mask's unbond parameter\", flag_type=\"--\" ) psanaMask_unbondnrs: str = Field( \"True\", description=\"Psana mask's unbondnbrs parameter\", flag_type=\"--\" ) mask: str = Field( \"\", description=\"Path to an additional mask to apply\", flag_type=\"--\" ) clen: str = Field( description=\"Epics variable storing the camera length\", flag_type=\"--\" ) coffset: float = Field(0, description=\"Camera offset in m\", flag_type=\"--\") minPeaks: int = Field( 15, description=\"Minimum number of peaks to mark frame for indexing\", flag_type=\"--\", ) maxPeaks: int = Field( 15, description=\"Maximum number of peaks to mark frame for indexing\", flag_type=\"--\", ) minRes: int = Field( 0, description=\"Minimum peak resolution to mark frame for indexing \", flag_type=\"--\", ) sample: str = Field(\"\", description=\"Sample name\", flag_type=\"--\") instrument: Union[None, str] = Field( None, description=\"Instrument name\", flag_type=\"--\" ) pixelSize: float = Field(0.0, description=\"Pixel size\", flag_type=\"--\") auto: str = Field( \"False\", description=( \"Whether to automatically determine peak per event peak \" \"finding parameters\" ), flag_type=\"--\", ) detectorDistance: float = Field( 0.0, description=\"Detector distance from interaction point in m\", flag_type=\"--\" ) access: Literal[\"ana\", \"ffb\"] = Field( \"ana\", description=\"Data node type: {ana,ffb}\", flag_type=\"--\" ) szfile: str = Field(\"qoz.json\", description=\"Path to SZ's JSON configuration file\") lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"sz.json\", output_path=\"\", # Will want to change where this goes... ), description=\"Template information for the sz.json file\", ) sz_parameters: SZParameters = Field( description=\"Configuration parameters for SZ Compression\", flag_type=\"\" ) @validator(\"e\", always=True) def validate_e(cls, e: str, values: Dict[str, Any]) -> str: if e == \"\": return values[\"lute_config\"].experiment return e @validator(\"r\", always=True) def validate_r(cls, r: int, values: Dict[str, Any]) -> int: if r == -1: return values[\"lute_config\"].run return r @validator(\"lute_template_cfg\", always=True) def set_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"szfile\"] return lute_template_cfg @validator(\"sz_parameters\", always=True) def set_sz_compression_parameters( cls, sz_parameters: SZParameters, values: Dict[str, Any] ) -> None: values[\"compressor\"] = sz_parameters.compressor values[\"binSize\"] = sz_parameters.binSize values[\"roiWindowSize\"] = sz_parameters.roiWindowSize if sz_parameters.compressor == \"qoz\": values[\"pressio_opts\"] = { \"pressio:abs\": sz_parameters.absError, \"qoz\": {\"qoz:stride\": 8}, } else: values[\"pressio_opts\"] = {\"pressio:abs\": sz_parameters.absError} return None @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) directory: str = values[\"outDir\"] fname: str = f\"{exp}_{run:04d}.lst\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values Config Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_from_params: str = '' class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. FindPeaksPyAlgosParameters Bases: TaskParameters Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPyAlgosParameters(TaskParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" class SZCompressorParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description='Compression algorithm (\"qoz\" or \"sz3\")' ) abs_error: float = Field(10.0, description=\"Absolute error bound\") bin_size: int = Field(2, description=\"Bin size\") roi_window_size: int = Field( 9, description=\"Default window size\", ) outdir: str = Field( description=\"Output directory for cxi files\", ) n_events: int = Field( 0, description=\"Number of events to process (0 to process all events)\", ) det_name: str = Field( description=\"Psana name of the detector storing the image data\", ) event_receiver: Literal[\"evr0\", \"evr1\"] = Field( description=\"Event Receiver to be used: evr0 or evr1\", ) tag: str = Field( \"\", description=\"Tag to add to the output file names\", ) pv_camera_length: Union[str, float] = Field( \"\", description=\"PV associated with camera length \" \"(if a number, camera length directly)\", ) event_logic: bool = Field( False, description=\"True if only events with a specific event code should be \" \"processed. False if the event code should be ignored\", ) event_code: int = Field( 0, description=\"Required events code for events to be processed if event logic \" \"is True\", ) psana_mask: bool = Field( False, description=\"If True, apply mask from psana Detector object\", ) mask_file: Union[str, None] = Field( None, description=\"File with a custom mask to apply. If None, no custom mask is \" \"applied\", ) min_peaks: int = Field(2, description=\"Minimum number of peaks per image\") max_peaks: int = Field( 2048, description=\"Maximum number of peaks per image\", ) npix_min: int = Field( 2, description=\"Minimum number of pixels per peak\", ) npix_max: int = Field( 30, description=\"Maximum number of pixels per peak\", ) amax_thr: float = Field( 80.0, description=\"Minimum intensity threshold for starting a peak\", ) atot_thr: float = Field( 120.0, description=\"Minimum summed intensity threshold for pixel collection\", ) son_min: float = Field( 7.0, description=\"Minimum signal-to-noise ratio to be considered a peak\", ) peak_rank: int = Field( 3, description=\"Radius in which central peak pixel is a local maximum\", ) r0: float = Field( 3.0, description=\"Radius of ring for background evaluation in pixels\", ) dr: float = Field( 2.0, description=\"Width of ring for background evaluation in pixels\", ) nsigm: float = Field( 7.0, description=\"Intensity threshold to include pixel in connected group\", ) compression: Optional[SZCompressorParameters] = Field( None, description=\"Options for the SZ Compression Algorithm\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": fname: Path = ( Path(values[\"outdir\"]) / f\"{values['lute_config'].experiment}_{values['lute_config'].run}_\" f\"{values['tag']}.list\" ) return str(fname) return out_file Config Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. IndexCrystFELParameters Bases: ThirdPartyParameters Parameters for CrystFEL's indexamajig . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Source code in lute/io/models/sfx_index.py class IndexCrystFELParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html \"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/indexamajig\", description=\"CrystFEL's indexing binary.\", flag_type=\"\", ) # Basic options in_file: Optional[str] = Field( \"\", description=\"Path to input file.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) geometry: str = Field( \"\", description=\"Path to geometry file.\", flag_type=\"-\", rename_param=\"g\" ) zmq_input: Optional[str] = Field( description=\"ZMQ address to receive data over. `input` and `zmq-input` are mutually exclusive\", flag_type=\"--\", rename_param=\"zmq-input\", ) zmq_subscribe: Optional[str] = Field( # Can be used multiple times... description=\"Subscribe to ZMQ message of type `tag`\", flag_type=\"--\", rename_param=\"zmq-subscribe\", ) zmq_request: Optional[AnyUrl] = Field( description=\"Request new data over ZMQ by sending this value\", flag_type=\"--\", rename_param=\"zmq-request\", ) asapo_endpoint: Optional[str] = Field( description=\"ASAP::O endpoint. zmq-input and this are mutually exclusive.\", flag_type=\"--\", rename_param=\"asapo-endpoint\", ) asapo_token: Optional[str] = Field( description=\"ASAP::O authentication token.\", flag_type=\"--\", rename_param=\"asapo-token\", ) asapo_beamtime: Optional[str] = Field( description=\"ASAP::O beatime.\", flag_type=\"--\", rename_param=\"asapo-beamtime\", ) asapo_source: Optional[str] = Field( description=\"ASAP::O data source.\", flag_type=\"--\", rename_param=\"asapo-source\", ) asapo_group: Optional[str] = Field( description=\"ASAP::O consumer group.\", flag_type=\"--\", rename_param=\"asapo-group\", ) asapo_stream: Optional[str] = Field( description=\"ASAP::O stream.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) asapo_wait_for_stream: Optional[str] = Field( description=\"If ASAP::O stream does not exist, wait for it to appear.\", flag_type=\"--\", rename_param=\"asapo-wait-for-stream\", ) data_format: Optional[str] = Field( description=\"Specify format for ZMQ or ASAP::O. `msgpack`, `hdf5` or `seedee`.\", flag_type=\"--\", rename_param=\"data-format\", ) basename: bool = Field( False, description=\"Remove directory parts of filenames. Acts before prefix if prefix also given.\", flag_type=\"--\", ) prefix: Optional[str] = Field( description=\"Add a prefix to the filenames from the infile argument.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) nthreads: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of threads to use. See also `max_indexer_threads`.\", flag_type=\"-\", rename_param=\"j\", ) no_check_prefix: bool = Field( False, description=\"Don't attempt to correct the prefix if it seems incorrect.\", flag_type=\"--\", rename_param=\"no-check-prefix\", ) highres: Optional[float] = Field( description=\"Mark all pixels greater than `x` has bad.\", flag_type=\"--\" ) profile: bool = Field( False, description=\"Display timing data to monitor performance.\", flag_type=\"--\" ) temp_dir: Optional[str] = Field( description=\"Specify a path for the temp files folder.\", flag_type=\"--\", rename_param=\"temp-dir\", ) wait_for_file: conint(gt=-2) = Field( 0, description=\"Wait at most `x` seconds for a file to be created. A value of -1 means wait forever.\", flag_type=\"--\", rename_param=\"wait-for-file\", ) no_image_data: bool = Field( False, description=\"Load only the metadata, no iamges. Can check indexability without high data requirements.\", flag_type=\"--\", rename_param=\"no-image-data\", ) # Peak-finding options # .... # Indexing options indexing: Optional[str] = Field( description=\"Comma-separated list of supported indexing algorithms to use. Default is to automatically detect.\", flag_type=\"--\", ) cell_file: Optional[str] = Field( description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) tolerance: str = Field( \"5,5,5,1.5\", description=( \"Tolerances (in percent) for unit cell comparison. \" \"Comma-separated list a,b,c,angle. Default=5,5,5,1.5\" ), flag_type=\"--\", ) no_check_cell: bool = Field( False, description=\"Do not check cell parameters against unit cell. Replaces '-raw' method.\", flag_type=\"--\", rename_param=\"no-check-cell\", ) no_check_peaks: bool = Field( False, description=\"Do not verify peaks are accounted for by solution.\", flag_type=\"--\", rename_param=\"no-check-peaks\", ) multi: bool = Field( False, description=\"Enable multi-lattice indexing.\", flag_type=\"--\" ) wavelength_estimate: Optional[float] = Field( description=\"Estimate for X-ray wavelength. Required for some methods.\", flag_type=\"--\", rename_param=\"wavelength-estimate\", ) camera_length_estimate: Optional[float] = Field( description=\"Estimate for camera distance. Required for some methods.\", flag_type=\"--\", rename_param=\"camera-length-estimate\", ) max_indexer_threads: Optional[PositiveInt] = Field( # 1, description=\"Some indexing algos can use multiple threads. In addition to image-based.\", flag_type=\"--\", rename_param=\"max-indexer-threads\", ) no_retry: bool = Field( False, description=\"Do not remove weak peaks and try again.\", flag_type=\"--\", rename_param=\"no-retry\", ) no_refine: bool = Field( False, description=\"Skip refinement step.\", flag_type=\"--\", rename_param=\"no-refine\", ) no_revalidate: bool = Field( False, description=\"Skip revalidation step.\", flag_type=\"--\", rename_param=\"no-revalidate\", ) # TakeTwo specific parameters taketwo_member_threshold: Optional[PositiveInt] = Field( # 20, description=\"Minimum number of vectors to consider.\", flag_type=\"--\", rename_param=\"taketwo-member-threshold\", ) taketwo_len_tolerance: Optional[PositiveFloat] = Field( # 0.001, description=\"TakeTwo length tolerance in Angstroms.\", flag_type=\"--\", rename_param=\"taketwo-len-tolerance\", ) taketwo_angle_tolerance: Optional[PositiveFloat] = Field( # 0.6, description=\"TakeTwo angle tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-angle-tolerance\", ) taketwo_trace_tolerance: Optional[PositiveFloat] = Field( # 3, description=\"Matrix trace tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-trace-tolerance\", ) # Felix-specific parameters # felix_domega # felix-fraction-max-visits # felix-max-internal-angle # felix-max-uniqueness # felix-min-completeness # felix-min-visits # felix-num-voxels # felix-sigma # felix-tthrange-max # felix-tthrange-min # XGANDALF-specific parameters xgandalf_sampling_pitch: Optional[NonNegativeInt] = Field( # 6, description=\"Density of reciprocal space sampling.\", flag_type=\"--\", rename_param=\"xgandalf-sampling-pitch\", ) xgandalf_grad_desc_iterations: Optional[NonNegativeInt] = Field( # 4, description=\"Number of gradient descent iterations.\", flag_type=\"--\", rename_param=\"xgandalf-grad-desc-iterations\", ) xgandalf_tolerance: Optional[PositiveFloat] = Field( # 0.02, description=\"Relative tolerance of lattice vectors\", flag_type=\"--\", rename_param=\"xgandalf-tolerance\", ) xgandalf_no_deviation_from_provided_cell: Optional[bool] = Field( description=\"Found unit cell must match provided.\", flag_type=\"--\", rename_param=\"xgandalf-no-deviation-from-provided-cell\", ) xgandalf_min_lattice_vector_length: Optional[PositiveFloat] = Field( # 30, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-min-lattice-vector-length\", ) xgandalf_max_lattice_vector_length: Optional[PositiveFloat] = Field( # 250, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-max-lattice-vector-length\", ) xgandalf_max_peaks: Optional[PositiveInt] = Field( # 250, description=\"Maximum number of peaks to use for indexing.\", flag_type=\"--\", rename_param=\"xgandalf-max-peaks\", ) xgandalf_fast_execution: bool = Field( False, description=\"Shortcut to set sampling-pitch=2, and grad-desc-iterations=3.\", flag_type=\"--\", rename_param=\"xgandalf-fast-execution\", ) # pinkIndexer parameters # ... # asdf_fast: bool = Field(False, description=\"Enable fast mode for asdf. 3x faster for 7% loss in accuracy.\", flag_type=\"--\", rename_param=\"asdf-fast\") # Integration parameters integration: str = Field( \"rings-nocen\", description=\"Method for integrating reflections.\", flag_type=\"--\" ) fix_profile_radius: Optional[float] = Field( description=\"Fix the profile radius (m^{-1})\", flag_type=\"--\", rename_param=\"fix-profile-radius\", ) fix_divergence: Optional[float] = Field( 0, description=\"Fix the divergence (rad, full angle).\", flag_type=\"--\", rename_param=\"fix-divergence\", ) int_radius: str = Field( \"4,5,7\", description=\"Inner, middle, and outer radii for 3-ring integration.\", flag_type=\"--\", rename_param=\"int-radius\", ) int_diag: str = Field( \"none\", description=\"Show detailed information on integration when condition is met.\", flag_type=\"--\", rename_param=\"int-diag\", ) push_res: str = Field( \"infinity\", description=\"Integrate `x` higher than apparent resolution limit (nm-1).\", flag_type=\"--\", rename_param=\"push-res\", ) overpredict: bool = Field( False, description=\"Over-predict reflections. Maybe useful with post-refinement.\", flag_type=\"--\", ) cell_parameters_only: bool = Field( False, description=\"Do not predict refletions at all\", flag_type=\"--\" ) # Output parameters no_non_hits_in_stream: bool = Field( False, description=\"Exclude non-hits from the stream file.\", flag_type=\"--\", rename_param=\"no-non-hits-in-stream\", ) copy_hheader: Optional[str] = Field( description=\"Copy information from header in the image to output stream.\", flag_type=\"--\", rename_param=\"copy-hheader\", ) no_peaks_in_stream: bool = Field( False, description=\"Do not record peaks in stream file.\", flag_type=\"--\", rename_param=\"no-peaks-in-stream\", ) no_refls_in_stream: bool = Field( False, description=\"Do not record reflections in stream.\", flag_type=\"--\", rename_param=\"no-refls-in-stream\", ) serial_offset: Optional[PositiveInt] = Field( description=\"Start numbering at `x` instead of 1.\", flag_type=\"--\", rename_param=\"serial-offset\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": filename: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPyAlgos\", \"out_file\" ) if filename is None: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) tag: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"tag\" ) out_dir: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"outDir\" ) if out_dir is not None: fname: str = f\"{out_dir}/{exp}_{run:04d}\" if tag is not None: fname = f\"{fname}_{tag}\" return f\"{fname}.lst\" else: return filename return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": expmt: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) work_dir: str = values[\"lute_config\"].work_dir fname: str = f\"{expmt}_r{run:04d}.stream\" return f\"{work_dir}/{fname}\" return out_file Config Bases: Config Source code in lute/io/models/sfx_index.py class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. ManipulateHKLParameters Bases: ThirdPartyParameters Parameters for CrystFEL's get_hkl for manipulating lists of reflections. This Task is predominantly used internally to convert hkl to mtz files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class ManipulateHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `get_hkl` for manipulating lists of reflections. This Task is predominantly used internally to convert `hkl` to `mtz` files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/get_hkl\", description=\"CrystFEL's reflection manipulation binary.\", flag_type=\"\", ) in_file: str = Field( \"\", description=\"Path to input HKL file.\", flag_type=\"-\", rename_param=\"i\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) output_format: str = Field( \"mtz\", description=\"Output format. One of mtz, mtz-bij, or xds. Otherwise CrystFEL format.\", flag_type=\"--\", rename_param=\"output-format\", ) expand: Optional[str] = Field( description=\"Reflections will be expanded to fill asymmetric unit of specified point group.\", flag_type=\"--\", ) # Reducing reflections to higher symmetry twin: Optional[str] = Field( description=\"Reflections equivalent to specified point group will have intensities summed.\", flag_type=\"--\", ) no_need_all_parts: Optional[bool] = Field( description=\"Use with --twin to allow reflections missing a 'twin mate' to be written out.\", flag_type=\"--\", rename_param=\"no-need-all-parts\", ) # Noise - Add to data noise: Optional[bool] = Field( description=\"Generate 10% uniform noise.\", flag_type=\"--\" ) poisson: Optional[bool] = Field( description=\"Generate Poisson noise. Intensities assumed to be A.U.\", flag_type=\"--\", ) adu_per_photon: Optional[int] = Field( description=\"Use with --poisson to convert A.U. to photons.\", flag_type=\"--\", rename_param=\"adu-per-photon\", ) # Remove duplicate reflections trim_centrics: Optional[bool] = Field( description=\"Duplicated reflections (according to symmetry) are removed.\", flag_type=\"--\", ) # Restrict to template file template: Optional[str] = Field( description=\"Only reflections which also appear in specified file are written out.\", flag_type=\"--\", ) # Multiplicity multiplicity: Optional[bool] = Field( description=\"Reflections are multiplied by their symmetric multiplicites.\", flag_type=\"--\", ) # Resolution cutoffs cutoff_angstroms: Optional[Union[str, int, float]] = Field( description=\"Either n, or n1,n2,n3. For n, reflections < n are removed. For n1,n2,n3 anisotropic trunction performed at separate resolution limits for a*, b*, c*.\", flag_type=\"--\", rename_param=\"cutoff-angstroms\", ) lowres: Optional[float] = Field( description=\"Remove reflections with d > n\", flag_type=\"--\" ) highres: Optional[float] = Field( description=\"Synonym for first form of --cutoff-angstroms\" ) reindex: Optional[str] = Field( description=\"Reindex according to specified operator. E.g. k,h,-l.\", flag_type=\"--\", ) # Override input symmetry symmetry: Optional[str] = Field( description=\"Point group symmetry to use to override. Almost always OMIT this option.\", flag_type=\"--\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: return partialator_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: mtz_out: str = partialator_file.split(\".\")[0] mtz_out = f\"{mtz_out}.mtz\" return mtz_out return out_file @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. MergePartialatorParameters Bases: ThirdPartyParameters Parameters for CrystFEL's partialator . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class MergePartialatorParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `partialator`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/partialator\", description=\"CrystFEL's Partialator binary.\", flag_type=\"\", ) in_file: Optional[str] = Field( \"\", description=\"Path to input stream.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) symmetry: str = Field(description=\"Point group symmetry.\", flag_type=\"--\") niter: Optional[int] = Field( description=\"Number of cycles of scaling and post-refinement.\", flag_type=\"-\", rename_param=\"n\", ) no_scale: Optional[bool] = Field( description=\"Disable scaling.\", flag_type=\"--\", rename_param=\"no-scale\" ) no_Bscale: Optional[bool] = Field( description=\"Disable Debye-Waller part of scaling.\", flag_type=\"--\", rename_param=\"no-Bscale\", ) no_pr: Optional[bool] = Field( description=\"Disable orientation model.\", flag_type=\"--\", rename_param=\"no-pr\" ) no_deltacchalf: Optional[bool] = Field( description=\"Disable rejection based on deltaCC1/2.\", flag_type=\"--\", rename_param=\"no-deltacchalf\", ) model: str = Field( \"unity\", description=\"Partiality model. Options: xsphere, unity, offset, ggpm.\", flag_type=\"--\", ) nthreads: int = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of parallel analyses.\", flag_type=\"-\", rename_param=\"j\", ) polarisation: Optional[str] = Field( description=\"Specification of incident polarisation. Refer to CrystFEL docs for more info.\", flag_type=\"--\", ) no_polarisation: Optional[bool] = Field( description=\"Synonym for --polarisation=none\", flag_type=\"--\", rename_param=\"no-polarisation\", ) max_adu: Optional[float] = Field( description=\"Maximum intensity of reflection to include.\", flag_type=\"--\", rename_param=\"max-adu\", ) min_res: Optional[float] = Field( description=\"Only include crystals diffracting to a minimum resolution.\", flag_type=\"--\", rename_param=\"min-res\", ) min_measurements: int = Field( 2, description=\"Include a reflection only if it appears a minimum number of times.\", flag_type=\"--\", rename_param=\"min-measurements\", ) push_res: Optional[float] = Field( description=\"Merge reflections up to higher than the apparent resolution limit.\", flag_type=\"--\", rename_param=\"push-res\", ) start_after: int = Field( 0, description=\"Ignore the first n crystals.\", flag_type=\"--\", rename_param=\"start-after\", ) stop_after: int = Field( 0, description=\"Stop after processing n crystals. 0 means process all.\", flag_type=\"--\", rename_param=\"stop-after\", ) no_free: Optional[bool] = Field( description=\"Disable cross-validation. Testing ONLY.\", flag_type=\"--\", rename_param=\"no-free\", ) custom_split: Optional[str] = Field( description=\"Read a set of filenames, event and dataset IDs from a filename.\", flag_type=\"--\", rename_param=\"custom-split\", ) max_rel_B: float = Field( 100, description=\"Reject crystals if |relB| > n sq Angstroms.\", flag_type=\"--\", rename_param=\"max-rel-B\", ) output_every_cycle: bool = Field( False, description=\"Write per-crystal params after every refinement cycle.\", flag_type=\"--\", rename_param=\"output-every-cycle\", ) no_logs: bool = Field( False, description=\"Do not write logs needed for plots, maps and graphs.\", flag_type=\"--\", rename_param=\"no-logs\", ) set_symmetry: Optional[str] = Field( description=\"Set the apparent symmetry of the crystals to a point group.\", flag_type=\"-\", rename_param=\"w\", ) operator: Optional[str] = Field( description=\"Specify an ambiguity operator. E.g. k,h,-l.\", flag_type=\"--\" ) force_bandwidth: Optional[float] = Field( description=\"Set X-ray bandwidth. As percent, e.g. 0.0013 (0.13%).\", flag_type=\"--\", rename_param=\"force-bandwidth\", ) force_radius: Optional[float] = Field( description=\"Set the initial profile radius (nm-1).\", flag_type=\"--\", rename_param=\"force-radius\", ) force_lambda: Optional[float] = Field( description=\"Set the wavelength. In Angstroms.\", flag_type=\"--\", rename_param=\"force-lambda\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ConcatenateStreamFiles\", \"out_file\", ) if stream_file: return stream_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": in_file: str = values[\"in_file\"] if in_file: tag: str = in_file.split(\".\")[0] return f\"{tag}.hkl\" else: return \"partialator.hkl\" return out_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. RunSHELXCParameters Bases: ThirdPartyParameters Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html Source code in lute/io/models/sfx_solve.py class RunSHELXCParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/shelxc\", description=\"CCP4 SHELXC. Generates input files for SHELXD/SHELXE.\", flag_type=\"\", ) placeholder: str = Field( \"xx\", description=\"Placeholder filename stem.\", flag_type=\"\" ) in_file: str = Field( \"\", description=\"Input file for SHELXC with reflections AND proper records.\", flag_type=\"\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": # get_hkl needed to be run to produce an XDS format file... xds_format_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if xds_format_file: in_file = xds_format_file if in_file[0] != \"<\": # Need to add a redirection for this program # Runs like `shelxc xx <input_file.xds` in_file = f\"<{in_file}\" return in_file SubmitSMDParameters Bases: ThirdPartyParameters Parameters for running smalldata to produce reduced HDF5 files. Source code in lute/io/models/smd.py class SubmitSMDParameters(ThirdPartyParameters): \"\"\"Parameters for running smalldata to produce reduced HDF5 files.\"\"\" class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) m: str = Field( \"mpi4py.run\", description=\"Python option to execute a module's contents as __main__ module.\", flag_type=\"-\", ) producer: str = Field( \"\", description=\"Path to the SmallData producer Python script.\", flag_type=\"\" ) run: str = Field( os.environ.get(\"RUN_NUM\", \"\"), description=\"DAQ Run Number.\", flag_type=\"--\" ) experiment: str = Field( os.environ.get(\"EXPERIMENT\", \"\"), description=\"LCLS Experiment Number.\", flag_type=\"--\", ) stn: NonNegativeInt = Field(0, description=\"Hutch endstation.\", flag_type=\"--\") nevents: int = Field( int(1e9), description=\"Number of events to process.\", flag_type=\"--\" ) directory: Optional[str] = Field( None, description=\"Optional output directory. If None, will be in ${EXP_FOLDER}/hdf5/smalldata.\", flag_type=\"--\", ) ## Need mechanism to set result_from_param=True ... gather_interval: PositiveInt = Field( 25, description=\"Number of events to collect at a time.\", flag_type=\"--\" ) norecorder: bool = Field( False, description=\"Whether to ignore recorder streams.\", flag_type=\"--\" ) url: HttpUrl = Field( \"https://pswww.slac.stanford.edu/ws-auth/lgbk\", description=\"Base URL for eLog posting.\", flag_type=\"--\", ) epicsAll: bool = Field( False, description=\"Whether to store all EPICS PVs. Use with care.\", flag_type=\"--\", ) full: bool = Field( False, description=\"Whether to store all data. Use with EXTRA care.\", flag_type=\"--\", ) fullSum: bool = Field( False, description=\"Whether to store sums for all area detector images.\", flag_type=\"--\", ) default: bool = Field( False, description=\"Whether to store only the default minimal set of data.\", flag_type=\"--\", ) image: bool = Field( False, description=\"Whether to save everything as images. Use with care.\", flag_type=\"--\", ) tiff: bool = Field( False, description=\"Whether to save all images as a single TIFF. Use with EXTRA care.\", flag_type=\"--\", ) centerpix: bool = Field( False, description=\"Whether to mask center pixels for Epix10k2M detectors.\", flag_type=\"--\", ) postRuntable: bool = Field( False, description=\"Whether to post run tables. Also used as a trigger for summary jobs.\", flag_type=\"--\", ) wait: bool = Field( False, description=\"Whether to wait for a file to appear.\", flag_type=\"--\" ) xtcav: bool = Field( False, description=\"Whether to add XTCAV processing to the HDF5 generation.\", flag_type=\"--\", ) noarch: bool = Field( False, description=\"Whether to not use archiver data.\", flag_type=\"--\" ) lute_template_cfg: TemplateConfig = TemplateConfig(template_name=\"\", output_path=\"\") @validator(\"producer\", always=True) def validate_producer_path(cls, producer: str) -> str: return producer @validator(\"lute_template_cfg\", always=True) def use_producer( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if not lute_template_cfg.output_path: lute_template_cfg.output_path = values[\"producer\"] return lute_template_cfg @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment hutch: str = exp[:3] run: int = int(values[\"lute_config\"].run) directory: Optional[str] = values[\"directory\"] if directory is None: directory = f\"/sdf/data/lcls/ds/{hutch}/{exp}/hdf5/smalldata\" fname: str = f\"{exp}_Run{run:04d}.h5\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values Config Bases: Config Identical to super-class Config but includes a result. Source code in lute/io/models/smd.py class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_from_params: str = '' class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. TaskParameters Bases: BaseSettings Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). Source code in lute/io/models/base.py class TaskParameters(BaseSettings): \"\"\"Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note: Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). \"\"\" class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" lute_config: AnalysisHeader Config Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. Only used if set_result==True result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if set_result==True Source code in lute/io/models/base.py class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" impl_schemas: Optional[str] = None class-attribute instance-attribute Schema specification for output result. Will be passed to TaskResult. result_from_params: Optional[str] = None class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. result_summary: Optional[str] = None class-attribute instance-attribute Format a TaskResult.summary from output. run_directory: Optional[str] = None class-attribute instance-attribute Set the directory that the Task is run from. set_result: bool = False class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. TemplateConfig Bases: BaseModel Parameters used for templating of third party configuration files. Attributes: template_name ( str ) \u2013 The name of the template to use. This template must live in config/templates . output_path ( str ) \u2013 The FULL path, including filename to write the rendered template to. Source code in lute/io/models/base.py class TemplateConfig(BaseModel): \"\"\"Parameters used for templating of third party configuration files. Attributes: template_name (str): The name of the template to use. This template must live in `config/templates`. output_path (str): The FULL path, including filename to write the rendered template to. \"\"\" template_name: str output_path: str TemplateParameters Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable params. The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the params Field. Source code in lute/io/models/base.py @dataclass class TemplateParameters: \"\"\"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable `params.` The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the `params` Field. \"\"\" params: Any TestBinaryErrParameters Bases: ThirdPartyParameters Same as TestBinary, but exits with non-zero code. Source code in lute/io/models/tests.py class TestBinaryErrParameters(ThirdPartyParameters): \"\"\"Same as TestBinary, but exits with non-zero code.\"\"\" executable: str = Field( \"/sdf/home/d/dorlhiac/test_tasks/test_threads_err\", description=\"Multi-threaded tes tbinary with non-zero exit code.\", ) p_arg1: int = Field(1, description=\"Number of threads.\") TestParameters Bases: TaskParameters Parameters for the test Task Test . Source code in lute/io/models/tests.py class TestParameters(TaskParameters): \"\"\"Parameters for the test Task `Test`.\"\"\" float_var: float = Field(0.01, description=\"A floating point number.\") str_var: str = Field(\"test\", description=\"A string.\") class CompoundVar(BaseModel): int_var: int = 1 dict_var: Dict[str, str] = {\"a\": \"b\"} compound_var: CompoundVar = Field( description=( \"A compound parameter - consists of a `int_var` (int) and `dict_var`\" \" (Dict[str, str]).\" ) ) throw_error: bool = Field( False, description=\"If `True`, raise an exception to test error handling.\" ) ThirdPartyParameters Bases: TaskParameters Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. Source code in lute/io/models/base.py class ThirdPartyParameters(TaskParameters): \"\"\"Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. \"\"\" class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" # lute_template_cfg: TemplateConfig @root_validator(pre=False) def extra_fields_to_thirdparty(cls, values: Dict[str, Any]): for key in values: if key not in cls.__fields__: values[key] = TemplateParameters(values[key]) return values Config Bases: Config Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base TaskParameters.Config class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. ThirdPartyTask-specific ( Optional [ str ] ) \u2013 extra ( str ) \u2013 \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq ( bool ) \u2013 False. If True, \"short\" command-line args are passed as -x=arg . ThirdPartyTask-specific. long_flags_use_eq ( bool ) \u2013 False. If True, \"long\" command-line args are passed as --long=arg . ThirdPartyTask-specific. Source code in lute/io/models/base.py class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = False class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. short_flags_use_eq: bool = False class-attribute instance-attribute Whether short command-line arguments are passed like -x=arg . parse_config(task_name='test', config_path='') Parse a configuration file and validate the contents. Parameters: task_name ( str , default: 'test' ) \u2013 Name of the specific task that will be run. config_path ( str , default: '' ) \u2013 Path to the configuration file. Returns: params ( TaskParameters ) \u2013 A TaskParameters object of validated task-specific parameters. Parameters are accessed with \"dot\" notation. E.g. params.param1 . Raises: ValidationError \u2013 Raised if there are problems with the configuration file. Passed through from Pydantic. Source code in lute/io/config.py def parse_config(task_name: str = \"test\", config_path: str = \"\") -> TaskParameters: \"\"\"Parse a configuration file and validate the contents. Args: task_name (str): Name of the specific task that will be run. config_path (str): Path to the configuration file. Returns: params (TaskParameters): A TaskParameters object of validated task-specific parameters. Parameters are accessed with \"dot\" notation. E.g. `params.param1`. Raises: ValidationError: Raised if there are problems with the configuration file. Passed through from Pydantic. \"\"\" task_config_name: str = f\"{task_name}Parameters\" with open(config_path, \"r\") as f: docs: Iterator[Dict[str, Any]] = yaml.load_all(stream=f, Loader=yaml.FullLoader) header: Dict[str, Any] = next(docs) config: Dict[str, Any] = next(docs) substitute_variables(header, header) substitute_variables(header, config) LUTE_DEBUG_EXIT(\"LUTE_DEBUG_EXIT_AT_YAML\", pprint.pformat(config)) lute_config: Dict[str, AnalysisHeader] = {\"lute_config\": AnalysisHeader(**header)} try: task_config: Dict[str, Any] = dict(config[task_name]) lute_config.update(task_config) except KeyError as err: warnings.warn( ( f\"{task_name} has no parameter definitions in YAML file.\" \" Attempting default parameter initialization.\" ) ) parsed_parameters: TaskParameters = globals()[task_config_name](**lute_config) return parsed_parameters substitute_variables(header, config, curr_key=None) Performs variable substitutions on a dictionary read from config YAML file. Can be used to define input parameters in terms of other input parameters. This is similar to functionality employed by validators for parameters in the specific Task models, but is intended to be more accessible to users. Variable substitutions are defined using a minimal syntax from Jinja: {{ experiment }} defines a substitution of the variable experiment . The characters {{ }} can be escaped if the literal symbols are needed in place. For example, a path to a file can be defined in terms of experiment and run values in the config file: MyTask: experiment: myexp run: 2 special_file: /path/to/{{ experiment }}/{{ run }}/file.inp Acceptable variables for substitutions are values defined elsewhere in the YAML file. Environment variables can also be used if prefaced with a $ character. E.g. to get the experiment from an environment variable: MyTask: run: 2 special_file: /path/to/{{ $EXPERIMENT }}/{{ run }}/file.inp Parameters: config ( Dict [ str , Any ] ) \u2013 A dictionary of parsed configuration. curr_key ( Optional [ str ] , default: None ) \u2013 Used to keep track of recursion level when scanning through iterable items in the config dictionary. Returns: subbed_config ( Dict [ str , Any ] ) \u2013 The config dictionary after substitutions have been made. May be identical to the input if no substitutions are needed. Source code in lute/io/config.py def substitute_variables( header: Dict[str, Any], config: Dict[str, Any], curr_key: Optional[str] = None ) -> None: \"\"\"Performs variable substitutions on a dictionary read from config YAML file. Can be used to define input parameters in terms of other input parameters. This is similar to functionality employed by validators for parameters in the specific Task models, but is intended to be more accessible to users. Variable substitutions are defined using a minimal syntax from Jinja: {{ experiment }} defines a substitution of the variable `experiment`. The characters `{{ }}` can be escaped if the literal symbols are needed in place. For example, a path to a file can be defined in terms of experiment and run values in the config file: MyTask: experiment: myexp run: 2 special_file: /path/to/{{ experiment }}/{{ run }}/file.inp Acceptable variables for substitutions are values defined elsewhere in the YAML file. Environment variables can also be used if prefaced with a `$` character. E.g. to get the experiment from an environment variable: MyTask: run: 2 special_file: /path/to/{{ $EXPERIMENT }}/{{ run }}/file.inp Args: config (Dict[str, Any]): A dictionary of parsed configuration. curr_key (Optional[str]): Used to keep track of recursion level when scanning through iterable items in the config dictionary. Returns: subbed_config (Dict[str, Any]): The config dictionary after substitutions have been made. May be identical to the input if no substitutions are needed. \"\"\" _sub_pattern = r\"\\{\\{[^}{]*\\}\\}\" iterable: Dict[str, Any] = config if curr_key is not None: # Need to handle nested levels by interpreting curr_key keys_by_level: List[str] = curr_key.split(\".\") for key in keys_by_level: iterable = iterable[key] else: ... # iterable = config for param, value in iterable.items(): if isinstance(value, dict): new_key: str if curr_key is None: new_key = param else: new_key = f\"{curr_key}.{param}\" substitute_variables(header, config, curr_key=new_key) elif isinstance(value, list): ... # Scalars str - we skip numeric types elif isinstance(value, str): matches: List[str] = re.findall(_sub_pattern, value) for m in matches: key_to_sub_maybe_with_fmt: List[str] = m[2:-2].strip().split(\":\") key_to_sub: str = key_to_sub_maybe_with_fmt[0] fmt: Optional[str] = None if len(key_to_sub_maybe_with_fmt) == 2: fmt = key_to_sub_maybe_with_fmt[1] sub: Any if key_to_sub[0] == \"$\": sub = os.getenv(key_to_sub[1:], None) if sub is None: print( f\"Environment variable {key_to_sub[1:]} not found! Cannot substitute in YAML config!\", flush=True, ) continue # substitutions from env vars will be strings, so convert back # to numeric in order to perform formatting later on (e.g. {var:04d}) sub = _check_str_numeric(sub) else: try: sub = config for key in key_to_sub.split(\".\"): sub = sub[key] except KeyError: sub = header[key_to_sub] pattern: str = ( m.replace(\"{{\", r\"\\{\\{\").replace(\"}}\", r\"\\}\\}\").replace(\"$\", r\"\\$\") ) if fmt is not None: sub = f\"{sub:{fmt}}\" else: sub = f\"{sub}\" iterable[param] = re.sub(pattern, sub, iterable[param]) # Reconvert back to numeric values if needed... iterable[param] = _check_str_numeric(iterable[param])","title":"config"},{"location":"source/io/config/#io.config.AnalysisHeader","text":"Bases: BaseModel Header information for LUTE analysis runs. Source code in lute/io/models/base.py class AnalysisHeader(BaseModel): \"\"\"Header information for LUTE analysis runs.\"\"\" title: str = Field( \"LUTE Task Configuration\", description=\"Description of the configuration or experiment.\", ) experiment: str = Field(\"\", description=\"Experiment.\") run: Union[str, int] = Field(\"\", description=\"Data acquisition run.\") date: str = Field(\"1970/01/01\", description=\"Start date of analysis.\") lute_version: Union[float, str] = Field( 0.1, description=\"Version of LUTE used for analysis.\" ) task_timeout: PositiveInt = Field( 600, description=( \"Time in seconds until a task times out. Should be slightly shorter\" \" than job timeout if using a job manager (e.g. SLURM).\" ), ) work_dir: str = Field(\"\", description=\"Main working directory for LUTE.\") @validator(\"work_dir\", always=True) def validate_work_dir(cls, directory: str, values: Dict[str, Any]) -> str: work_dir: str if directory == \"\": std_work_dir = ( f\"/sdf/data/lcls/ds/{values['experiment'][:3]}/\" f\"{values['experiment']}/scratch\" ) work_dir = std_work_dir else: work_dir = directory # Check existence and permissions if not os.path.exists(work_dir): raise ValueError(f\"Working Directory: {work_dir} does not exist!\") if not os.access(work_dir, os.W_OK): # Need write access for database, files etc. raise ValueError(f\"Not write access for working directory: {work_dir}!\") return work_dir @validator(\"run\", always=True) def validate_run( cls, run: Union[str, int], values: Dict[str, Any] ) -> Union[str, int]: if run == \"\": # From Airflow RUN_NUM should have Format \"RUN_DATETIME\" - Num is first part run_time: str = os.environ.get(\"RUN_NUM\", \"\") if run_time != \"\": return int(run_time.split(\"_\")[0]) return run @validator(\"experiment\", always=True) def validate_experiment(cls, experiment: str, values: Dict[str, Any]) -> str: if experiment == \"\": arp_exp: str = os.environ.get(\"EXPERIMENT\", \"EXPX00000\") return arp_exp return experiment","title":"AnalysisHeader"},{"location":"source/io/config/#io.config.CompareHKLParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's compare_hkl for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class CompareHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `compare_hkl` for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/compare_hkl\", description=\"CrystFEL's reflection comparison binary.\", flag_type=\"\", ) in_files: Optional[str] = Field( \"\", description=\"Path to input HKLs. Space-separated list of 2. Use output of partialator e.g.\", flag_type=\"\", ) ## Need mechanism to set is_result=True ... symmetry: str = Field(\"\", description=\"Point group symmetry.\", flag_type=\"--\") cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) fom: str = Field( \"Rsplit\", description=\"Specify figure of merit to calculate.\", flag_type=\"--\" ) nshells: int = Field(10, description=\"Use n resolution shells.\", flag_type=\"--\") # NEED A NEW CASE FOR THIS -> Boolean flag, no arg, one hyphen... # fix_unity: bool = Field( # False, # description=\"Fix scale factors to unity.\", # flag_type=\"-\", # rename_param=\"u\", # ) shell_file: str = Field( \"\", description=\"Write the statistics in resolution shells to a file.\", flag_type=\"--\", rename_param=\"shell-file\", is_result=True, ) ignore_negs: bool = Field( False, description=\"Ignore reflections with negative reflections.\", flag_type=\"--\", rename_param=\"ignore-negs\", ) zero_negs: bool = Field( False, description=\"Set negative intensities to 0.\", flag_type=\"--\", rename_param=\"zero-negs\", ) sigma_cutoff: Optional[Union[float, int, str]] = Field( # \"-infinity\", description=\"Discard reflections with I/sigma(I) < n. -infinity means no cutoff.\", flag_type=\"--\", rename_param=\"sigma-cutoff\", ) rmin: Optional[float] = Field( description=\"Low resolution cutoff of 1/d (m-1). Use this or --lowres NOT both.\", flag_type=\"--\", ) lowres: Optional[float] = Field( descirption=\"Low resolution cutoff in Angstroms. Use this or --rmin NOT both.\", flag_type=\"--\", ) rmax: Optional[float] = Field( description=\"High resolution cutoff in 1/d (m-1). Use this or --highres NOT both.\", flag_type=\"--\", ) highres: Optional[float] = Field( description=\"High resolution cutoff in Angstroms. Use this or --rmax NOT both.\", flag_type=\"--\", ) @validator(\"in_files\", always=True) def validate_in_files(cls, in_files: str, values: Dict[str, Any]) -> str: if in_files == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: hkls: str = f\"{partialator_file}1 {partialator_file}2\" return hkls return in_files @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file @validator(\"symmetry\", always=True) def validate_symmetry(cls, symmetry: str, values: Dict[str, Any]) -> str: if symmetry == \"\": partialator_sym: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"symmetry\" ) if partialator_sym: return partialator_sym return symmetry @validator(\"shell_file\", always=True) def validate_shell_file(cls, shell_file: str, values: Dict[str, Any]) -> str: if shell_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: shells_out: str = partialator_file.split(\".\")[0] shells_out = f\"{shells_out}_{values['fom']}_n{values['nshells']}.dat\" return shells_out return shell_file","title":"CompareHKLParameters"},{"location":"source/io/config/#io.config.CompareHKLParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.CompareHKLParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/config/#io.config.CompareHKLParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.ConcatenateStreamFilesParameters","text":"Bases: TaskParameters Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. Source code in lute/io/models/sfx_index.py class ConcatenateStreamFilesParameters(TaskParameters): \"\"\"Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" in_file: str = Field( \"\", description=\"Root of directory tree storing stream files to merge.\", ) tag: Optional[str] = Field( \"\", description=\"Tag identifying the stream files to merge.\", ) out_file: str = Field( \"\", description=\"Path to merged output stream file.\", is_result=True ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_dir: str = str(Path(stream_file).parent) return stream_dir return in_file @validator(\"tag\", always=True) def validate_tag(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_tag: str = Path(stream_file).name.split(\"_\")[0] return stream_tag return tag @validator(\"out_file\", always=True) def validate_out_file(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_out_file: str = str( Path(values[\"in_file\"]).parent / f\"{values['tag'].stream}\" ) return stream_out_file return tag","title":"ConcatenateStreamFilesParameters"},{"location":"source/io/config/#io.config.ConcatenateStreamFilesParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_index.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.ConcatenateStreamFilesParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.DimpleSolveParameters","text":"Bases: ThirdPartyParameters Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ Source code in lute/io/models/sfx_solve.py class DimpleSolveParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/dimple\", description=\"CCP4 Dimple for solving structures with MR.\", flag_type=\"\", ) # Positional requirements - all required. in_file: str = Field( \"\", description=\"Path to input mtz.\", flag_type=\"\", ) pdb: str = Field(\"\", description=\"Path to a PDB.\", flag_type=\"\") out_dir: str = Field(\"\", description=\"Output DIRECTORY.\", flag_type=\"\") # Most used options mr_thresh: PositiveFloat = Field( 0.4, description=\"Threshold for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-when-r\", ) slow: Optional[bool] = Field( False, description=\"Perform more refinement.\", flag_type=\"--\" ) # Other options (IO) hklout: str = Field( \"final.mtz\", description=\"Output mtz file name.\", flag_type=\"--\" ) xyzout: str = Field( \"final.pdb\", description=\"Output PDB file name.\", flag_type=\"--\" ) icolumn: Optional[str] = Field( # \"IMEAN\", description=\"Name for the I column.\", flag_type=\"--\", ) sigicolumn: Optional[str] = Field( # \"SIG<ICOL>\", description=\"Name for the Sig<I> column.\", flag_type=\"--\", ) fcolumn: Optional[str] = Field( # \"F\", description=\"Name for the F column.\", flag_type=\"--\", ) sigfcolumn: Optional[str] = Field( # \"F\", description=\"Name for the Sig<F> column.\", flag_type=\"--\", ) libin: Optional[str] = Field( description=\"Ligand descriptions for refmac (LIBIN).\", flag_type=\"--\" ) refmac_key: Optional[str] = Field( description=\"Extra Refmac keywords to use in refinement.\", flag_type=\"--\", rename_param=\"refmac-key\", ) free_r_flags: Optional[str] = Field( description=\"Path to a mtz file with freeR flags.\", flag_type=\"--\", rename_param=\"free-r-flags\", ) freecolumn: Optional[Union[int, float]] = Field( # 0, description=\"Refree column with an optional value.\", flag_type=\"--\", ) img_format: Optional[str] = Field( description=\"Format of generated images. (png, jpeg, none).\", flag_type=\"-\", rename_param=\"f\", ) white_bg: bool = Field( False, description=\"Use a white background in Coot and in images.\", flag_type=\"--\", rename_param=\"white-bg\", ) no_cleanup: bool = Field( False, description=\"Retain intermediate files.\", flag_type=\"--\", rename_param=\"no-cleanup\", ) # Calculations no_blob_search: bool = Field( False, description=\"Do not search for unmodelled blobs.\", flag_type=\"--\", rename_param=\"no-blob-search\", ) anode: bool = Field( False, description=\"Use SHELX/AnoDe to find peaks in the anomalous map.\" ) # Run customization no_hetatm: bool = Field( False, description=\"Remove heteroatoms from the given model.\", flag_type=\"--\", rename_param=\"no-hetatm\", ) rigid_cycles: Optional[PositiveInt] = Field( # 10, description=\"Number of cycles of rigid-body refinement to perform.\", flag_type=\"--\", rename_param=\"rigid-cycles\", ) jelly: Optional[PositiveInt] = Field( # 4, description=\"Number of cycles of jelly-body refinement to perform.\", flag_type=\"--\", ) restr_cycles: Optional[PositiveInt] = Field( # 8, description=\"Number of cycles of refmac final refinement to perform.\", flag_type=\"--\", rename_param=\"restr-cycles\", ) lim_resolution: Optional[PositiveFloat] = Field( description=\"Limit the final resolution.\", flag_type=\"--\", rename_param=\"reso\" ) weight: Optional[str] = Field( # \"auto-weight\", description=\"The refmac matrix weight.\", flag_type=\"--\", ) mr_prog: Optional[str] = Field( # \"phaser\", description=\"Molecular replacement program. phaser or molrep.\", flag_type=\"--\", rename_param=\"mr-prog\", ) mr_num: Optional[Union[str, int]] = Field( # \"auto\", description=\"Number of molecules to use for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-num\", ) mr_reso: Optional[PositiveFloat] = Field( # 3.25, description=\"High resolution for molecular replacement. If >10 interpreted as eLLG.\", flag_type=\"--\", rename_param=\"mr-reso\", ) itof_prog: Optional[str] = Field( description=\"Program to calculate amplitudes. truncate, or ctruncate.\", flag_type=\"--\", rename_param=\"ItoF-prog\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return get_hkl_file return in_file @validator(\"out_dir\", always=True) def validate_out_dir(cls, out_dir: str, values: Dict[str, Any]) -> str: if out_dir == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return os.path.dirname(get_hkl_file) return out_dir","title":"DimpleSolveParameters"},{"location":"source/io/config/#io.config.FindOverlapXSSParameters","text":"Bases: TaskParameters TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. Source code in lute/io/models/smd.py class FindOverlapXSSParameters(TaskParameters): \"\"\"TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. \"\"\" class ExpConfig(BaseModel): det_name: str ipm_var: str scan_var: Union[str, List[str]] class Thresholds(BaseModel): min_Iscat: Union[int, float] min_ipm: Union[int, float] class AnalysisFlags(BaseModel): use_pyfai: bool = True use_asymls: bool = False exp_config: ExpConfig thresholds: Thresholds analysis_flags: AnalysisFlags","title":"FindOverlapXSSParameters"},{"location":"source/io/config/#io.config.FindPeaksPsocakeParameters","text":"Bases: ThirdPartyParameters Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPsocakeParameters(ThirdPartyParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" class SZParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description=\"SZ compression algorithm (qoz, sz3)\" ) binSize: int = Field(2, description=\"SZ compression's bin size paramater\") roiWindowSize: int = Field( 2, description=\"SZ compression's ROI window size paramater\" ) absError: float = Field(10, descriptionp=\"Maximum absolute error value\") executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) mca: str = Field( \"btl ^openib\", description=\"Mca option for the MPI executable\", flag_type=\"--\" ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) p_arg2: str = Field( \"findPeaksSZ.py\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\", ) d: str = Field(description=\"Detector name\", flag_type=\"-\") e: str = Field(\"\", description=\"Experiment name\", flag_type=\"-\") r: int = Field(-1, description=\"Run number\", flag_type=\"-\") outDir: str = Field( description=\"Output directory where .cxi will be saved\", flag_type=\"--\" ) algorithm: int = Field(1, description=\"PyAlgos algorithm to use\", flag_type=\"--\") alg_npix_min: float = Field( 1.0, description=\"PyAlgos algorithm's npix_min parameter\", flag_type=\"--\" ) alg_npix_max: float = Field( 45.0, description=\"PyAlgos algorithm's npix_max parameter\", flag_type=\"--\" ) alg_amax_thr: float = Field( 250.0, description=\"PyAlgos algorithm's amax_thr parameter\", flag_type=\"--\" ) alg_atot_thr: float = Field( 330.0, description=\"PyAlgos algorithm's atot_thr parameter\", flag_type=\"--\" ) alg_son_min: float = Field( 10.0, description=\"PyAlgos algorithm's son_min parameter\", flag_type=\"--\" ) alg1_thr_low: float = Field( 80.0, description=\"PyAlgos algorithm's thr_low parameter\", flag_type=\"--\" ) alg1_thr_high: float = Field( 270.0, description=\"PyAlgos algorithm's thr_high parameter\", flag_type=\"--\" ) alg1_rank: int = Field( 3, description=\"PyAlgos algorithm's rank parameter\", flag_type=\"--\" ) alg1_radius: int = Field( 3, description=\"PyAlgos algorithm's radius parameter\", flag_type=\"--\" ) alg1_dr: int = Field( 1, description=\"PyAlgos algorithm's dr parameter\", flag_type=\"--\" ) psanaMask_on: str = Field( \"True\", description=\"Whether psana's mask should be used\", flag_type=\"--\" ) psanaMask_calib: str = Field( \"True\", description=\"Psana mask's calib parameter\", flag_type=\"--\" ) psanaMask_status: str = Field( \"True\", description=\"Psana mask's status parameter\", flag_type=\"--\" ) psanaMask_edges: str = Field( \"True\", description=\"Psana mask's edges parameter\", flag_type=\"--\" ) psanaMask_central: str = Field( \"True\", description=\"Psana mask's central parameter\", flag_type=\"--\" ) psanaMask_unbond: str = Field( \"True\", description=\"Psana mask's unbond parameter\", flag_type=\"--\" ) psanaMask_unbondnrs: str = Field( \"True\", description=\"Psana mask's unbondnbrs parameter\", flag_type=\"--\" ) mask: str = Field( \"\", description=\"Path to an additional mask to apply\", flag_type=\"--\" ) clen: str = Field( description=\"Epics variable storing the camera length\", flag_type=\"--\" ) coffset: float = Field(0, description=\"Camera offset in m\", flag_type=\"--\") minPeaks: int = Field( 15, description=\"Minimum number of peaks to mark frame for indexing\", flag_type=\"--\", ) maxPeaks: int = Field( 15, description=\"Maximum number of peaks to mark frame for indexing\", flag_type=\"--\", ) minRes: int = Field( 0, description=\"Minimum peak resolution to mark frame for indexing \", flag_type=\"--\", ) sample: str = Field(\"\", description=\"Sample name\", flag_type=\"--\") instrument: Union[None, str] = Field( None, description=\"Instrument name\", flag_type=\"--\" ) pixelSize: float = Field(0.0, description=\"Pixel size\", flag_type=\"--\") auto: str = Field( \"False\", description=( \"Whether to automatically determine peak per event peak \" \"finding parameters\" ), flag_type=\"--\", ) detectorDistance: float = Field( 0.0, description=\"Detector distance from interaction point in m\", flag_type=\"--\" ) access: Literal[\"ana\", \"ffb\"] = Field( \"ana\", description=\"Data node type: {ana,ffb}\", flag_type=\"--\" ) szfile: str = Field(\"qoz.json\", description=\"Path to SZ's JSON configuration file\") lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"sz.json\", output_path=\"\", # Will want to change where this goes... ), description=\"Template information for the sz.json file\", ) sz_parameters: SZParameters = Field( description=\"Configuration parameters for SZ Compression\", flag_type=\"\" ) @validator(\"e\", always=True) def validate_e(cls, e: str, values: Dict[str, Any]) -> str: if e == \"\": return values[\"lute_config\"].experiment return e @validator(\"r\", always=True) def validate_r(cls, r: int, values: Dict[str, Any]) -> int: if r == -1: return values[\"lute_config\"].run return r @validator(\"lute_template_cfg\", always=True) def set_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"szfile\"] return lute_template_cfg @validator(\"sz_parameters\", always=True) def set_sz_compression_parameters( cls, sz_parameters: SZParameters, values: Dict[str, Any] ) -> None: values[\"compressor\"] = sz_parameters.compressor values[\"binSize\"] = sz_parameters.binSize values[\"roiWindowSize\"] = sz_parameters.roiWindowSize if sz_parameters.compressor == \"qoz\": values[\"pressio_opts\"] = { \"pressio:abs\": sz_parameters.absError, \"qoz\": {\"qoz:stride\": 8}, } else: values[\"pressio_opts\"] = {\"pressio:abs\": sz_parameters.absError} return None @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) directory: str = values[\"outDir\"] fname: str = f\"{exp}_{run:04d}.lst\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values","title":"FindPeaksPsocakeParameters"},{"location":"source/io/config/#io.config.FindPeaksPsocakeParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.FindPeaksPsocakeParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/config/#io.config.FindPeaksPsocakeParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.FindPeaksPyAlgosParameters","text":"Bases: TaskParameters Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPyAlgosParameters(TaskParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" class SZCompressorParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description='Compression algorithm (\"qoz\" or \"sz3\")' ) abs_error: float = Field(10.0, description=\"Absolute error bound\") bin_size: int = Field(2, description=\"Bin size\") roi_window_size: int = Field( 9, description=\"Default window size\", ) outdir: str = Field( description=\"Output directory for cxi files\", ) n_events: int = Field( 0, description=\"Number of events to process (0 to process all events)\", ) det_name: str = Field( description=\"Psana name of the detector storing the image data\", ) event_receiver: Literal[\"evr0\", \"evr1\"] = Field( description=\"Event Receiver to be used: evr0 or evr1\", ) tag: str = Field( \"\", description=\"Tag to add to the output file names\", ) pv_camera_length: Union[str, float] = Field( \"\", description=\"PV associated with camera length \" \"(if a number, camera length directly)\", ) event_logic: bool = Field( False, description=\"True if only events with a specific event code should be \" \"processed. False if the event code should be ignored\", ) event_code: int = Field( 0, description=\"Required events code for events to be processed if event logic \" \"is True\", ) psana_mask: bool = Field( False, description=\"If True, apply mask from psana Detector object\", ) mask_file: Union[str, None] = Field( None, description=\"File with a custom mask to apply. If None, no custom mask is \" \"applied\", ) min_peaks: int = Field(2, description=\"Minimum number of peaks per image\") max_peaks: int = Field( 2048, description=\"Maximum number of peaks per image\", ) npix_min: int = Field( 2, description=\"Minimum number of pixels per peak\", ) npix_max: int = Field( 30, description=\"Maximum number of pixels per peak\", ) amax_thr: float = Field( 80.0, description=\"Minimum intensity threshold for starting a peak\", ) atot_thr: float = Field( 120.0, description=\"Minimum summed intensity threshold for pixel collection\", ) son_min: float = Field( 7.0, description=\"Minimum signal-to-noise ratio to be considered a peak\", ) peak_rank: int = Field( 3, description=\"Radius in which central peak pixel is a local maximum\", ) r0: float = Field( 3.0, description=\"Radius of ring for background evaluation in pixels\", ) dr: float = Field( 2.0, description=\"Width of ring for background evaluation in pixels\", ) nsigm: float = Field( 7.0, description=\"Intensity threshold to include pixel in connected group\", ) compression: Optional[SZCompressorParameters] = Field( None, description=\"Options for the SZ Compression Algorithm\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": fname: Path = ( Path(values[\"outdir\"]) / f\"{values['lute_config'].experiment}_{values['lute_config'].run}_\" f\"{values['tag']}.list\" ) return str(fname) return out_file","title":"FindPeaksPyAlgosParameters"},{"location":"source/io/config/#io.config.FindPeaksPyAlgosParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.FindPeaksPyAlgosParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.IndexCrystFELParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's indexamajig . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Source code in lute/io/models/sfx_index.py class IndexCrystFELParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html \"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/indexamajig\", description=\"CrystFEL's indexing binary.\", flag_type=\"\", ) # Basic options in_file: Optional[str] = Field( \"\", description=\"Path to input file.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) geometry: str = Field( \"\", description=\"Path to geometry file.\", flag_type=\"-\", rename_param=\"g\" ) zmq_input: Optional[str] = Field( description=\"ZMQ address to receive data over. `input` and `zmq-input` are mutually exclusive\", flag_type=\"--\", rename_param=\"zmq-input\", ) zmq_subscribe: Optional[str] = Field( # Can be used multiple times... description=\"Subscribe to ZMQ message of type `tag`\", flag_type=\"--\", rename_param=\"zmq-subscribe\", ) zmq_request: Optional[AnyUrl] = Field( description=\"Request new data over ZMQ by sending this value\", flag_type=\"--\", rename_param=\"zmq-request\", ) asapo_endpoint: Optional[str] = Field( description=\"ASAP::O endpoint. zmq-input and this are mutually exclusive.\", flag_type=\"--\", rename_param=\"asapo-endpoint\", ) asapo_token: Optional[str] = Field( description=\"ASAP::O authentication token.\", flag_type=\"--\", rename_param=\"asapo-token\", ) asapo_beamtime: Optional[str] = Field( description=\"ASAP::O beatime.\", flag_type=\"--\", rename_param=\"asapo-beamtime\", ) asapo_source: Optional[str] = Field( description=\"ASAP::O data source.\", flag_type=\"--\", rename_param=\"asapo-source\", ) asapo_group: Optional[str] = Field( description=\"ASAP::O consumer group.\", flag_type=\"--\", rename_param=\"asapo-group\", ) asapo_stream: Optional[str] = Field( description=\"ASAP::O stream.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) asapo_wait_for_stream: Optional[str] = Field( description=\"If ASAP::O stream does not exist, wait for it to appear.\", flag_type=\"--\", rename_param=\"asapo-wait-for-stream\", ) data_format: Optional[str] = Field( description=\"Specify format for ZMQ or ASAP::O. `msgpack`, `hdf5` or `seedee`.\", flag_type=\"--\", rename_param=\"data-format\", ) basename: bool = Field( False, description=\"Remove directory parts of filenames. Acts before prefix if prefix also given.\", flag_type=\"--\", ) prefix: Optional[str] = Field( description=\"Add a prefix to the filenames from the infile argument.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) nthreads: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of threads to use. See also `max_indexer_threads`.\", flag_type=\"-\", rename_param=\"j\", ) no_check_prefix: bool = Field( False, description=\"Don't attempt to correct the prefix if it seems incorrect.\", flag_type=\"--\", rename_param=\"no-check-prefix\", ) highres: Optional[float] = Field( description=\"Mark all pixels greater than `x` has bad.\", flag_type=\"--\" ) profile: bool = Field( False, description=\"Display timing data to monitor performance.\", flag_type=\"--\" ) temp_dir: Optional[str] = Field( description=\"Specify a path for the temp files folder.\", flag_type=\"--\", rename_param=\"temp-dir\", ) wait_for_file: conint(gt=-2) = Field( 0, description=\"Wait at most `x` seconds for a file to be created. A value of -1 means wait forever.\", flag_type=\"--\", rename_param=\"wait-for-file\", ) no_image_data: bool = Field( False, description=\"Load only the metadata, no iamges. Can check indexability without high data requirements.\", flag_type=\"--\", rename_param=\"no-image-data\", ) # Peak-finding options # .... # Indexing options indexing: Optional[str] = Field( description=\"Comma-separated list of supported indexing algorithms to use. Default is to automatically detect.\", flag_type=\"--\", ) cell_file: Optional[str] = Field( description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) tolerance: str = Field( \"5,5,5,1.5\", description=( \"Tolerances (in percent) for unit cell comparison. \" \"Comma-separated list a,b,c,angle. Default=5,5,5,1.5\" ), flag_type=\"--\", ) no_check_cell: bool = Field( False, description=\"Do not check cell parameters against unit cell. Replaces '-raw' method.\", flag_type=\"--\", rename_param=\"no-check-cell\", ) no_check_peaks: bool = Field( False, description=\"Do not verify peaks are accounted for by solution.\", flag_type=\"--\", rename_param=\"no-check-peaks\", ) multi: bool = Field( False, description=\"Enable multi-lattice indexing.\", flag_type=\"--\" ) wavelength_estimate: Optional[float] = Field( description=\"Estimate for X-ray wavelength. Required for some methods.\", flag_type=\"--\", rename_param=\"wavelength-estimate\", ) camera_length_estimate: Optional[float] = Field( description=\"Estimate for camera distance. Required for some methods.\", flag_type=\"--\", rename_param=\"camera-length-estimate\", ) max_indexer_threads: Optional[PositiveInt] = Field( # 1, description=\"Some indexing algos can use multiple threads. In addition to image-based.\", flag_type=\"--\", rename_param=\"max-indexer-threads\", ) no_retry: bool = Field( False, description=\"Do not remove weak peaks and try again.\", flag_type=\"--\", rename_param=\"no-retry\", ) no_refine: bool = Field( False, description=\"Skip refinement step.\", flag_type=\"--\", rename_param=\"no-refine\", ) no_revalidate: bool = Field( False, description=\"Skip revalidation step.\", flag_type=\"--\", rename_param=\"no-revalidate\", ) # TakeTwo specific parameters taketwo_member_threshold: Optional[PositiveInt] = Field( # 20, description=\"Minimum number of vectors to consider.\", flag_type=\"--\", rename_param=\"taketwo-member-threshold\", ) taketwo_len_tolerance: Optional[PositiveFloat] = Field( # 0.001, description=\"TakeTwo length tolerance in Angstroms.\", flag_type=\"--\", rename_param=\"taketwo-len-tolerance\", ) taketwo_angle_tolerance: Optional[PositiveFloat] = Field( # 0.6, description=\"TakeTwo angle tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-angle-tolerance\", ) taketwo_trace_tolerance: Optional[PositiveFloat] = Field( # 3, description=\"Matrix trace tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-trace-tolerance\", ) # Felix-specific parameters # felix_domega # felix-fraction-max-visits # felix-max-internal-angle # felix-max-uniqueness # felix-min-completeness # felix-min-visits # felix-num-voxels # felix-sigma # felix-tthrange-max # felix-tthrange-min # XGANDALF-specific parameters xgandalf_sampling_pitch: Optional[NonNegativeInt] = Field( # 6, description=\"Density of reciprocal space sampling.\", flag_type=\"--\", rename_param=\"xgandalf-sampling-pitch\", ) xgandalf_grad_desc_iterations: Optional[NonNegativeInt] = Field( # 4, description=\"Number of gradient descent iterations.\", flag_type=\"--\", rename_param=\"xgandalf-grad-desc-iterations\", ) xgandalf_tolerance: Optional[PositiveFloat] = Field( # 0.02, description=\"Relative tolerance of lattice vectors\", flag_type=\"--\", rename_param=\"xgandalf-tolerance\", ) xgandalf_no_deviation_from_provided_cell: Optional[bool] = Field( description=\"Found unit cell must match provided.\", flag_type=\"--\", rename_param=\"xgandalf-no-deviation-from-provided-cell\", ) xgandalf_min_lattice_vector_length: Optional[PositiveFloat] = Field( # 30, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-min-lattice-vector-length\", ) xgandalf_max_lattice_vector_length: Optional[PositiveFloat] = Field( # 250, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-max-lattice-vector-length\", ) xgandalf_max_peaks: Optional[PositiveInt] = Field( # 250, description=\"Maximum number of peaks to use for indexing.\", flag_type=\"--\", rename_param=\"xgandalf-max-peaks\", ) xgandalf_fast_execution: bool = Field( False, description=\"Shortcut to set sampling-pitch=2, and grad-desc-iterations=3.\", flag_type=\"--\", rename_param=\"xgandalf-fast-execution\", ) # pinkIndexer parameters # ... # asdf_fast: bool = Field(False, description=\"Enable fast mode for asdf. 3x faster for 7% loss in accuracy.\", flag_type=\"--\", rename_param=\"asdf-fast\") # Integration parameters integration: str = Field( \"rings-nocen\", description=\"Method for integrating reflections.\", flag_type=\"--\" ) fix_profile_radius: Optional[float] = Field( description=\"Fix the profile radius (m^{-1})\", flag_type=\"--\", rename_param=\"fix-profile-radius\", ) fix_divergence: Optional[float] = Field( 0, description=\"Fix the divergence (rad, full angle).\", flag_type=\"--\", rename_param=\"fix-divergence\", ) int_radius: str = Field( \"4,5,7\", description=\"Inner, middle, and outer radii for 3-ring integration.\", flag_type=\"--\", rename_param=\"int-radius\", ) int_diag: str = Field( \"none\", description=\"Show detailed information on integration when condition is met.\", flag_type=\"--\", rename_param=\"int-diag\", ) push_res: str = Field( \"infinity\", description=\"Integrate `x` higher than apparent resolution limit (nm-1).\", flag_type=\"--\", rename_param=\"push-res\", ) overpredict: bool = Field( False, description=\"Over-predict reflections. Maybe useful with post-refinement.\", flag_type=\"--\", ) cell_parameters_only: bool = Field( False, description=\"Do not predict refletions at all\", flag_type=\"--\" ) # Output parameters no_non_hits_in_stream: bool = Field( False, description=\"Exclude non-hits from the stream file.\", flag_type=\"--\", rename_param=\"no-non-hits-in-stream\", ) copy_hheader: Optional[str] = Field( description=\"Copy information from header in the image to output stream.\", flag_type=\"--\", rename_param=\"copy-hheader\", ) no_peaks_in_stream: bool = Field( False, description=\"Do not record peaks in stream file.\", flag_type=\"--\", rename_param=\"no-peaks-in-stream\", ) no_refls_in_stream: bool = Field( False, description=\"Do not record reflections in stream.\", flag_type=\"--\", rename_param=\"no-refls-in-stream\", ) serial_offset: Optional[PositiveInt] = Field( description=\"Start numbering at `x` instead of 1.\", flag_type=\"--\", rename_param=\"serial-offset\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": filename: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPyAlgos\", \"out_file\" ) if filename is None: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) tag: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"tag\" ) out_dir: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"outDir\" ) if out_dir is not None: fname: str = f\"{out_dir}/{exp}_{run:04d}\" if tag is not None: fname = f\"{fname}_{tag}\" return f\"{fname}.lst\" else: return filename return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": expmt: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) work_dir: str = values[\"lute_config\"].work_dir fname: str = f\"{expmt}_r{run:04d}.stream\" return f\"{work_dir}/{fname}\" return out_file","title":"IndexCrystFELParameters"},{"location":"source/io/config/#io.config.IndexCrystFELParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_index.py class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.IndexCrystFELParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/config/#io.config.IndexCrystFELParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.ManipulateHKLParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's get_hkl for manipulating lists of reflections. This Task is predominantly used internally to convert hkl to mtz files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class ManipulateHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `get_hkl` for manipulating lists of reflections. This Task is predominantly used internally to convert `hkl` to `mtz` files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/get_hkl\", description=\"CrystFEL's reflection manipulation binary.\", flag_type=\"\", ) in_file: str = Field( \"\", description=\"Path to input HKL file.\", flag_type=\"-\", rename_param=\"i\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) output_format: str = Field( \"mtz\", description=\"Output format. One of mtz, mtz-bij, or xds. Otherwise CrystFEL format.\", flag_type=\"--\", rename_param=\"output-format\", ) expand: Optional[str] = Field( description=\"Reflections will be expanded to fill asymmetric unit of specified point group.\", flag_type=\"--\", ) # Reducing reflections to higher symmetry twin: Optional[str] = Field( description=\"Reflections equivalent to specified point group will have intensities summed.\", flag_type=\"--\", ) no_need_all_parts: Optional[bool] = Field( description=\"Use with --twin to allow reflections missing a 'twin mate' to be written out.\", flag_type=\"--\", rename_param=\"no-need-all-parts\", ) # Noise - Add to data noise: Optional[bool] = Field( description=\"Generate 10% uniform noise.\", flag_type=\"--\" ) poisson: Optional[bool] = Field( description=\"Generate Poisson noise. Intensities assumed to be A.U.\", flag_type=\"--\", ) adu_per_photon: Optional[int] = Field( description=\"Use with --poisson to convert A.U. to photons.\", flag_type=\"--\", rename_param=\"adu-per-photon\", ) # Remove duplicate reflections trim_centrics: Optional[bool] = Field( description=\"Duplicated reflections (according to symmetry) are removed.\", flag_type=\"--\", ) # Restrict to template file template: Optional[str] = Field( description=\"Only reflections which also appear in specified file are written out.\", flag_type=\"--\", ) # Multiplicity multiplicity: Optional[bool] = Field( description=\"Reflections are multiplied by their symmetric multiplicites.\", flag_type=\"--\", ) # Resolution cutoffs cutoff_angstroms: Optional[Union[str, int, float]] = Field( description=\"Either n, or n1,n2,n3. For n, reflections < n are removed. For n1,n2,n3 anisotropic trunction performed at separate resolution limits for a*, b*, c*.\", flag_type=\"--\", rename_param=\"cutoff-angstroms\", ) lowres: Optional[float] = Field( description=\"Remove reflections with d > n\", flag_type=\"--\" ) highres: Optional[float] = Field( description=\"Synonym for first form of --cutoff-angstroms\" ) reindex: Optional[str] = Field( description=\"Reindex according to specified operator. E.g. k,h,-l.\", flag_type=\"--\", ) # Override input symmetry symmetry: Optional[str] = Field( description=\"Point group symmetry to use to override. Almost always OMIT this option.\", flag_type=\"--\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: return partialator_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: mtz_out: str = partialator_file.split(\".\")[0] mtz_out = f\"{mtz_out}.mtz\" return mtz_out return out_file @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file","title":"ManipulateHKLParameters"},{"location":"source/io/config/#io.config.ManipulateHKLParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.ManipulateHKLParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/config/#io.config.ManipulateHKLParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.MergePartialatorParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's partialator . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class MergePartialatorParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `partialator`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/partialator\", description=\"CrystFEL's Partialator binary.\", flag_type=\"\", ) in_file: Optional[str] = Field( \"\", description=\"Path to input stream.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) symmetry: str = Field(description=\"Point group symmetry.\", flag_type=\"--\") niter: Optional[int] = Field( description=\"Number of cycles of scaling and post-refinement.\", flag_type=\"-\", rename_param=\"n\", ) no_scale: Optional[bool] = Field( description=\"Disable scaling.\", flag_type=\"--\", rename_param=\"no-scale\" ) no_Bscale: Optional[bool] = Field( description=\"Disable Debye-Waller part of scaling.\", flag_type=\"--\", rename_param=\"no-Bscale\", ) no_pr: Optional[bool] = Field( description=\"Disable orientation model.\", flag_type=\"--\", rename_param=\"no-pr\" ) no_deltacchalf: Optional[bool] = Field( description=\"Disable rejection based on deltaCC1/2.\", flag_type=\"--\", rename_param=\"no-deltacchalf\", ) model: str = Field( \"unity\", description=\"Partiality model. Options: xsphere, unity, offset, ggpm.\", flag_type=\"--\", ) nthreads: int = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of parallel analyses.\", flag_type=\"-\", rename_param=\"j\", ) polarisation: Optional[str] = Field( description=\"Specification of incident polarisation. Refer to CrystFEL docs for more info.\", flag_type=\"--\", ) no_polarisation: Optional[bool] = Field( description=\"Synonym for --polarisation=none\", flag_type=\"--\", rename_param=\"no-polarisation\", ) max_adu: Optional[float] = Field( description=\"Maximum intensity of reflection to include.\", flag_type=\"--\", rename_param=\"max-adu\", ) min_res: Optional[float] = Field( description=\"Only include crystals diffracting to a minimum resolution.\", flag_type=\"--\", rename_param=\"min-res\", ) min_measurements: int = Field( 2, description=\"Include a reflection only if it appears a minimum number of times.\", flag_type=\"--\", rename_param=\"min-measurements\", ) push_res: Optional[float] = Field( description=\"Merge reflections up to higher than the apparent resolution limit.\", flag_type=\"--\", rename_param=\"push-res\", ) start_after: int = Field( 0, description=\"Ignore the first n crystals.\", flag_type=\"--\", rename_param=\"start-after\", ) stop_after: int = Field( 0, description=\"Stop after processing n crystals. 0 means process all.\", flag_type=\"--\", rename_param=\"stop-after\", ) no_free: Optional[bool] = Field( description=\"Disable cross-validation. Testing ONLY.\", flag_type=\"--\", rename_param=\"no-free\", ) custom_split: Optional[str] = Field( description=\"Read a set of filenames, event and dataset IDs from a filename.\", flag_type=\"--\", rename_param=\"custom-split\", ) max_rel_B: float = Field( 100, description=\"Reject crystals if |relB| > n sq Angstroms.\", flag_type=\"--\", rename_param=\"max-rel-B\", ) output_every_cycle: bool = Field( False, description=\"Write per-crystal params after every refinement cycle.\", flag_type=\"--\", rename_param=\"output-every-cycle\", ) no_logs: bool = Field( False, description=\"Do not write logs needed for plots, maps and graphs.\", flag_type=\"--\", rename_param=\"no-logs\", ) set_symmetry: Optional[str] = Field( description=\"Set the apparent symmetry of the crystals to a point group.\", flag_type=\"-\", rename_param=\"w\", ) operator: Optional[str] = Field( description=\"Specify an ambiguity operator. E.g. k,h,-l.\", flag_type=\"--\" ) force_bandwidth: Optional[float] = Field( description=\"Set X-ray bandwidth. As percent, e.g. 0.0013 (0.13%).\", flag_type=\"--\", rename_param=\"force-bandwidth\", ) force_radius: Optional[float] = Field( description=\"Set the initial profile radius (nm-1).\", flag_type=\"--\", rename_param=\"force-radius\", ) force_lambda: Optional[float] = Field( description=\"Set the wavelength. In Angstroms.\", flag_type=\"--\", rename_param=\"force-lambda\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ConcatenateStreamFiles\", \"out_file\", ) if stream_file: return stream_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": in_file: str = values[\"in_file\"] if in_file: tag: str = in_file.split(\".\")[0] return f\"{tag}.hkl\" else: return \"partialator.hkl\" return out_file","title":"MergePartialatorParameters"},{"location":"source/io/config/#io.config.MergePartialatorParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.MergePartialatorParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/config/#io.config.MergePartialatorParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.RunSHELXCParameters","text":"Bases: ThirdPartyParameters Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html Source code in lute/io/models/sfx_solve.py class RunSHELXCParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/shelxc\", description=\"CCP4 SHELXC. Generates input files for SHELXD/SHELXE.\", flag_type=\"\", ) placeholder: str = Field( \"xx\", description=\"Placeholder filename stem.\", flag_type=\"\" ) in_file: str = Field( \"\", description=\"Input file for SHELXC with reflections AND proper records.\", flag_type=\"\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": # get_hkl needed to be run to produce an XDS format file... xds_format_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if xds_format_file: in_file = xds_format_file if in_file[0] != \"<\": # Need to add a redirection for this program # Runs like `shelxc xx <input_file.xds` in_file = f\"<{in_file}\" return in_file","title":"RunSHELXCParameters"},{"location":"source/io/config/#io.config.SubmitSMDParameters","text":"Bases: ThirdPartyParameters Parameters for running smalldata to produce reduced HDF5 files. Source code in lute/io/models/smd.py class SubmitSMDParameters(ThirdPartyParameters): \"\"\"Parameters for running smalldata to produce reduced HDF5 files.\"\"\" class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) m: str = Field( \"mpi4py.run\", description=\"Python option to execute a module's contents as __main__ module.\", flag_type=\"-\", ) producer: str = Field( \"\", description=\"Path to the SmallData producer Python script.\", flag_type=\"\" ) run: str = Field( os.environ.get(\"RUN_NUM\", \"\"), description=\"DAQ Run Number.\", flag_type=\"--\" ) experiment: str = Field( os.environ.get(\"EXPERIMENT\", \"\"), description=\"LCLS Experiment Number.\", flag_type=\"--\", ) stn: NonNegativeInt = Field(0, description=\"Hutch endstation.\", flag_type=\"--\") nevents: int = Field( int(1e9), description=\"Number of events to process.\", flag_type=\"--\" ) directory: Optional[str] = Field( None, description=\"Optional output directory. If None, will be in ${EXP_FOLDER}/hdf5/smalldata.\", flag_type=\"--\", ) ## Need mechanism to set result_from_param=True ... gather_interval: PositiveInt = Field( 25, description=\"Number of events to collect at a time.\", flag_type=\"--\" ) norecorder: bool = Field( False, description=\"Whether to ignore recorder streams.\", flag_type=\"--\" ) url: HttpUrl = Field( \"https://pswww.slac.stanford.edu/ws-auth/lgbk\", description=\"Base URL for eLog posting.\", flag_type=\"--\", ) epicsAll: bool = Field( False, description=\"Whether to store all EPICS PVs. Use with care.\", flag_type=\"--\", ) full: bool = Field( False, description=\"Whether to store all data. Use with EXTRA care.\", flag_type=\"--\", ) fullSum: bool = Field( False, description=\"Whether to store sums for all area detector images.\", flag_type=\"--\", ) default: bool = Field( False, description=\"Whether to store only the default minimal set of data.\", flag_type=\"--\", ) image: bool = Field( False, description=\"Whether to save everything as images. Use with care.\", flag_type=\"--\", ) tiff: bool = Field( False, description=\"Whether to save all images as a single TIFF. Use with EXTRA care.\", flag_type=\"--\", ) centerpix: bool = Field( False, description=\"Whether to mask center pixels for Epix10k2M detectors.\", flag_type=\"--\", ) postRuntable: bool = Field( False, description=\"Whether to post run tables. Also used as a trigger for summary jobs.\", flag_type=\"--\", ) wait: bool = Field( False, description=\"Whether to wait for a file to appear.\", flag_type=\"--\" ) xtcav: bool = Field( False, description=\"Whether to add XTCAV processing to the HDF5 generation.\", flag_type=\"--\", ) noarch: bool = Field( False, description=\"Whether to not use archiver data.\", flag_type=\"--\" ) lute_template_cfg: TemplateConfig = TemplateConfig(template_name=\"\", output_path=\"\") @validator(\"producer\", always=True) def validate_producer_path(cls, producer: str) -> str: return producer @validator(\"lute_template_cfg\", always=True) def use_producer( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if not lute_template_cfg.output_path: lute_template_cfg.output_path = values[\"producer\"] return lute_template_cfg @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment hutch: str = exp[:3] run: int = int(values[\"lute_config\"].run) directory: Optional[str] = values[\"directory\"] if directory is None: directory = f\"/sdf/data/lcls/ds/{hutch}/{exp}/hdf5/smalldata\" fname: str = f\"{exp}_Run{run:04d}.h5\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values","title":"SubmitSMDParameters"},{"location":"source/io/config/#io.config.SubmitSMDParameters.Config","text":"Bases: Config Identical to super-class Config but includes a result. Source code in lute/io/models/smd.py class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.SubmitSMDParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/config/#io.config.SubmitSMDParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.TaskParameters","text":"Bases: BaseSettings Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). Source code in lute/io/models/base.py class TaskParameters(BaseSettings): \"\"\"Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note: Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). \"\"\" class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" lute_config: AnalysisHeader","title":"TaskParameters"},{"location":"source/io/config/#io.config.TaskParameters.Config","text":"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. Only used if set_result==True result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if set_result==True Source code in lute/io/models/base.py class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.TaskParameters.Config.impl_schemas","text":"Schema specification for output result. Will be passed to TaskResult.","title":"impl_schemas"},{"location":"source/io/config/#io.config.TaskParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/config/#io.config.TaskParameters.Config.result_summary","text":"Format a TaskResult.summary from output.","title":"result_summary"},{"location":"source/io/config/#io.config.TaskParameters.Config.run_directory","text":"Set the directory that the Task is run from.","title":"run_directory"},{"location":"source/io/config/#io.config.TaskParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.TemplateConfig","text":"Bases: BaseModel Parameters used for templating of third party configuration files. Attributes: template_name ( str ) \u2013 The name of the template to use. This template must live in config/templates . output_path ( str ) \u2013 The FULL path, including filename to write the rendered template to. Source code in lute/io/models/base.py class TemplateConfig(BaseModel): \"\"\"Parameters used for templating of third party configuration files. Attributes: template_name (str): The name of the template to use. This template must live in `config/templates`. output_path (str): The FULL path, including filename to write the rendered template to. \"\"\" template_name: str output_path: str","title":"TemplateConfig"},{"location":"source/io/config/#io.config.TemplateParameters","text":"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable params. The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the params Field. Source code in lute/io/models/base.py @dataclass class TemplateParameters: \"\"\"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable `params.` The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the `params` Field. \"\"\" params: Any","title":"TemplateParameters"},{"location":"source/io/config/#io.config.TestBinaryErrParameters","text":"Bases: ThirdPartyParameters Same as TestBinary, but exits with non-zero code. Source code in lute/io/models/tests.py class TestBinaryErrParameters(ThirdPartyParameters): \"\"\"Same as TestBinary, but exits with non-zero code.\"\"\" executable: str = Field( \"/sdf/home/d/dorlhiac/test_tasks/test_threads_err\", description=\"Multi-threaded tes tbinary with non-zero exit code.\", ) p_arg1: int = Field(1, description=\"Number of threads.\")","title":"TestBinaryErrParameters"},{"location":"source/io/config/#io.config.TestParameters","text":"Bases: TaskParameters Parameters for the test Task Test . Source code in lute/io/models/tests.py class TestParameters(TaskParameters): \"\"\"Parameters for the test Task `Test`.\"\"\" float_var: float = Field(0.01, description=\"A floating point number.\") str_var: str = Field(\"test\", description=\"A string.\") class CompoundVar(BaseModel): int_var: int = 1 dict_var: Dict[str, str] = {\"a\": \"b\"} compound_var: CompoundVar = Field( description=( \"A compound parameter - consists of a `int_var` (int) and `dict_var`\" \" (Dict[str, str]).\" ) ) throw_error: bool = Field( False, description=\"If `True`, raise an exception to test error handling.\" )","title":"TestParameters"},{"location":"source/io/config/#io.config.ThirdPartyParameters","text":"Bases: TaskParameters Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. Source code in lute/io/models/base.py class ThirdPartyParameters(TaskParameters): \"\"\"Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. \"\"\" class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" # lute_template_cfg: TemplateConfig @root_validator(pre=False) def extra_fields_to_thirdparty(cls, values: Dict[str, Any]): for key in values: if key not in cls.__fields__: values[key] = TemplateParameters(values[key]) return values","title":"ThirdPartyParameters"},{"location":"source/io/config/#io.config.ThirdPartyParameters.Config","text":"Bases: Config Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base TaskParameters.Config class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. ThirdPartyTask-specific ( Optional [ str ] ) \u2013 extra ( str ) \u2013 \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq ( bool ) \u2013 False. If True, \"short\" command-line args are passed as -x=arg . ThirdPartyTask-specific. long_flags_use_eq ( bool ) \u2013 False. If True, \"long\" command-line args are passed as --long=arg . ThirdPartyTask-specific. Source code in lute/io/models/base.py class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/config/#io.config.ThirdPartyParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/config/#io.config.ThirdPartyParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/config/#io.config.ThirdPartyParameters.Config.short_flags_use_eq","text":"Whether short command-line arguments are passed like -x=arg .","title":"short_flags_use_eq"},{"location":"source/io/config/#io.config.parse_config","text":"Parse a configuration file and validate the contents. Parameters: task_name ( str , default: 'test' ) \u2013 Name of the specific task that will be run. config_path ( str , default: '' ) \u2013 Path to the configuration file. Returns: params ( TaskParameters ) \u2013 A TaskParameters object of validated task-specific parameters. Parameters are accessed with \"dot\" notation. E.g. params.param1 . Raises: ValidationError \u2013 Raised if there are problems with the configuration file. Passed through from Pydantic. Source code in lute/io/config.py def parse_config(task_name: str = \"test\", config_path: str = \"\") -> TaskParameters: \"\"\"Parse a configuration file and validate the contents. Args: task_name (str): Name of the specific task that will be run. config_path (str): Path to the configuration file. Returns: params (TaskParameters): A TaskParameters object of validated task-specific parameters. Parameters are accessed with \"dot\" notation. E.g. `params.param1`. Raises: ValidationError: Raised if there are problems with the configuration file. Passed through from Pydantic. \"\"\" task_config_name: str = f\"{task_name}Parameters\" with open(config_path, \"r\") as f: docs: Iterator[Dict[str, Any]] = yaml.load_all(stream=f, Loader=yaml.FullLoader) header: Dict[str, Any] = next(docs) config: Dict[str, Any] = next(docs) substitute_variables(header, header) substitute_variables(header, config) LUTE_DEBUG_EXIT(\"LUTE_DEBUG_EXIT_AT_YAML\", pprint.pformat(config)) lute_config: Dict[str, AnalysisHeader] = {\"lute_config\": AnalysisHeader(**header)} try: task_config: Dict[str, Any] = dict(config[task_name]) lute_config.update(task_config) except KeyError as err: warnings.warn( ( f\"{task_name} has no parameter definitions in YAML file.\" \" Attempting default parameter initialization.\" ) ) parsed_parameters: TaskParameters = globals()[task_config_name](**lute_config) return parsed_parameters","title":"parse_config"},{"location":"source/io/config/#io.config.substitute_variables","text":"Performs variable substitutions on a dictionary read from config YAML file. Can be used to define input parameters in terms of other input parameters. This is similar to functionality employed by validators for parameters in the specific Task models, but is intended to be more accessible to users. Variable substitutions are defined using a minimal syntax from Jinja: {{ experiment }} defines a substitution of the variable experiment . The characters {{ }} can be escaped if the literal symbols are needed in place. For example, a path to a file can be defined in terms of experiment and run values in the config file: MyTask: experiment: myexp run: 2 special_file: /path/to/{{ experiment }}/{{ run }}/file.inp Acceptable variables for substitutions are values defined elsewhere in the YAML file. Environment variables can also be used if prefaced with a $ character. E.g. to get the experiment from an environment variable: MyTask: run: 2 special_file: /path/to/{{ $EXPERIMENT }}/{{ run }}/file.inp Parameters: config ( Dict [ str , Any ] ) \u2013 A dictionary of parsed configuration. curr_key ( Optional [ str ] , default: None ) \u2013 Used to keep track of recursion level when scanning through iterable items in the config dictionary. Returns: subbed_config ( Dict [ str , Any ] ) \u2013 The config dictionary after substitutions have been made. May be identical to the input if no substitutions are needed. Source code in lute/io/config.py def substitute_variables( header: Dict[str, Any], config: Dict[str, Any], curr_key: Optional[str] = None ) -> None: \"\"\"Performs variable substitutions on a dictionary read from config YAML file. Can be used to define input parameters in terms of other input parameters. This is similar to functionality employed by validators for parameters in the specific Task models, but is intended to be more accessible to users. Variable substitutions are defined using a minimal syntax from Jinja: {{ experiment }} defines a substitution of the variable `experiment`. The characters `{{ }}` can be escaped if the literal symbols are needed in place. For example, a path to a file can be defined in terms of experiment and run values in the config file: MyTask: experiment: myexp run: 2 special_file: /path/to/{{ experiment }}/{{ run }}/file.inp Acceptable variables for substitutions are values defined elsewhere in the YAML file. Environment variables can also be used if prefaced with a `$` character. E.g. to get the experiment from an environment variable: MyTask: run: 2 special_file: /path/to/{{ $EXPERIMENT }}/{{ run }}/file.inp Args: config (Dict[str, Any]): A dictionary of parsed configuration. curr_key (Optional[str]): Used to keep track of recursion level when scanning through iterable items in the config dictionary. Returns: subbed_config (Dict[str, Any]): The config dictionary after substitutions have been made. May be identical to the input if no substitutions are needed. \"\"\" _sub_pattern = r\"\\{\\{[^}{]*\\}\\}\" iterable: Dict[str, Any] = config if curr_key is not None: # Need to handle nested levels by interpreting curr_key keys_by_level: List[str] = curr_key.split(\".\") for key in keys_by_level: iterable = iterable[key] else: ... # iterable = config for param, value in iterable.items(): if isinstance(value, dict): new_key: str if curr_key is None: new_key = param else: new_key = f\"{curr_key}.{param}\" substitute_variables(header, config, curr_key=new_key) elif isinstance(value, list): ... # Scalars str - we skip numeric types elif isinstance(value, str): matches: List[str] = re.findall(_sub_pattern, value) for m in matches: key_to_sub_maybe_with_fmt: List[str] = m[2:-2].strip().split(\":\") key_to_sub: str = key_to_sub_maybe_with_fmt[0] fmt: Optional[str] = None if len(key_to_sub_maybe_with_fmt) == 2: fmt = key_to_sub_maybe_with_fmt[1] sub: Any if key_to_sub[0] == \"$\": sub = os.getenv(key_to_sub[1:], None) if sub is None: print( f\"Environment variable {key_to_sub[1:]} not found! Cannot substitute in YAML config!\", flush=True, ) continue # substitutions from env vars will be strings, so convert back # to numeric in order to perform formatting later on (e.g. {var:04d}) sub = _check_str_numeric(sub) else: try: sub = config for key in key_to_sub.split(\".\"): sub = sub[key] except KeyError: sub = header[key_to_sub] pattern: str = ( m.replace(\"{{\", r\"\\{\\{\").replace(\"}}\", r\"\\}\\}\").replace(\"$\", r\"\\$\") ) if fmt is not None: sub = f\"{sub:{fmt}}\" else: sub = f\"{sub}\" iterable[param] = re.sub(pattern, sub, iterable[param]) # Reconvert back to numeric values if needed... iterable[param] = _check_str_numeric(iterable[param])","title":"substitute_variables"},{"location":"source/io/db/","text":"Tools for working with the LUTE parameter and configuration database. The current implementation relies on a sqlite backend database. In the future this may change - therefore relatively few high-level API function calls are intended to be public. These abstract away the details of the database interface and work exclusively on LUTE objects. Functions: Name Description record_analysis_db DescribedAnalysis) -> None: Writes the configuration to the backend database. read_latest_db_entry str, task_name: str, param: str) -> Any: Retrieve the most recent entry from a database for a specific Task. Raises: DatabaseError \u2013 Generic exception raised for LUTE database errors. DatabaseError Bases: Exception General LUTE database error. Source code in lute/io/db.py class DatabaseError(Exception): \"\"\"General LUTE database error.\"\"\" ... read_latest_db_entry(db_dir, task_name, param, valid_only=True) Read most recent value entered into the database for a Task parameter. (Will be updated for schema compliance as well as Task name.) Parameters: db_dir ( str ) \u2013 Database location. task_name ( str ) \u2013 The name of the Task to check the database for. param ( str ) \u2013 The parameter name for the Task that we want to retrieve. valid_only ( bool , default: True ) \u2013 Whether to consider only valid results or not. E.g. An input file may be useful even if the Task result is invalid (Failed). Default = True. Returns: val ( Any ) \u2013 The most recently entered value for param of task_name that can be found in the database. Returns None if nothing found. Source code in lute/io/db.py def read_latest_db_entry( db_dir: str, task_name: str, param: str, valid_only: bool = True ) -> Optional[Any]: \"\"\"Read most recent value entered into the database for a Task parameter. (Will be updated for schema compliance as well as Task name.) Args: db_dir (str): Database location. task_name (str): The name of the Task to check the database for. param (str): The parameter name for the Task that we want to retrieve. valid_only (bool): Whether to consider only valid results or not. E.g. An input file may be useful even if the Task result is invalid (Failed). Default = True. Returns: val (Any): The most recently entered value for `param` of `task_name` that can be found in the database. Returns None if nothing found. \"\"\" import sqlite3 from ._sqlite import _select_from_db con: sqlite3.Connection = sqlite3.Connection(f\"{db_dir}/lute.db\") with con: try: cond: Dict[str, str] = {} if valid_only: cond = {\"valid_flag\": \"1\"} entry: Any = _select_from_db(con, task_name, param, cond) except sqlite3.OperationalError as err: logger.debug(f\"Cannot retrieve value {param} due to: {err}\") entry = None return entry record_analysis_db(cfg) Write an DescribedAnalysis object to the database. The DescribedAnalysis object is maintained by the Executor and contains all information necessary to fully describe a single Task execution. The contained fields are split across multiple tables within the database as some of the information can be shared across multiple Tasks. Refer to docs/design/database.md for more information on the database specification. Source code in lute/io/db.py def record_analysis_db(cfg: DescribedAnalysis) -> None: \"\"\"Write an DescribedAnalysis object to the database. The DescribedAnalysis object is maintained by the Executor and contains all information necessary to fully describe a single `Task` execution. The contained fields are split across multiple tables within the database as some of the information can be shared across multiple Tasks. Refer to `docs/design/database.md` for more information on the database specification. \"\"\" import sqlite3 from ._sqlite import ( _make_shared_table, _make_task_table, _add_row_no_duplicate, _add_task_entry, ) try: work_dir: str = cfg.task_parameters.lute_config.work_dir except AttributeError: logger.info( ( \"Unable to access TaskParameters object. Likely wasn't created. \" \"Cannot store result.\" ) ) return del cfg.task_parameters.lute_config.work_dir exec_entry, exec_columns = _cfg_to_exec_entry_cols(cfg) task_name: str = cfg.task_result.task_name # All `Task`s have an AnalysisHeader, but this info can be shared so is # split into a different table ( task_entry, # Dict[str, Any] task_columns, # Dict[str, str] gen_entry, # Dict[str, Any] gen_columns, # Dict[str, str] ) = _params_to_entry_cols(cfg.task_parameters) x, y = _result_to_entry_cols(cfg.task_result) task_entry.update(x) task_columns.update(y) con: sqlite3.Connection = sqlite3.Connection(f\"{work_dir}/lute.db\") with con: # --- Table Creation ---# if not _make_shared_table(con, \"gen_cfg\", gen_columns): raise DatabaseError(\"Could not make general configuration table!\") if not _make_shared_table(con, \"exec_cfg\", exec_columns): raise DatabaseError(\"Could not make Executor configuration table!\") if not _make_task_table(con, task_name, task_columns): raise DatabaseError(f\"Could not make Task table for: {task_name}!\") # --- Row Addition ---# gen_id: int = _add_row_no_duplicate(con, \"gen_cfg\", gen_entry) exec_id: int = _add_row_no_duplicate(con, \"exec_cfg\", exec_entry) full_task_entry: Dict[str, Any] = { \"gen_cfg_id\": gen_id, \"exec_cfg_id\": exec_id, } full_task_entry.update(task_entry) # Prepare flag to indicate whether the task entry is valid or not # By default we say it is assuming proper completion valid_flag: int = ( 1 if cfg.task_result.task_status == TaskStatus.COMPLETED else 0 ) full_task_entry.update({\"valid_flag\": valid_flag}) _add_task_entry(con, task_name, full_task_entry)","title":"db"},{"location":"source/io/db/#io.db.DatabaseError","text":"Bases: Exception General LUTE database error. Source code in lute/io/db.py class DatabaseError(Exception): \"\"\"General LUTE database error.\"\"\" ...","title":"DatabaseError"},{"location":"source/io/db/#io.db.read_latest_db_entry","text":"Read most recent value entered into the database for a Task parameter. (Will be updated for schema compliance as well as Task name.) Parameters: db_dir ( str ) \u2013 Database location. task_name ( str ) \u2013 The name of the Task to check the database for. param ( str ) \u2013 The parameter name for the Task that we want to retrieve. valid_only ( bool , default: True ) \u2013 Whether to consider only valid results or not. E.g. An input file may be useful even if the Task result is invalid (Failed). Default = True. Returns: val ( Any ) \u2013 The most recently entered value for param of task_name that can be found in the database. Returns None if nothing found. Source code in lute/io/db.py def read_latest_db_entry( db_dir: str, task_name: str, param: str, valid_only: bool = True ) -> Optional[Any]: \"\"\"Read most recent value entered into the database for a Task parameter. (Will be updated for schema compliance as well as Task name.) Args: db_dir (str): Database location. task_name (str): The name of the Task to check the database for. param (str): The parameter name for the Task that we want to retrieve. valid_only (bool): Whether to consider only valid results or not. E.g. An input file may be useful even if the Task result is invalid (Failed). Default = True. Returns: val (Any): The most recently entered value for `param` of `task_name` that can be found in the database. Returns None if nothing found. \"\"\" import sqlite3 from ._sqlite import _select_from_db con: sqlite3.Connection = sqlite3.Connection(f\"{db_dir}/lute.db\") with con: try: cond: Dict[str, str] = {} if valid_only: cond = {\"valid_flag\": \"1\"} entry: Any = _select_from_db(con, task_name, param, cond) except sqlite3.OperationalError as err: logger.debug(f\"Cannot retrieve value {param} due to: {err}\") entry = None return entry","title":"read_latest_db_entry"},{"location":"source/io/db/#io.db.record_analysis_db","text":"Write an DescribedAnalysis object to the database. The DescribedAnalysis object is maintained by the Executor and contains all information necessary to fully describe a single Task execution. The contained fields are split across multiple tables within the database as some of the information can be shared across multiple Tasks. Refer to docs/design/database.md for more information on the database specification. Source code in lute/io/db.py def record_analysis_db(cfg: DescribedAnalysis) -> None: \"\"\"Write an DescribedAnalysis object to the database. The DescribedAnalysis object is maintained by the Executor and contains all information necessary to fully describe a single `Task` execution. The contained fields are split across multiple tables within the database as some of the information can be shared across multiple Tasks. Refer to `docs/design/database.md` for more information on the database specification. \"\"\" import sqlite3 from ._sqlite import ( _make_shared_table, _make_task_table, _add_row_no_duplicate, _add_task_entry, ) try: work_dir: str = cfg.task_parameters.lute_config.work_dir except AttributeError: logger.info( ( \"Unable to access TaskParameters object. Likely wasn't created. \" \"Cannot store result.\" ) ) return del cfg.task_parameters.lute_config.work_dir exec_entry, exec_columns = _cfg_to_exec_entry_cols(cfg) task_name: str = cfg.task_result.task_name # All `Task`s have an AnalysisHeader, but this info can be shared so is # split into a different table ( task_entry, # Dict[str, Any] task_columns, # Dict[str, str] gen_entry, # Dict[str, Any] gen_columns, # Dict[str, str] ) = _params_to_entry_cols(cfg.task_parameters) x, y = _result_to_entry_cols(cfg.task_result) task_entry.update(x) task_columns.update(y) con: sqlite3.Connection = sqlite3.Connection(f\"{work_dir}/lute.db\") with con: # --- Table Creation ---# if not _make_shared_table(con, \"gen_cfg\", gen_columns): raise DatabaseError(\"Could not make general configuration table!\") if not _make_shared_table(con, \"exec_cfg\", exec_columns): raise DatabaseError(\"Could not make Executor configuration table!\") if not _make_task_table(con, task_name, task_columns): raise DatabaseError(f\"Could not make Task table for: {task_name}!\") # --- Row Addition ---# gen_id: int = _add_row_no_duplicate(con, \"gen_cfg\", gen_entry) exec_id: int = _add_row_no_duplicate(con, \"exec_cfg\", exec_entry) full_task_entry: Dict[str, Any] = { \"gen_cfg_id\": gen_id, \"exec_cfg_id\": exec_id, } full_task_entry.update(task_entry) # Prepare flag to indicate whether the task entry is valid or not # By default we say it is assuming proper completion valid_flag: int = ( 1 if cfg.task_result.task_status == TaskStatus.COMPLETED else 0 ) full_task_entry.update({\"valid_flag\": valid_flag}) _add_task_entry(con, task_name, full_task_entry)","title":"record_analysis_db"},{"location":"source/io/elog/","text":"Provides utilities for communicating with the LCLS eLog. Make use of various eLog API endpoint to retrieve information or post results. Functions: Name Description get_elog_opr_auth str): Return an authorization object to interact with eLog API as an opr account for the hutch where exp was conducted. get_elog_kerberos_auth Return the authorization headers for the user account submitting the job. elog_http_request str, request_type: str, **params): Make an HTTP request to the API endpoint at url . format_file_for_post Union[str, tuple, list]): Prepare files according to the specification needed to add them as attachments to eLog posts. post_elog_message str, msg: str, tag: Optional[str], title: Optional[str], in_files: List[Union[str, tuple, list]], auth: Optional[Union[HTTPBasicAuth, Dict]] = None) Post a message to the eLog. post_elog_run_status Dict[str, Union[str, int, float]], update_url: Optional[str] = None) Post a run status to the summary section on the Workflows>Control tab. post_elog_run_table str, run: int, data: Dict[str, Any], auth: Optional[Union[HTTPBasicAuth, Dict]] = None) Update run table in the eLog. get_elog_runs_by_tag str, tag: str, auth: Optional[Union[HTTPBasicAuth, Dict]] = None) Return a list of runs with a specific tag. get_elog_params_by_run str, params: List[str], runs: Optional[List[int]]) Retrieve the requested parameters by run. If no run is provided, retrieve the requested parameters for all runs. elog_http_request(exp, endpoint, request_type, **params) Make an HTTP request to the eLog. This method will determine the proper authorization method and update the passed parameters appropriately. Functions implementing specific endpoint functionality and calling this function should only pass the necessary endpoint-specific parameters and not include the authorization objects. Parameters: exp ( str ) \u2013 Experiment. endpoint ( str ) \u2013 eLog API endpoint. request_type ( str ) \u2013 Type of request to make. Recognized options: POST or GET. **params ( Dict , default: {} ) \u2013 Endpoint parameters to pass with the HTTP request! Differs depending on the API endpoint. Do not include auth objects. Returns: status_code ( int ) \u2013 Response status code. Can be checked for errors. msg ( str ) \u2013 An error message, or a message saying SUCCESS. value ( Optional [ Any ] ) \u2013 For GET requests ONLY, return the requested information. Source code in lute/io/elog.py def elog_http_request( exp: str, endpoint: str, request_type: str, **params ) -> Tuple[int, str, Optional[Any]]: \"\"\"Make an HTTP request to the eLog. This method will determine the proper authorization method and update the passed parameters appropriately. Functions implementing specific endpoint functionality and calling this function should only pass the necessary endpoint-specific parameters and not include the authorization objects. Args: exp (str): Experiment. endpoint (str): eLog API endpoint. request_type (str): Type of request to make. Recognized options: POST or GET. **params (Dict): Endpoint parameters to pass with the HTTP request! Differs depending on the API endpoint. Do not include auth objects. Returns: status_code (int): Response status code. Can be checked for errors. msg (str): An error message, or a message saying SUCCESS. value (Optional[Any]): For GET requests ONLY, return the requested information. \"\"\" auth: Union[HTTPBasicAuth, Dict[str, str]] = get_elog_auth(exp) base_url: str if isinstance(auth, HTTPBasicAuth): params.update({\"auth\": auth}) base_url = \"https://pswww.slac.stanford.edu/ws-auth/lgbk/lgbk\" elif isinstance(auth, dict): params.update({\"headers\": auth}) base_url = \"https://pswww.slac.stanford.edu/ws-kerb/lgbk/lgbk\" url: str = f\"{base_url}/{endpoint}\" resp: requests.models.Response if request_type.upper() == \"POST\": resp = requests.post(url, **params) elif request_type.upper() == \"GET\": resp = requests.get(url, **params) else: return (-1, \"Invalid request type!\", None) status_code: int = resp.status_code msg: str = \"SUCCESS\" if resp.json()[\"success\"] and request_type.upper() == \"GET\": return (status_code, msg, resp.json()[\"value\"]) if status_code >= 300: msg = f\"Error when posting to eLog: Response {status_code}\" if not resp.json()[\"success\"]: err_msg = resp.json()[\"error_msg\"] msg += f\"\\nInclude message: {err_msg}\" return (resp.status_code, msg, None) format_file_for_post(in_file) Format a file for attachment to an eLog post. The eLog API expects a specifically formatted tuple when adding file attachments. This function prepares the tuple to specification given a number of different input types. Parameters: in_file ( str | tuple | list ) \u2013 File to include as an attachment in an eLog post. Source code in lute/io/elog.py def format_file_for_post( in_file: Union[str, tuple, list] ) -> Tuple[str, Tuple[str, BufferedReader], Any]: \"\"\"Format a file for attachment to an eLog post. The eLog API expects a specifically formatted tuple when adding file attachments. This function prepares the tuple to specification given a number of different input types. Args: in_file (str | tuple | list): File to include as an attachment in an eLog post. \"\"\" description: str fptr: BufferedReader ftype: Optional[str] if isinstance(in_file, str): description = os.path.basename(in_file) fptr = open(in_file, \"rb\") ftype = mimetypes.guess_type(in_file)[0] elif isinstance(in_file, tuple) or isinstance(in_file, list): description = in_file[1] fptr = open(in_file[0], \"rb\") ftype = mimetypes.guess_type(in_file[0])[0] else: raise ElogFileFormatError(f\"Unrecognized format: {in_file}\") out_file: Tuple[str, Tuple[str, BufferedReader], Any] = ( \"files\", (description, fptr), ftype, ) return out_file get_elog_active_expmt(hutch, *, endstation=0) Get the current active experiment for a hutch. This function is one of two functions to manage the HTTP request independently. This is because it does not require an authorization object, and its result is needed for the generic function elog_http_request to work properly. Parameters: hutch ( str ) \u2013 The hutch to get the active experiment for. endstation ( int , default: 0 ) \u2013 The hutch endstation to get the experiment for. This should generally be 0. Source code in lute/io/elog.py def get_elog_active_expmt(hutch: str, *, endstation: int = 0) -> str: \"\"\"Get the current active experiment for a hutch. This function is one of two functions to manage the HTTP request independently. This is because it does not require an authorization object, and its result is needed for the generic function `elog_http_request` to work properly. Args: hutch (str): The hutch to get the active experiment for. endstation (int): The hutch endstation to get the experiment for. This should generally be 0. \"\"\" base_url: str = \"https://pswww.slac.stanford.edu/ws/lgbk/lgbk\" endpoint: str = \"ws/activeexperiment_for_instrument_station\" url: str = f\"{base_url}/{endpoint}\" params: Dict[str, str] = {\"instrument_name\": hutch, \"station\": f\"{endstation}\"} resp: requests.models.Response = requests.get(url, params) if resp.status_code > 300: raise RuntimeError( f\"Error getting current experiment!\\n\\t\\tIncorrect hutch: '{hutch}'?\" ) if resp.json()[\"success\"]: return resp.json()[\"value\"][\"name\"] else: msg: str = resp.json()[\"error_msg\"] raise RuntimeError(f\"Error getting current experiment! Err: {msg}\") get_elog_auth(exp) Determine the appropriate auth method depending on experiment state. Returns: auth ( HTTPBasicAuth | Dict [ str , str ] ) \u2013 Depending on whether an experiment is active/live, returns authorization for the hutch operator account or the current user submitting a job. Source code in lute/io/elog.py def get_elog_auth(exp: str) -> Union[HTTPBasicAuth, Dict[str, str]]: \"\"\"Determine the appropriate auth method depending on experiment state. Returns: auth (HTTPBasicAuth | Dict[str, str]): Depending on whether an experiment is active/live, returns authorization for the hutch operator account or the current user submitting a job. \"\"\" hutch: str = exp[:3] if exp.lower() == get_elog_active_expmt(hutch=hutch).lower(): return get_elog_opr_auth(exp) else: return get_elog_kerberos_auth() get_elog_kerberos_auth() Returns Kerberos authorization key. This functions returns authorization for the USER account submitting jobs. It assumes that kinit has been run. Returns: auth ( Dict [ str , str ] ) \u2013 Dictionary containing Kerberos authorization key. Source code in lute/io/elog.py def get_elog_kerberos_auth() -> Dict[str, str]: \"\"\"Returns Kerberos authorization key. This functions returns authorization for the USER account submitting jobs. It assumes that `kinit` has been run. Returns: auth (Dict[str, str]): Dictionary containing Kerberos authorization key. \"\"\" from krtc import KerberosTicket return KerberosTicket(\"HTTP@pswww.slac.stanford.edu\").getAuthHeaders() get_elog_opr_auth(exp) Produce authentication for the \"opr\" user associated to an experiment. This method uses basic authentication using username and password. Parameters: exp ( str ) \u2013 Name of the experiment to produce authentication for. Returns: auth ( HTTPBasicAuth ) \u2013 HTTPBasicAuth for an active experiment based on username and password for the associated operator account. Source code in lute/io/elog.py def get_elog_opr_auth(exp: str) -> HTTPBasicAuth: \"\"\"Produce authentication for the \"opr\" user associated to an experiment. This method uses basic authentication using username and password. Args: exp (str): Name of the experiment to produce authentication for. Returns: auth (HTTPBasicAuth): HTTPBasicAuth for an active experiment based on username and password for the associated operator account. \"\"\" opr: str = f\"{exp[:3]}opr\" with open(\"/sdf/group/lcls/ds/tools/forElogPost.txt\", \"r\") as f: pw: str = f.readline()[:-1] return HTTPBasicAuth(opr, pw) get_elog_params_by_run(exp, params, runs=None) Retrieve requested parameters by run or for all runs. Parameters: exp ( str ) \u2013 Experiment to retrieve parameters for. params ( List [ str ] ) \u2013 A list of parameters to retrieve. These can be any parameter recorded in the eLog (PVs, parameters posted by other Tasks, etc.) Source code in lute/io/elog.py def get_elog_params_by_run( exp: str, params: List[str], runs: Optional[List[int]] = None ) -> Dict[str, str]: \"\"\"Retrieve requested parameters by run or for all runs. Args: exp (str): Experiment to retrieve parameters for. params (List[str]): A list of parameters to retrieve. These can be any parameter recorded in the eLog (PVs, parameters posted by other Tasks, etc.) \"\"\" ... get_elog_runs_by_tag(exp, tag, auth=None) Retrieve run numbers with a specified tag. Parameters: exp ( str ) \u2013 Experiment name. tag ( str ) \u2013 The tag to retrieve runs for. Source code in lute/io/elog.py def get_elog_runs_by_tag( exp: str, tag: str, auth: Optional[Union[HTTPBasicAuth, Dict]] = None ) -> List[int]: \"\"\"Retrieve run numbers with a specified tag. Args: exp (str): Experiment name. tag (str): The tag to retrieve runs for. \"\"\" endpoint: str = f\"{exp}/ws/get_runs_with_tag?tag={tag}\" params: Dict[str, Any] = {} status_code, resp_msg, tagged_runs = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"GET\", **params ) if not tagged_runs: tagged_runs = [] return tagged_runs get_elog_workflows(exp) Get the current workflow definitions for an experiment. Returns: defns ( Dict [ str , str ] ) \u2013 A dictionary of workflow definitions. Source code in lute/io/elog.py def get_elog_workflows(exp: str) -> Dict[str, str]: \"\"\"Get the current workflow definitions for an experiment. Returns: defns (Dict[str, str]): A dictionary of workflow definitions. \"\"\" raise NotImplementedError post_elog_message(exp, msg, *, tag, title, in_files=[]) Post a new message to the eLog. Inspired by the elog package. Parameters: exp ( str ) \u2013 Experiment name. msg ( str ) \u2013 BODY of the eLog post. tag ( str | None ) \u2013 Optional \"tag\" to associate with the eLog post. title ( str | None ) \u2013 Optional title to include in the eLog post. in_files ( List [ str | tuple | list ] , default: [] ) \u2013 Files to include as attachments in the eLog post. Returns: err_msg ( str | None ) \u2013 If successful, nothing is returned, otherwise, return an error message. Source code in lute/io/elog.py def post_elog_message( exp: str, msg: str, *, tag: Optional[str], title: Optional[str], in_files: List[Union[str, tuple, list]] = [], ) -> Optional[str]: \"\"\"Post a new message to the eLog. Inspired by the `elog` package. Args: exp (str): Experiment name. msg (str): BODY of the eLog post. tag (str | None): Optional \"tag\" to associate with the eLog post. title (str | None): Optional title to include in the eLog post. in_files (List[str | tuple | list]): Files to include as attachments in the eLog post. Returns: err_msg (str | None): If successful, nothing is returned, otherwise, return an error message. \"\"\" # MOSTLY CORRECT out_files: list = [] for f in in_files: try: out_files.append(format_file_for_post(in_file=f)) except ElogFileFormatError as err: logger.debug(f\"ElogFileFormatError: {err}\") post: Dict[str, str] = {} post[\"log_text\"] = msg if tag: post[\"log_tags\"] = tag if title: post[\"log_title\"] = title endpoint: str = f\"{exp}/ws/new_elog_entry\" params: Dict[str, Any] = {\"data\": post} if out_files: params.update({\"files\": out_files}) status_code, resp_msg, _ = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"POST\", **params ) if resp_msg != \"SUCCESS\": return resp_msg post_elog_run_status(data, update_url=None) Post a summary to the status/report section of a specific run. In contrast to most eLog update/post mechanisms, this function searches for a specific environment variable which contains a specific URL for posting. This is updated every job/run as jobs are submitted by the JID. The URL can optionally be passed to this function if it is known. Parameters: data ( Dict [ str , Union [ str , int , float ]] ) \u2013 The data to post to the eLog report section. Formatted in key:value pairs. update_url ( Optional [ str ] , default: None ) \u2013 Optional update URL. If not provided, the function searches for the corresponding environment variable. If neither is found, the function aborts Source code in lute/io/elog.py def post_elog_run_status( data: Dict[str, Union[str, int, float]], update_url: Optional[str] = None ) -> None: \"\"\"Post a summary to the status/report section of a specific run. In contrast to most eLog update/post mechanisms, this function searches for a specific environment variable which contains a specific URL for posting. This is updated every job/run as jobs are submitted by the JID. The URL can optionally be passed to this function if it is known. Args: data (Dict[str, Union[str, int, float]]): The data to post to the eLog report section. Formatted in key:value pairs. update_url (Optional[str]): Optional update URL. If not provided, the function searches for the corresponding environment variable. If neither is found, the function aborts \"\"\" if update_url is None: update_url = os.environ.get(\"JID_UPDATE_COUNTERS\") if update_url is None: logger.info(\"eLog Update Failed! JID_UPDATE_COUNTERS is not defined!\") return current_status: Dict[str, Union[str, int, float]] = _get_current_run_status( update_url ) current_status.update(data) post_list: List[Dict[str, str]] = [ {\"key\": f\"{key}\", \"value\": f\"{value}\"} for key, value in current_status.items() ] params: Dict[str, List[Dict[str, str]]] = {\"json\": post_list} resp: requests.models.Response = requests.post(update_url, **params) post_elog_run_table(exp, run, data) Post data for eLog run tables. Parameters: exp ( str ) \u2013 Experiment name. run ( int ) \u2013 Run number corresponding to the data being posted. data ( Dict [ str , Any ] ) \u2013 Data to be posted in format data[\"column_header\"] = value. Returns: err_msg ( None | str ) \u2013 If successful, nothing is returned, otherwise, return an error message. Source code in lute/io/elog.py def post_elog_run_table( exp: str, run: int, data: Dict[str, Any], ) -> Optional[str]: \"\"\"Post data for eLog run tables. Args: exp (str): Experiment name. run (int): Run number corresponding to the data being posted. data (Dict[str, Any]): Data to be posted in format data[\"column_header\"] = value. Returns: err_msg (None | str): If successful, nothing is returned, otherwise, return an error message. \"\"\" endpoint: str = f\"run_control/{exp}/ws/add_run_params\" params: Dict[str, Any] = {\"params\": {\"run_num\": run}, \"json\": data} status_code, resp_msg, _ = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"POST\", **params ) if resp_msg != \"SUCCESS\": return resp_msg post_elog_workflow(exp, name, executable, wf_params, *, trigger='run_end', location='S3DF', **trig_args) Create a new eLog workflow, or update an existing one. The workflow will run a specific executable as a batch job when the specified trigger occurs. The precise arguments may vary depending on the selected trigger type. Parameters: name ( str ) \u2013 An identifying name for the workflow. E.g. \"process data\" executable ( str ) \u2013 Full path to the executable to be run. wf_params ( str ) \u2013 All command-line parameters for the executable as a string. trigger ( str , default: 'run_end' ) \u2013 When to trigger execution of the specified executable. One of: - 'manual': Must be manually triggered. No automatic processing. - 'run_start': Execute immediately if a new run begins. - 'run_end': As soon as a run ends. - 'param_is': As soon as a parameter has a specific value for a run. location ( str , default: 'S3DF' ) \u2013 Where to submit the job. S3DF or NERSC. **trig_args ( str , default: {} ) \u2013 Arguments required for a specific trigger type. trigger='param_is' - 2 Arguments trig_param (str): Name of the parameter to watch for. trig_param_val (str): Value the parameter should have to trigger. Source code in lute/io/elog.py def post_elog_workflow( exp: str, name: str, executable: str, wf_params: str, *, trigger: str = \"run_end\", location: str = \"S3DF\", **trig_args: str, ) -> None: \"\"\"Create a new eLog workflow, or update an existing one. The workflow will run a specific executable as a batch job when the specified trigger occurs. The precise arguments may vary depending on the selected trigger type. Args: name (str): An identifying name for the workflow. E.g. \"process data\" executable (str): Full path to the executable to be run. wf_params (str): All command-line parameters for the executable as a string. trigger (str): When to trigger execution of the specified executable. One of: - 'manual': Must be manually triggered. No automatic processing. - 'run_start': Execute immediately if a new run begins. - 'run_end': As soon as a run ends. - 'param_is': As soon as a parameter has a specific value for a run. location (str): Where to submit the job. S3DF or NERSC. **trig_args (str): Arguments required for a specific trigger type. trigger='param_is' - 2 Arguments trig_param (str): Name of the parameter to watch for. trig_param_val (str): Value the parameter should have to trigger. \"\"\" endpoint: str = f\"{exp}/ws/create_update_workflow_def\" trig_map: Dict[str, str] = { \"manual\": \"MANUAL\", \"run_start\": \"START_OF_RUN\", \"run_end\": \"END_OF_RUN\", \"param_is\": \"RUN_PARAM_IS_VALUE\", } if trigger not in trig_map.keys(): raise NotImplementedError( f\"Cannot create workflow with trigger type: {trigger}\" ) wf_defn: Dict[str, str] = { \"name\": name, \"executable\": executable, \"parameters\": wf_params, \"trigger\": trig_map[trigger], \"location\": location, } if trigger == \"param_is\": if \"trig_param\" not in trig_args or \"trig_param_val\" not in trig_args: raise RuntimeError( \"Trigger type 'param_is' requires: 'trig_param' and 'trig_param_val' arguments\" ) wf_defn.update( { \"run_param_name\": trig_args[\"trig_param\"], \"run_param_val\": trig_args[\"trig_param_val\"], } ) post_params: Dict[str, Dict[str, str]] = {\"json\": wf_defn} status_code, resp_msg, _ = elog_http_request( exp, endpoint=endpoint, request_type=\"POST\", **post_params )","title":"elog"},{"location":"source/io/elog/#io.elog.elog_http_request","text":"Make an HTTP request to the eLog. This method will determine the proper authorization method and update the passed parameters appropriately. Functions implementing specific endpoint functionality and calling this function should only pass the necessary endpoint-specific parameters and not include the authorization objects. Parameters: exp ( str ) \u2013 Experiment. endpoint ( str ) \u2013 eLog API endpoint. request_type ( str ) \u2013 Type of request to make. Recognized options: POST or GET. **params ( Dict , default: {} ) \u2013 Endpoint parameters to pass with the HTTP request! Differs depending on the API endpoint. Do not include auth objects. Returns: status_code ( int ) \u2013 Response status code. Can be checked for errors. msg ( str ) \u2013 An error message, or a message saying SUCCESS. value ( Optional [ Any ] ) \u2013 For GET requests ONLY, return the requested information. Source code in lute/io/elog.py def elog_http_request( exp: str, endpoint: str, request_type: str, **params ) -> Tuple[int, str, Optional[Any]]: \"\"\"Make an HTTP request to the eLog. This method will determine the proper authorization method and update the passed parameters appropriately. Functions implementing specific endpoint functionality and calling this function should only pass the necessary endpoint-specific parameters and not include the authorization objects. Args: exp (str): Experiment. endpoint (str): eLog API endpoint. request_type (str): Type of request to make. Recognized options: POST or GET. **params (Dict): Endpoint parameters to pass with the HTTP request! Differs depending on the API endpoint. Do not include auth objects. Returns: status_code (int): Response status code. Can be checked for errors. msg (str): An error message, or a message saying SUCCESS. value (Optional[Any]): For GET requests ONLY, return the requested information. \"\"\" auth: Union[HTTPBasicAuth, Dict[str, str]] = get_elog_auth(exp) base_url: str if isinstance(auth, HTTPBasicAuth): params.update({\"auth\": auth}) base_url = \"https://pswww.slac.stanford.edu/ws-auth/lgbk/lgbk\" elif isinstance(auth, dict): params.update({\"headers\": auth}) base_url = \"https://pswww.slac.stanford.edu/ws-kerb/lgbk/lgbk\" url: str = f\"{base_url}/{endpoint}\" resp: requests.models.Response if request_type.upper() == \"POST\": resp = requests.post(url, **params) elif request_type.upper() == \"GET\": resp = requests.get(url, **params) else: return (-1, \"Invalid request type!\", None) status_code: int = resp.status_code msg: str = \"SUCCESS\" if resp.json()[\"success\"] and request_type.upper() == \"GET\": return (status_code, msg, resp.json()[\"value\"]) if status_code >= 300: msg = f\"Error when posting to eLog: Response {status_code}\" if not resp.json()[\"success\"]: err_msg = resp.json()[\"error_msg\"] msg += f\"\\nInclude message: {err_msg}\" return (resp.status_code, msg, None)","title":"elog_http_request"},{"location":"source/io/elog/#io.elog.format_file_for_post","text":"Format a file for attachment to an eLog post. The eLog API expects a specifically formatted tuple when adding file attachments. This function prepares the tuple to specification given a number of different input types. Parameters: in_file ( str | tuple | list ) \u2013 File to include as an attachment in an eLog post. Source code in lute/io/elog.py def format_file_for_post( in_file: Union[str, tuple, list] ) -> Tuple[str, Tuple[str, BufferedReader], Any]: \"\"\"Format a file for attachment to an eLog post. The eLog API expects a specifically formatted tuple when adding file attachments. This function prepares the tuple to specification given a number of different input types. Args: in_file (str | tuple | list): File to include as an attachment in an eLog post. \"\"\" description: str fptr: BufferedReader ftype: Optional[str] if isinstance(in_file, str): description = os.path.basename(in_file) fptr = open(in_file, \"rb\") ftype = mimetypes.guess_type(in_file)[0] elif isinstance(in_file, tuple) or isinstance(in_file, list): description = in_file[1] fptr = open(in_file[0], \"rb\") ftype = mimetypes.guess_type(in_file[0])[0] else: raise ElogFileFormatError(f\"Unrecognized format: {in_file}\") out_file: Tuple[str, Tuple[str, BufferedReader], Any] = ( \"files\", (description, fptr), ftype, ) return out_file","title":"format_file_for_post"},{"location":"source/io/elog/#io.elog.get_elog_active_expmt","text":"Get the current active experiment for a hutch. This function is one of two functions to manage the HTTP request independently. This is because it does not require an authorization object, and its result is needed for the generic function elog_http_request to work properly. Parameters: hutch ( str ) \u2013 The hutch to get the active experiment for. endstation ( int , default: 0 ) \u2013 The hutch endstation to get the experiment for. This should generally be 0. Source code in lute/io/elog.py def get_elog_active_expmt(hutch: str, *, endstation: int = 0) -> str: \"\"\"Get the current active experiment for a hutch. This function is one of two functions to manage the HTTP request independently. This is because it does not require an authorization object, and its result is needed for the generic function `elog_http_request` to work properly. Args: hutch (str): The hutch to get the active experiment for. endstation (int): The hutch endstation to get the experiment for. This should generally be 0. \"\"\" base_url: str = \"https://pswww.slac.stanford.edu/ws/lgbk/lgbk\" endpoint: str = \"ws/activeexperiment_for_instrument_station\" url: str = f\"{base_url}/{endpoint}\" params: Dict[str, str] = {\"instrument_name\": hutch, \"station\": f\"{endstation}\"} resp: requests.models.Response = requests.get(url, params) if resp.status_code > 300: raise RuntimeError( f\"Error getting current experiment!\\n\\t\\tIncorrect hutch: '{hutch}'?\" ) if resp.json()[\"success\"]: return resp.json()[\"value\"][\"name\"] else: msg: str = resp.json()[\"error_msg\"] raise RuntimeError(f\"Error getting current experiment! Err: {msg}\")","title":"get_elog_active_expmt"},{"location":"source/io/elog/#io.elog.get_elog_auth","text":"Determine the appropriate auth method depending on experiment state. Returns: auth ( HTTPBasicAuth | Dict [ str , str ] ) \u2013 Depending on whether an experiment is active/live, returns authorization for the hutch operator account or the current user submitting a job. Source code in lute/io/elog.py def get_elog_auth(exp: str) -> Union[HTTPBasicAuth, Dict[str, str]]: \"\"\"Determine the appropriate auth method depending on experiment state. Returns: auth (HTTPBasicAuth | Dict[str, str]): Depending on whether an experiment is active/live, returns authorization for the hutch operator account or the current user submitting a job. \"\"\" hutch: str = exp[:3] if exp.lower() == get_elog_active_expmt(hutch=hutch).lower(): return get_elog_opr_auth(exp) else: return get_elog_kerberos_auth()","title":"get_elog_auth"},{"location":"source/io/elog/#io.elog.get_elog_kerberos_auth","text":"Returns Kerberos authorization key. This functions returns authorization for the USER account submitting jobs. It assumes that kinit has been run. Returns: auth ( Dict [ str , str ] ) \u2013 Dictionary containing Kerberos authorization key. Source code in lute/io/elog.py def get_elog_kerberos_auth() -> Dict[str, str]: \"\"\"Returns Kerberos authorization key. This functions returns authorization for the USER account submitting jobs. It assumes that `kinit` has been run. Returns: auth (Dict[str, str]): Dictionary containing Kerberos authorization key. \"\"\" from krtc import KerberosTicket return KerberosTicket(\"HTTP@pswww.slac.stanford.edu\").getAuthHeaders()","title":"get_elog_kerberos_auth"},{"location":"source/io/elog/#io.elog.get_elog_opr_auth","text":"Produce authentication for the \"opr\" user associated to an experiment. This method uses basic authentication using username and password. Parameters: exp ( str ) \u2013 Name of the experiment to produce authentication for. Returns: auth ( HTTPBasicAuth ) \u2013 HTTPBasicAuth for an active experiment based on username and password for the associated operator account. Source code in lute/io/elog.py def get_elog_opr_auth(exp: str) -> HTTPBasicAuth: \"\"\"Produce authentication for the \"opr\" user associated to an experiment. This method uses basic authentication using username and password. Args: exp (str): Name of the experiment to produce authentication for. Returns: auth (HTTPBasicAuth): HTTPBasicAuth for an active experiment based on username and password for the associated operator account. \"\"\" opr: str = f\"{exp[:3]}opr\" with open(\"/sdf/group/lcls/ds/tools/forElogPost.txt\", \"r\") as f: pw: str = f.readline()[:-1] return HTTPBasicAuth(opr, pw)","title":"get_elog_opr_auth"},{"location":"source/io/elog/#io.elog.get_elog_params_by_run","text":"Retrieve requested parameters by run or for all runs. Parameters: exp ( str ) \u2013 Experiment to retrieve parameters for. params ( List [ str ] ) \u2013 A list of parameters to retrieve. These can be any parameter recorded in the eLog (PVs, parameters posted by other Tasks, etc.) Source code in lute/io/elog.py def get_elog_params_by_run( exp: str, params: List[str], runs: Optional[List[int]] = None ) -> Dict[str, str]: \"\"\"Retrieve requested parameters by run or for all runs. Args: exp (str): Experiment to retrieve parameters for. params (List[str]): A list of parameters to retrieve. These can be any parameter recorded in the eLog (PVs, parameters posted by other Tasks, etc.) \"\"\" ...","title":"get_elog_params_by_run"},{"location":"source/io/elog/#io.elog.get_elog_runs_by_tag","text":"Retrieve run numbers with a specified tag. Parameters: exp ( str ) \u2013 Experiment name. tag ( str ) \u2013 The tag to retrieve runs for. Source code in lute/io/elog.py def get_elog_runs_by_tag( exp: str, tag: str, auth: Optional[Union[HTTPBasicAuth, Dict]] = None ) -> List[int]: \"\"\"Retrieve run numbers with a specified tag. Args: exp (str): Experiment name. tag (str): The tag to retrieve runs for. \"\"\" endpoint: str = f\"{exp}/ws/get_runs_with_tag?tag={tag}\" params: Dict[str, Any] = {} status_code, resp_msg, tagged_runs = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"GET\", **params ) if not tagged_runs: tagged_runs = [] return tagged_runs","title":"get_elog_runs_by_tag"},{"location":"source/io/elog/#io.elog.get_elog_workflows","text":"Get the current workflow definitions for an experiment. Returns: defns ( Dict [ str , str ] ) \u2013 A dictionary of workflow definitions. Source code in lute/io/elog.py def get_elog_workflows(exp: str) -> Dict[str, str]: \"\"\"Get the current workflow definitions for an experiment. Returns: defns (Dict[str, str]): A dictionary of workflow definitions. \"\"\" raise NotImplementedError","title":"get_elog_workflows"},{"location":"source/io/elog/#io.elog.post_elog_message","text":"Post a new message to the eLog. Inspired by the elog package. Parameters: exp ( str ) \u2013 Experiment name. msg ( str ) \u2013 BODY of the eLog post. tag ( str | None ) \u2013 Optional \"tag\" to associate with the eLog post. title ( str | None ) \u2013 Optional title to include in the eLog post. in_files ( List [ str | tuple | list ] , default: [] ) \u2013 Files to include as attachments in the eLog post. Returns: err_msg ( str | None ) \u2013 If successful, nothing is returned, otherwise, return an error message. Source code in lute/io/elog.py def post_elog_message( exp: str, msg: str, *, tag: Optional[str], title: Optional[str], in_files: List[Union[str, tuple, list]] = [], ) -> Optional[str]: \"\"\"Post a new message to the eLog. Inspired by the `elog` package. Args: exp (str): Experiment name. msg (str): BODY of the eLog post. tag (str | None): Optional \"tag\" to associate with the eLog post. title (str | None): Optional title to include in the eLog post. in_files (List[str | tuple | list]): Files to include as attachments in the eLog post. Returns: err_msg (str | None): If successful, nothing is returned, otherwise, return an error message. \"\"\" # MOSTLY CORRECT out_files: list = [] for f in in_files: try: out_files.append(format_file_for_post(in_file=f)) except ElogFileFormatError as err: logger.debug(f\"ElogFileFormatError: {err}\") post: Dict[str, str] = {} post[\"log_text\"] = msg if tag: post[\"log_tags\"] = tag if title: post[\"log_title\"] = title endpoint: str = f\"{exp}/ws/new_elog_entry\" params: Dict[str, Any] = {\"data\": post} if out_files: params.update({\"files\": out_files}) status_code, resp_msg, _ = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"POST\", **params ) if resp_msg != \"SUCCESS\": return resp_msg","title":"post_elog_message"},{"location":"source/io/elog/#io.elog.post_elog_run_status","text":"Post a summary to the status/report section of a specific run. In contrast to most eLog update/post mechanisms, this function searches for a specific environment variable which contains a specific URL for posting. This is updated every job/run as jobs are submitted by the JID. The URL can optionally be passed to this function if it is known. Parameters: data ( Dict [ str , Union [ str , int , float ]] ) \u2013 The data to post to the eLog report section. Formatted in key:value pairs. update_url ( Optional [ str ] , default: None ) \u2013 Optional update URL. If not provided, the function searches for the corresponding environment variable. If neither is found, the function aborts Source code in lute/io/elog.py def post_elog_run_status( data: Dict[str, Union[str, int, float]], update_url: Optional[str] = None ) -> None: \"\"\"Post a summary to the status/report section of a specific run. In contrast to most eLog update/post mechanisms, this function searches for a specific environment variable which contains a specific URL for posting. This is updated every job/run as jobs are submitted by the JID. The URL can optionally be passed to this function if it is known. Args: data (Dict[str, Union[str, int, float]]): The data to post to the eLog report section. Formatted in key:value pairs. update_url (Optional[str]): Optional update URL. If not provided, the function searches for the corresponding environment variable. If neither is found, the function aborts \"\"\" if update_url is None: update_url = os.environ.get(\"JID_UPDATE_COUNTERS\") if update_url is None: logger.info(\"eLog Update Failed! JID_UPDATE_COUNTERS is not defined!\") return current_status: Dict[str, Union[str, int, float]] = _get_current_run_status( update_url ) current_status.update(data) post_list: List[Dict[str, str]] = [ {\"key\": f\"{key}\", \"value\": f\"{value}\"} for key, value in current_status.items() ] params: Dict[str, List[Dict[str, str]]] = {\"json\": post_list} resp: requests.models.Response = requests.post(update_url, **params)","title":"post_elog_run_status"},{"location":"source/io/elog/#io.elog.post_elog_run_table","text":"Post data for eLog run tables. Parameters: exp ( str ) \u2013 Experiment name. run ( int ) \u2013 Run number corresponding to the data being posted. data ( Dict [ str , Any ] ) \u2013 Data to be posted in format data[\"column_header\"] = value. Returns: err_msg ( None | str ) \u2013 If successful, nothing is returned, otherwise, return an error message. Source code in lute/io/elog.py def post_elog_run_table( exp: str, run: int, data: Dict[str, Any], ) -> Optional[str]: \"\"\"Post data for eLog run tables. Args: exp (str): Experiment name. run (int): Run number corresponding to the data being posted. data (Dict[str, Any]): Data to be posted in format data[\"column_header\"] = value. Returns: err_msg (None | str): If successful, nothing is returned, otherwise, return an error message. \"\"\" endpoint: str = f\"run_control/{exp}/ws/add_run_params\" params: Dict[str, Any] = {\"params\": {\"run_num\": run}, \"json\": data} status_code, resp_msg, _ = elog_http_request( exp=exp, endpoint=endpoint, request_type=\"POST\", **params ) if resp_msg != \"SUCCESS\": return resp_msg","title":"post_elog_run_table"},{"location":"source/io/elog/#io.elog.post_elog_workflow","text":"Create a new eLog workflow, or update an existing one. The workflow will run a specific executable as a batch job when the specified trigger occurs. The precise arguments may vary depending on the selected trigger type. Parameters: name ( str ) \u2013 An identifying name for the workflow. E.g. \"process data\" executable ( str ) \u2013 Full path to the executable to be run. wf_params ( str ) \u2013 All command-line parameters for the executable as a string. trigger ( str , default: 'run_end' ) \u2013 When to trigger execution of the specified executable. One of: - 'manual': Must be manually triggered. No automatic processing. - 'run_start': Execute immediately if a new run begins. - 'run_end': As soon as a run ends. - 'param_is': As soon as a parameter has a specific value for a run. location ( str , default: 'S3DF' ) \u2013 Where to submit the job. S3DF or NERSC. **trig_args ( str , default: {} ) \u2013 Arguments required for a specific trigger type. trigger='param_is' - 2 Arguments trig_param (str): Name of the parameter to watch for. trig_param_val (str): Value the parameter should have to trigger. Source code in lute/io/elog.py def post_elog_workflow( exp: str, name: str, executable: str, wf_params: str, *, trigger: str = \"run_end\", location: str = \"S3DF\", **trig_args: str, ) -> None: \"\"\"Create a new eLog workflow, or update an existing one. The workflow will run a specific executable as a batch job when the specified trigger occurs. The precise arguments may vary depending on the selected trigger type. Args: name (str): An identifying name for the workflow. E.g. \"process data\" executable (str): Full path to the executable to be run. wf_params (str): All command-line parameters for the executable as a string. trigger (str): When to trigger execution of the specified executable. One of: - 'manual': Must be manually triggered. No automatic processing. - 'run_start': Execute immediately if a new run begins. - 'run_end': As soon as a run ends. - 'param_is': As soon as a parameter has a specific value for a run. location (str): Where to submit the job. S3DF or NERSC. **trig_args (str): Arguments required for a specific trigger type. trigger='param_is' - 2 Arguments trig_param (str): Name of the parameter to watch for. trig_param_val (str): Value the parameter should have to trigger. \"\"\" endpoint: str = f\"{exp}/ws/create_update_workflow_def\" trig_map: Dict[str, str] = { \"manual\": \"MANUAL\", \"run_start\": \"START_OF_RUN\", \"run_end\": \"END_OF_RUN\", \"param_is\": \"RUN_PARAM_IS_VALUE\", } if trigger not in trig_map.keys(): raise NotImplementedError( f\"Cannot create workflow with trigger type: {trigger}\" ) wf_defn: Dict[str, str] = { \"name\": name, \"executable\": executable, \"parameters\": wf_params, \"trigger\": trig_map[trigger], \"location\": location, } if trigger == \"param_is\": if \"trig_param\" not in trig_args or \"trig_param_val\" not in trig_args: raise RuntimeError( \"Trigger type 'param_is' requires: 'trig_param' and 'trig_param_val' arguments\" ) wf_defn.update( { \"run_param_name\": trig_args[\"trig_param\"], \"run_param_val\": trig_args[\"trig_param_val\"], } ) post_params: Dict[str, Dict[str, str]] = {\"json\": wf_defn} status_code, resp_msg, _ = elog_http_request( exp, endpoint=endpoint, request_type=\"POST\", **post_params )","title":"post_elog_workflow"},{"location":"source/io/exceptions/","text":"Specifies custom exceptions defined for IO problems. Raises: ElogFileFormatError \u2013 Raised if an attachment is specified in an incorrect format. ElogFileFormatError Bases: Exception Raised when an eLog attachment is specified in an invalid format. Source code in lute/io/exceptions.py class ElogFileFormatError(Exception): \"\"\"Raised when an eLog attachment is specified in an invalid format.\"\"\" ...","title":"exceptions"},{"location":"source/io/exceptions/#io.exceptions.ElogFileFormatError","text":"Bases: Exception Raised when an eLog attachment is specified in an invalid format. Source code in lute/io/exceptions.py class ElogFileFormatError(Exception): \"\"\"Raised when an eLog attachment is specified in an invalid format.\"\"\" ...","title":"ElogFileFormatError"},{"location":"source/io/models/base/","text":"Base classes for describing Task parameters. Classes: Name Description AnalysisHeader Model holding shared configuration across Tasks. E.g. experiment name, run number and working directory. TaskParameters Base class for Task parameters. Subclasses specify a model of parameters and their types for validation. ThirdPartyParameters Base class for Third-party, binary executable Tasks. TemplateParameters Dataclass to represent parameters of binary (third-party) Tasks which are used for additional config files. TemplateConfig Class for holding information on where templates are stored in order to properly handle ThirdPartyParameter objects. AnalysisHeader Bases: BaseModel Header information for LUTE analysis runs. Source code in lute/io/models/base.py class AnalysisHeader(BaseModel): \"\"\"Header information for LUTE analysis runs.\"\"\" title: str = Field( \"LUTE Task Configuration\", description=\"Description of the configuration or experiment.\", ) experiment: str = Field(\"\", description=\"Experiment.\") run: Union[str, int] = Field(\"\", description=\"Data acquisition run.\") date: str = Field(\"1970/01/01\", description=\"Start date of analysis.\") lute_version: Union[float, str] = Field( 0.1, description=\"Version of LUTE used for analysis.\" ) task_timeout: PositiveInt = Field( 600, description=( \"Time in seconds until a task times out. Should be slightly shorter\" \" than job timeout if using a job manager (e.g. SLURM).\" ), ) work_dir: str = Field(\"\", description=\"Main working directory for LUTE.\") @validator(\"work_dir\", always=True) def validate_work_dir(cls, directory: str, values: Dict[str, Any]) -> str: work_dir: str if directory == \"\": std_work_dir = ( f\"/sdf/data/lcls/ds/{values['experiment'][:3]}/\" f\"{values['experiment']}/scratch\" ) work_dir = std_work_dir else: work_dir = directory # Check existence and permissions if not os.path.exists(work_dir): raise ValueError(f\"Working Directory: {work_dir} does not exist!\") if not os.access(work_dir, os.W_OK): # Need write access for database, files etc. raise ValueError(f\"Not write access for working directory: {work_dir}!\") return work_dir @validator(\"run\", always=True) def validate_run( cls, run: Union[str, int], values: Dict[str, Any] ) -> Union[str, int]: if run == \"\": # From Airflow RUN_NUM should have Format \"RUN_DATETIME\" - Num is first part run_time: str = os.environ.get(\"RUN_NUM\", \"\") if run_time != \"\": return int(run_time.split(\"_\")[0]) return run @validator(\"experiment\", always=True) def validate_experiment(cls, experiment: str, values: Dict[str, Any]) -> str: if experiment == \"\": arp_exp: str = os.environ.get(\"EXPERIMENT\", \"EXPX00000\") return arp_exp return experiment TaskParameters Bases: BaseSettings Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). Source code in lute/io/models/base.py class TaskParameters(BaseSettings): \"\"\"Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note: Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). \"\"\" class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" lute_config: AnalysisHeader Config Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. Only used if set_result==True result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if set_result==True Source code in lute/io/models/base.py class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" impl_schemas: Optional[str] = None class-attribute instance-attribute Schema specification for output result. Will be passed to TaskResult. result_from_params: Optional[str] = None class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. result_summary: Optional[str] = None class-attribute instance-attribute Format a TaskResult.summary from output. run_directory: Optional[str] = None class-attribute instance-attribute Set the directory that the Task is run from. set_result: bool = False class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. TemplateConfig Bases: BaseModel Parameters used for templating of third party configuration files. Attributes: template_name ( str ) \u2013 The name of the template to use. This template must live in config/templates . output_path ( str ) \u2013 The FULL path, including filename to write the rendered template to. Source code in lute/io/models/base.py class TemplateConfig(BaseModel): \"\"\"Parameters used for templating of third party configuration files. Attributes: template_name (str): The name of the template to use. This template must live in `config/templates`. output_path (str): The FULL path, including filename to write the rendered template to. \"\"\" template_name: str output_path: str TemplateParameters Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable params. The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the params Field. Source code in lute/io/models/base.py @dataclass class TemplateParameters: \"\"\"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable `params.` The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the `params` Field. \"\"\" params: Any ThirdPartyParameters Bases: TaskParameters Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. Source code in lute/io/models/base.py class ThirdPartyParameters(TaskParameters): \"\"\"Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. \"\"\" class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" # lute_template_cfg: TemplateConfig @root_validator(pre=False) def extra_fields_to_thirdparty(cls, values: Dict[str, Any]): for key in values: if key not in cls.__fields__: values[key] = TemplateParameters(values[key]) return values Config Bases: Config Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base TaskParameters.Config class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. ThirdPartyTask-specific ( Optional [ str ] ) \u2013 extra ( str ) \u2013 \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq ( bool ) \u2013 False. If True, \"short\" command-line args are passed as -x=arg . ThirdPartyTask-specific. long_flags_use_eq ( bool ) \u2013 False. If True, \"long\" command-line args are passed as --long=arg . ThirdPartyTask-specific. Source code in lute/io/models/base.py class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = False class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. short_flags_use_eq: bool = False class-attribute instance-attribute Whether short command-line arguments are passed like -x=arg .","title":"base"},{"location":"source/io/models/base/#io.models.base.AnalysisHeader","text":"Bases: BaseModel Header information for LUTE analysis runs. Source code in lute/io/models/base.py class AnalysisHeader(BaseModel): \"\"\"Header information for LUTE analysis runs.\"\"\" title: str = Field( \"LUTE Task Configuration\", description=\"Description of the configuration or experiment.\", ) experiment: str = Field(\"\", description=\"Experiment.\") run: Union[str, int] = Field(\"\", description=\"Data acquisition run.\") date: str = Field(\"1970/01/01\", description=\"Start date of analysis.\") lute_version: Union[float, str] = Field( 0.1, description=\"Version of LUTE used for analysis.\" ) task_timeout: PositiveInt = Field( 600, description=( \"Time in seconds until a task times out. Should be slightly shorter\" \" than job timeout if using a job manager (e.g. SLURM).\" ), ) work_dir: str = Field(\"\", description=\"Main working directory for LUTE.\") @validator(\"work_dir\", always=True) def validate_work_dir(cls, directory: str, values: Dict[str, Any]) -> str: work_dir: str if directory == \"\": std_work_dir = ( f\"/sdf/data/lcls/ds/{values['experiment'][:3]}/\" f\"{values['experiment']}/scratch\" ) work_dir = std_work_dir else: work_dir = directory # Check existence and permissions if not os.path.exists(work_dir): raise ValueError(f\"Working Directory: {work_dir} does not exist!\") if not os.access(work_dir, os.W_OK): # Need write access for database, files etc. raise ValueError(f\"Not write access for working directory: {work_dir}!\") return work_dir @validator(\"run\", always=True) def validate_run( cls, run: Union[str, int], values: Dict[str, Any] ) -> Union[str, int]: if run == \"\": # From Airflow RUN_NUM should have Format \"RUN_DATETIME\" - Num is first part run_time: str = os.environ.get(\"RUN_NUM\", \"\") if run_time != \"\": return int(run_time.split(\"_\")[0]) return run @validator(\"experiment\", always=True) def validate_experiment(cls, experiment: str, values: Dict[str, Any]) -> str: if experiment == \"\": arp_exp: str = os.environ.get(\"EXPERIMENT\", \"EXPX00000\") return arp_exp return experiment","title":"AnalysisHeader"},{"location":"source/io/models/base/#io.models.base.TaskParameters","text":"Bases: BaseSettings Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). Source code in lute/io/models/base.py class TaskParameters(BaseSettings): \"\"\"Base class for models of task parameters to be validated. Parameters are read from a configuration YAML file and validated against subclasses of this type in order to ensure that both all parameters are present, and that the parameters are of the correct type. Note: Pydantic is used for data validation. Pydantic does not perform \"strict\" validation by default. Parameter values may be cast to conform with the model specified by the subclass definition if it is possible to do so. Consider whether this may cause issues (e.g. if a float is cast to an int). \"\"\" class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\" lute_config: AnalysisHeader","title":"TaskParameters"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config","text":"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. Only used if set_result==True result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if set_result==True Source code in lute/io/models/base.py class Config: \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration. A number of LUTE-specific configuration has also been placed here. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). False. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. Only used if `set_result==True` result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. Only used if `set_result==True` impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if `set_result==True`. \"\"\" env_prefix = \"LUTE_\" underscore_attrs_are_private: bool = True copy_on_model_validation: str = \"deep\" allow_inf_nan: bool = False run_directory: Optional[str] = None \"\"\"Set the directory that the Task is run from.\"\"\" set_result: bool = False \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: Optional[str] = None \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_summary: Optional[str] = None \"\"\"Format a TaskResult.summary from output.\"\"\" impl_schemas: Optional[str] = None \"\"\"Schema specification for output result. Will be passed to TaskResult.\"\"\"","title":"Config"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config.impl_schemas","text":"Schema specification for output result. Will be passed to TaskResult.","title":"impl_schemas"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config.result_summary","text":"Format a TaskResult.summary from output.","title":"result_summary"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config.run_directory","text":"Set the directory that the Task is run from.","title":"run_directory"},{"location":"source/io/models/base/#io.models.base.TaskParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/base/#io.models.base.TemplateConfig","text":"Bases: BaseModel Parameters used for templating of third party configuration files. Attributes: template_name ( str ) \u2013 The name of the template to use. This template must live in config/templates . output_path ( str ) \u2013 The FULL path, including filename to write the rendered template to. Source code in lute/io/models/base.py class TemplateConfig(BaseModel): \"\"\"Parameters used for templating of third party configuration files. Attributes: template_name (str): The name of the template to use. This template must live in `config/templates`. output_path (str): The FULL path, including filename to write the rendered template to. \"\"\" template_name: str output_path: str","title":"TemplateConfig"},{"location":"source/io/models/base/#io.models.base.TemplateParameters","text":"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable params. The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the params Field. Source code in lute/io/models/base.py @dataclass class TemplateParameters: \"\"\"Class for representing parameters for third party configuration files. These parameters can represent arbitrary data types and are used in conjunction with templates for modifying third party configuration files from the single LUTE YAML. Due to the storage of arbitrary data types, and the use of a template file, a single instance of this class can hold from a single template variable to an entire configuration file. The data parsing is done by jinja using the complementary template. All data is stored in the single model variable `params.` The pydantic \"dataclass\" is used over the BaseModel/Settings to allow positional argument instantiation of the `params` Field. \"\"\" params: Any","title":"TemplateParameters"},{"location":"source/io/models/base/#io.models.base.ThirdPartyParameters","text":"Bases: TaskParameters Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. Source code in lute/io/models/base.py class ThirdPartyParameters(TaskParameters): \"\"\"Base class for third party task parameters. Contains special validators for extra arguments and handling of parameters used for filling in third party configuration files. \"\"\" class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" # lute_template_cfg: TemplateConfig @root_validator(pre=False) def extra_fields_to_thirdparty(cls, values: Dict[str, Any]): for key in values: if key not in cls.__fields__: values[key] = TemplateParameters(values[key]) return values","title":"ThirdPartyParameters"},{"location":"source/io/models/base/#io.models.base.ThirdPartyParameters.Config","text":"Bases: Config Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base TaskParameters.Config class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix ( str ) \u2013 Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter input can be set with an environment variable: {env_prefix}input , in LUTE's case LUTE_input . underscore_attrs_are_private ( bool ) \u2013 Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation ( str ) \u2013 Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan ( bool ) \u2013 Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory ( Optional [ str ] ) \u2013 None. If set, it should be a valid path. The Task will be run from this directory. This may be useful for some Task s which rely on searching the working directory. result_from_params ( Optional [ str ] ) \u2013 None. Optionally used to define results from information available in the model using a custom validator. E.g. use a outdir and filename field to set result_from_params=f\"{outdir}/{filename} , etc. result_summary ( Optional [ str ] ) \u2013 None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. ThirdPartyTask-specific ( Optional [ str ] ) \u2013 extra ( str ) \u2013 \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq ( bool ) \u2013 False. If True, \"short\" command-line args are passed as -x=arg . ThirdPartyTask-specific. long_flags_use_eq ( bool ) \u2013 False. If True, \"long\" command-line args are passed as --long=arg . ThirdPartyTask-specific. Source code in lute/io/models/base.py class Config(TaskParameters.Config): \"\"\"Configuration for parameters model. The Config class holds Pydantic configuration and inherited configuration from the base `TaskParameters.Config` class. A number of values are also overridden, and there are some specific configuration options to ThirdPartyParameters. A full list of options (with TaskParameters options repeated) is described below. Attributes: env_prefix (str): Pydantic configuration. Will set parameters from environment variables containing this prefix. E.g. a model parameter `input` can be set with an environment variable: `{env_prefix}input`, in LUTE's case `LUTE_input`. underscore_attrs_are_private (bool): Pydantic configuration. Whether to hide attributes (parameters) prefixed with an underscore. copy_on_model_validation (str): Pydantic configuration. How to copy the input object passed to the class instance for model validation. Set to perform a deep copy. allow_inf_nan (bool): Pydantic configuration. Whether to allow infinity or NAN in float fields. run_directory (Optional[str]): None. If set, it should be a valid path. The `Task` will be run from this directory. This may be useful for some `Task`s which rely on searching the working directory. set_result (bool). True. If True, the model has information about setting the TaskResult object from the parameters it contains. E.g. it has an `output` parameter which is marked as the result. The result can be set with a field value of `is_result=True` on a specific parameter, or using `result_from_params` and a validator. result_from_params (Optional[str]): None. Optionally used to define results from information available in the model using a custom validator. E.g. use a `outdir` and `filename` field to set `result_from_params=f\"{outdir}/{filename}`, etc. result_summary (Optional[str]): None. Defines a result summary that can be known after processing the Pydantic model. Use of summary depends on the Executor running the Task. All summaries are stored in the database, however. impl_schemas (Optional[str]). Specifies a the schemas the output/results conform to. Only used if set_result is True. ----------------------- ThirdPartyTask-specific: extra (str): \"allow\". Pydantic configuration. Allow (or ignore) extra arguments. short_flags_use_eq (bool): False. If True, \"short\" command-line args are passed as `-x=arg`. ThirdPartyTask-specific. long_flags_use_eq (bool): False. If True, \"long\" command-line args are passed as `--long=arg`. ThirdPartyTask-specific. \"\"\" extra: str = \"allow\" short_flags_use_eq: bool = False \"\"\"Whether short command-line arguments are passed like `-x=arg`.\"\"\" long_flags_use_eq: bool = False \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/base/#io.models.base.ThirdPartyParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/models/base/#io.models.base.ThirdPartyParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/base/#io.models.base.ThirdPartyParameters.Config.short_flags_use_eq","text":"Whether short command-line arguments are passed like -x=arg .","title":"short_flags_use_eq"},{"location":"source/io/models/sfx_find_peaks/","text":"FindPeaksPsocakeParameters Bases: ThirdPartyParameters Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPsocakeParameters(ThirdPartyParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" class SZParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description=\"SZ compression algorithm (qoz, sz3)\" ) binSize: int = Field(2, description=\"SZ compression's bin size paramater\") roiWindowSize: int = Field( 2, description=\"SZ compression's ROI window size paramater\" ) absError: float = Field(10, descriptionp=\"Maximum absolute error value\") executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) mca: str = Field( \"btl ^openib\", description=\"Mca option for the MPI executable\", flag_type=\"--\" ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) p_arg2: str = Field( \"findPeaksSZ.py\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\", ) d: str = Field(description=\"Detector name\", flag_type=\"-\") e: str = Field(\"\", description=\"Experiment name\", flag_type=\"-\") r: int = Field(-1, description=\"Run number\", flag_type=\"-\") outDir: str = Field( description=\"Output directory where .cxi will be saved\", flag_type=\"--\" ) algorithm: int = Field(1, description=\"PyAlgos algorithm to use\", flag_type=\"--\") alg_npix_min: float = Field( 1.0, description=\"PyAlgos algorithm's npix_min parameter\", flag_type=\"--\" ) alg_npix_max: float = Field( 45.0, description=\"PyAlgos algorithm's npix_max parameter\", flag_type=\"--\" ) alg_amax_thr: float = Field( 250.0, description=\"PyAlgos algorithm's amax_thr parameter\", flag_type=\"--\" ) alg_atot_thr: float = Field( 330.0, description=\"PyAlgos algorithm's atot_thr parameter\", flag_type=\"--\" ) alg_son_min: float = Field( 10.0, description=\"PyAlgos algorithm's son_min parameter\", flag_type=\"--\" ) alg1_thr_low: float = Field( 80.0, description=\"PyAlgos algorithm's thr_low parameter\", flag_type=\"--\" ) alg1_thr_high: float = Field( 270.0, description=\"PyAlgos algorithm's thr_high parameter\", flag_type=\"--\" ) alg1_rank: int = Field( 3, description=\"PyAlgos algorithm's rank parameter\", flag_type=\"--\" ) alg1_radius: int = Field( 3, description=\"PyAlgos algorithm's radius parameter\", flag_type=\"--\" ) alg1_dr: int = Field( 1, description=\"PyAlgos algorithm's dr parameter\", flag_type=\"--\" ) psanaMask_on: str = Field( \"True\", description=\"Whether psana's mask should be used\", flag_type=\"--\" ) psanaMask_calib: str = Field( \"True\", description=\"Psana mask's calib parameter\", flag_type=\"--\" ) psanaMask_status: str = Field( \"True\", description=\"Psana mask's status parameter\", flag_type=\"--\" ) psanaMask_edges: str = Field( \"True\", description=\"Psana mask's edges parameter\", flag_type=\"--\" ) psanaMask_central: str = Field( \"True\", description=\"Psana mask's central parameter\", flag_type=\"--\" ) psanaMask_unbond: str = Field( \"True\", description=\"Psana mask's unbond parameter\", flag_type=\"--\" ) psanaMask_unbondnrs: str = Field( \"True\", description=\"Psana mask's unbondnbrs parameter\", flag_type=\"--\" ) mask: str = Field( \"\", description=\"Path to an additional mask to apply\", flag_type=\"--\" ) clen: str = Field( description=\"Epics variable storing the camera length\", flag_type=\"--\" ) coffset: float = Field(0, description=\"Camera offset in m\", flag_type=\"--\") minPeaks: int = Field( 15, description=\"Minimum number of peaks to mark frame for indexing\", flag_type=\"--\", ) maxPeaks: int = Field( 15, description=\"Maximum number of peaks to mark frame for indexing\", flag_type=\"--\", ) minRes: int = Field( 0, description=\"Minimum peak resolution to mark frame for indexing \", flag_type=\"--\", ) sample: str = Field(\"\", description=\"Sample name\", flag_type=\"--\") instrument: Union[None, str] = Field( None, description=\"Instrument name\", flag_type=\"--\" ) pixelSize: float = Field(0.0, description=\"Pixel size\", flag_type=\"--\") auto: str = Field( \"False\", description=( \"Whether to automatically determine peak per event peak \" \"finding parameters\" ), flag_type=\"--\", ) detectorDistance: float = Field( 0.0, description=\"Detector distance from interaction point in m\", flag_type=\"--\" ) access: Literal[\"ana\", \"ffb\"] = Field( \"ana\", description=\"Data node type: {ana,ffb}\", flag_type=\"--\" ) szfile: str = Field(\"qoz.json\", description=\"Path to SZ's JSON configuration file\") lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"sz.json\", output_path=\"\", # Will want to change where this goes... ), description=\"Template information for the sz.json file\", ) sz_parameters: SZParameters = Field( description=\"Configuration parameters for SZ Compression\", flag_type=\"\" ) @validator(\"e\", always=True) def validate_e(cls, e: str, values: Dict[str, Any]) -> str: if e == \"\": return values[\"lute_config\"].experiment return e @validator(\"r\", always=True) def validate_r(cls, r: int, values: Dict[str, Any]) -> int: if r == -1: return values[\"lute_config\"].run return r @validator(\"lute_template_cfg\", always=True) def set_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"szfile\"] return lute_template_cfg @validator(\"sz_parameters\", always=True) def set_sz_compression_parameters( cls, sz_parameters: SZParameters, values: Dict[str, Any] ) -> None: values[\"compressor\"] = sz_parameters.compressor values[\"binSize\"] = sz_parameters.binSize values[\"roiWindowSize\"] = sz_parameters.roiWindowSize if sz_parameters.compressor == \"qoz\": values[\"pressio_opts\"] = { \"pressio:abs\": sz_parameters.absError, \"qoz\": {\"qoz:stride\": 8}, } else: values[\"pressio_opts\"] = {\"pressio:abs\": sz_parameters.absError} return None @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) directory: str = values[\"outDir\"] fname: str = f\"{exp}_{run:04d}.lst\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values Config Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_from_params: str = '' class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. FindPeaksPyAlgosParameters Bases: TaskParameters Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPyAlgosParameters(TaskParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" class SZCompressorParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description='Compression algorithm (\"qoz\" or \"sz3\")' ) abs_error: float = Field(10.0, description=\"Absolute error bound\") bin_size: int = Field(2, description=\"Bin size\") roi_window_size: int = Field( 9, description=\"Default window size\", ) outdir: str = Field( description=\"Output directory for cxi files\", ) n_events: int = Field( 0, description=\"Number of events to process (0 to process all events)\", ) det_name: str = Field( description=\"Psana name of the detector storing the image data\", ) event_receiver: Literal[\"evr0\", \"evr1\"] = Field( description=\"Event Receiver to be used: evr0 or evr1\", ) tag: str = Field( \"\", description=\"Tag to add to the output file names\", ) pv_camera_length: Union[str, float] = Field( \"\", description=\"PV associated with camera length \" \"(if a number, camera length directly)\", ) event_logic: bool = Field( False, description=\"True if only events with a specific event code should be \" \"processed. False if the event code should be ignored\", ) event_code: int = Field( 0, description=\"Required events code for events to be processed if event logic \" \"is True\", ) psana_mask: bool = Field( False, description=\"If True, apply mask from psana Detector object\", ) mask_file: Union[str, None] = Field( None, description=\"File with a custom mask to apply. If None, no custom mask is \" \"applied\", ) min_peaks: int = Field(2, description=\"Minimum number of peaks per image\") max_peaks: int = Field( 2048, description=\"Maximum number of peaks per image\", ) npix_min: int = Field( 2, description=\"Minimum number of pixels per peak\", ) npix_max: int = Field( 30, description=\"Maximum number of pixels per peak\", ) amax_thr: float = Field( 80.0, description=\"Minimum intensity threshold for starting a peak\", ) atot_thr: float = Field( 120.0, description=\"Minimum summed intensity threshold for pixel collection\", ) son_min: float = Field( 7.0, description=\"Minimum signal-to-noise ratio to be considered a peak\", ) peak_rank: int = Field( 3, description=\"Radius in which central peak pixel is a local maximum\", ) r0: float = Field( 3.0, description=\"Radius of ring for background evaluation in pixels\", ) dr: float = Field( 2.0, description=\"Width of ring for background evaluation in pixels\", ) nsigm: float = Field( 7.0, description=\"Intensity threshold to include pixel in connected group\", ) compression: Optional[SZCompressorParameters] = Field( None, description=\"Options for the SZ Compression Algorithm\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": fname: Path = ( Path(values[\"outdir\"]) / f\"{values['lute_config'].experiment}_{values['lute_config'].run}_\" f\"{values['tag']}.list\" ) return str(fname) return out_file Config Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result.","title":"sfx_find_peaks"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPsocakeParameters","text":"Bases: ThirdPartyParameters Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPsocakeParameters(ThirdPartyParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using Psocake. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. NOTE: This Task is deprecated and provided for compatibility only. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" class SZParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description=\"SZ compression algorithm (qoz, sz3)\" ) binSize: int = Field(2, description=\"SZ compression's bin size paramater\") roiWindowSize: int = Field( 2, description=\"SZ compression's ROI window size paramater\" ) absError: float = Field(10, descriptionp=\"Maximum absolute error value\") executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) mca: str = Field( \"btl ^openib\", description=\"Mca option for the MPI executable\", flag_type=\"--\" ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) p_arg2: str = Field( \"findPeaksSZ.py\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\", ) d: str = Field(description=\"Detector name\", flag_type=\"-\") e: str = Field(\"\", description=\"Experiment name\", flag_type=\"-\") r: int = Field(-1, description=\"Run number\", flag_type=\"-\") outDir: str = Field( description=\"Output directory where .cxi will be saved\", flag_type=\"--\" ) algorithm: int = Field(1, description=\"PyAlgos algorithm to use\", flag_type=\"--\") alg_npix_min: float = Field( 1.0, description=\"PyAlgos algorithm's npix_min parameter\", flag_type=\"--\" ) alg_npix_max: float = Field( 45.0, description=\"PyAlgos algorithm's npix_max parameter\", flag_type=\"--\" ) alg_amax_thr: float = Field( 250.0, description=\"PyAlgos algorithm's amax_thr parameter\", flag_type=\"--\" ) alg_atot_thr: float = Field( 330.0, description=\"PyAlgos algorithm's atot_thr parameter\", flag_type=\"--\" ) alg_son_min: float = Field( 10.0, description=\"PyAlgos algorithm's son_min parameter\", flag_type=\"--\" ) alg1_thr_low: float = Field( 80.0, description=\"PyAlgos algorithm's thr_low parameter\", flag_type=\"--\" ) alg1_thr_high: float = Field( 270.0, description=\"PyAlgos algorithm's thr_high parameter\", flag_type=\"--\" ) alg1_rank: int = Field( 3, description=\"PyAlgos algorithm's rank parameter\", flag_type=\"--\" ) alg1_radius: int = Field( 3, description=\"PyAlgos algorithm's radius parameter\", flag_type=\"--\" ) alg1_dr: int = Field( 1, description=\"PyAlgos algorithm's dr parameter\", flag_type=\"--\" ) psanaMask_on: str = Field( \"True\", description=\"Whether psana's mask should be used\", flag_type=\"--\" ) psanaMask_calib: str = Field( \"True\", description=\"Psana mask's calib parameter\", flag_type=\"--\" ) psanaMask_status: str = Field( \"True\", description=\"Psana mask's status parameter\", flag_type=\"--\" ) psanaMask_edges: str = Field( \"True\", description=\"Psana mask's edges parameter\", flag_type=\"--\" ) psanaMask_central: str = Field( \"True\", description=\"Psana mask's central parameter\", flag_type=\"--\" ) psanaMask_unbond: str = Field( \"True\", description=\"Psana mask's unbond parameter\", flag_type=\"--\" ) psanaMask_unbondnrs: str = Field( \"True\", description=\"Psana mask's unbondnbrs parameter\", flag_type=\"--\" ) mask: str = Field( \"\", description=\"Path to an additional mask to apply\", flag_type=\"--\" ) clen: str = Field( description=\"Epics variable storing the camera length\", flag_type=\"--\" ) coffset: float = Field(0, description=\"Camera offset in m\", flag_type=\"--\") minPeaks: int = Field( 15, description=\"Minimum number of peaks to mark frame for indexing\", flag_type=\"--\", ) maxPeaks: int = Field( 15, description=\"Maximum number of peaks to mark frame for indexing\", flag_type=\"--\", ) minRes: int = Field( 0, description=\"Minimum peak resolution to mark frame for indexing \", flag_type=\"--\", ) sample: str = Field(\"\", description=\"Sample name\", flag_type=\"--\") instrument: Union[None, str] = Field( None, description=\"Instrument name\", flag_type=\"--\" ) pixelSize: float = Field(0.0, description=\"Pixel size\", flag_type=\"--\") auto: str = Field( \"False\", description=( \"Whether to automatically determine peak per event peak \" \"finding parameters\" ), flag_type=\"--\", ) detectorDistance: float = Field( 0.0, description=\"Detector distance from interaction point in m\", flag_type=\"--\" ) access: Literal[\"ana\", \"ffb\"] = Field( \"ana\", description=\"Data node type: {ana,ffb}\", flag_type=\"--\" ) szfile: str = Field(\"qoz.json\", description=\"Path to SZ's JSON configuration file\") lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"sz.json\", output_path=\"\", # Will want to change where this goes... ), description=\"Template information for the sz.json file\", ) sz_parameters: SZParameters = Field( description=\"Configuration parameters for SZ Compression\", flag_type=\"\" ) @validator(\"e\", always=True) def validate_e(cls, e: str, values: Dict[str, Any]) -> str: if e == \"\": return values[\"lute_config\"].experiment return e @validator(\"r\", always=True) def validate_r(cls, r: int, values: Dict[str, Any]) -> int: if r == -1: return values[\"lute_config\"].run return r @validator(\"lute_template_cfg\", always=True) def set_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"szfile\"] return lute_template_cfg @validator(\"sz_parameters\", always=True) def set_sz_compression_parameters( cls, sz_parameters: SZParameters, values: Dict[str, Any] ) -> None: values[\"compressor\"] = sz_parameters.compressor values[\"binSize\"] = sz_parameters.binSize values[\"roiWindowSize\"] = sz_parameters.roiWindowSize if sz_parameters.compressor == \"qoz\": values[\"pressio_opts\"] = { \"pressio:abs\": sz_parameters.absError, \"qoz\": {\"qoz:stride\": 8}, } else: values[\"pressio_opts\"] = {\"pressio:abs\": sz_parameters.absError} return None @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) directory: str = values[\"outDir\"] fname: str = f\"{exp}_{run:04d}.lst\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values","title":"FindPeaksPsocakeParameters"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPsocakeParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPsocakeParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPsocakeParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPyAlgosParameters","text":"Bases: TaskParameters Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. Source code in lute/io/models/sfx_find_peaks.py class FindPeaksPyAlgosParameters(TaskParameters): \"\"\"Parameters for crystallographic (Bragg) peak finding using PyAlgos. This peak finding Task optionally has the ability to compress/decompress data with SZ for the purpose of compression validation. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" class SZCompressorParameters(BaseModel): compressor: Literal[\"qoz\", \"sz3\"] = Field( \"qoz\", description='Compression algorithm (\"qoz\" or \"sz3\")' ) abs_error: float = Field(10.0, description=\"Absolute error bound\") bin_size: int = Field(2, description=\"Bin size\") roi_window_size: int = Field( 9, description=\"Default window size\", ) outdir: str = Field( description=\"Output directory for cxi files\", ) n_events: int = Field( 0, description=\"Number of events to process (0 to process all events)\", ) det_name: str = Field( description=\"Psana name of the detector storing the image data\", ) event_receiver: Literal[\"evr0\", \"evr1\"] = Field( description=\"Event Receiver to be used: evr0 or evr1\", ) tag: str = Field( \"\", description=\"Tag to add to the output file names\", ) pv_camera_length: Union[str, float] = Field( \"\", description=\"PV associated with camera length \" \"(if a number, camera length directly)\", ) event_logic: bool = Field( False, description=\"True if only events with a specific event code should be \" \"processed. False if the event code should be ignored\", ) event_code: int = Field( 0, description=\"Required events code for events to be processed if event logic \" \"is True\", ) psana_mask: bool = Field( False, description=\"If True, apply mask from psana Detector object\", ) mask_file: Union[str, None] = Field( None, description=\"File with a custom mask to apply. If None, no custom mask is \" \"applied\", ) min_peaks: int = Field(2, description=\"Minimum number of peaks per image\") max_peaks: int = Field( 2048, description=\"Maximum number of peaks per image\", ) npix_min: int = Field( 2, description=\"Minimum number of pixels per peak\", ) npix_max: int = Field( 30, description=\"Maximum number of pixels per peak\", ) amax_thr: float = Field( 80.0, description=\"Minimum intensity threshold for starting a peak\", ) atot_thr: float = Field( 120.0, description=\"Minimum summed intensity threshold for pixel collection\", ) son_min: float = Field( 7.0, description=\"Minimum signal-to-noise ratio to be considered a peak\", ) peak_rank: int = Field( 3, description=\"Radius in which central peak pixel is a local maximum\", ) r0: float = Field( 3.0, description=\"Radius of ring for background evaluation in pixels\", ) dr: float = Field( 2.0, description=\"Width of ring for background evaluation in pixels\", ) nsigm: float = Field( 7.0, description=\"Intensity threshold to include pixel in connected group\", ) compression: Optional[SZCompressorParameters] = Field( None, description=\"Options for the SZ Compression Algorithm\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": fname: Path = ( Path(values[\"outdir\"]) / f\"{values['lute_config'].experiment}_{values['lute_config'].run}_\" f\"{values['tag']}.list\" ) return str(fname) return out_file","title":"FindPeaksPyAlgosParameters"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPyAlgosParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_find_peaks.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_find_peaks/#io.models.sfx_find_peaks.FindPeaksPyAlgosParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_index/","text":"Models for serial femtosecond crystallography indexing. Classes: Name Description IndexCrystFELParameters Perform indexing of hits/peaks using CrystFEL's indexamajig . ConcatenateStreamFilesParameters Bases: TaskParameters Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. Source code in lute/io/models/sfx_index.py class ConcatenateStreamFilesParameters(TaskParameters): \"\"\"Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" in_file: str = Field( \"\", description=\"Root of directory tree storing stream files to merge.\", ) tag: Optional[str] = Field( \"\", description=\"Tag identifying the stream files to merge.\", ) out_file: str = Field( \"\", description=\"Path to merged output stream file.\", is_result=True ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_dir: str = str(Path(stream_file).parent) return stream_dir return in_file @validator(\"tag\", always=True) def validate_tag(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_tag: str = Path(stream_file).name.split(\"_\")[0] return stream_tag return tag @validator(\"out_file\", always=True) def validate_out_file(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_out_file: str = str( Path(values[\"in_file\"]).parent / f\"{values['tag'].stream}\" ) return stream_out_file return tag Config Bases: Config Source code in lute/io/models/sfx_index.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. IndexCrystFELParameters Bases: ThirdPartyParameters Parameters for CrystFEL's indexamajig . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Source code in lute/io/models/sfx_index.py class IndexCrystFELParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html \"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/indexamajig\", description=\"CrystFEL's indexing binary.\", flag_type=\"\", ) # Basic options in_file: Optional[str] = Field( \"\", description=\"Path to input file.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) geometry: str = Field( \"\", description=\"Path to geometry file.\", flag_type=\"-\", rename_param=\"g\" ) zmq_input: Optional[str] = Field( description=\"ZMQ address to receive data over. `input` and `zmq-input` are mutually exclusive\", flag_type=\"--\", rename_param=\"zmq-input\", ) zmq_subscribe: Optional[str] = Field( # Can be used multiple times... description=\"Subscribe to ZMQ message of type `tag`\", flag_type=\"--\", rename_param=\"zmq-subscribe\", ) zmq_request: Optional[AnyUrl] = Field( description=\"Request new data over ZMQ by sending this value\", flag_type=\"--\", rename_param=\"zmq-request\", ) asapo_endpoint: Optional[str] = Field( description=\"ASAP::O endpoint. zmq-input and this are mutually exclusive.\", flag_type=\"--\", rename_param=\"asapo-endpoint\", ) asapo_token: Optional[str] = Field( description=\"ASAP::O authentication token.\", flag_type=\"--\", rename_param=\"asapo-token\", ) asapo_beamtime: Optional[str] = Field( description=\"ASAP::O beatime.\", flag_type=\"--\", rename_param=\"asapo-beamtime\", ) asapo_source: Optional[str] = Field( description=\"ASAP::O data source.\", flag_type=\"--\", rename_param=\"asapo-source\", ) asapo_group: Optional[str] = Field( description=\"ASAP::O consumer group.\", flag_type=\"--\", rename_param=\"asapo-group\", ) asapo_stream: Optional[str] = Field( description=\"ASAP::O stream.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) asapo_wait_for_stream: Optional[str] = Field( description=\"If ASAP::O stream does not exist, wait for it to appear.\", flag_type=\"--\", rename_param=\"asapo-wait-for-stream\", ) data_format: Optional[str] = Field( description=\"Specify format for ZMQ or ASAP::O. `msgpack`, `hdf5` or `seedee`.\", flag_type=\"--\", rename_param=\"data-format\", ) basename: bool = Field( False, description=\"Remove directory parts of filenames. Acts before prefix if prefix also given.\", flag_type=\"--\", ) prefix: Optional[str] = Field( description=\"Add a prefix to the filenames from the infile argument.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) nthreads: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of threads to use. See also `max_indexer_threads`.\", flag_type=\"-\", rename_param=\"j\", ) no_check_prefix: bool = Field( False, description=\"Don't attempt to correct the prefix if it seems incorrect.\", flag_type=\"--\", rename_param=\"no-check-prefix\", ) highres: Optional[float] = Field( description=\"Mark all pixels greater than `x` has bad.\", flag_type=\"--\" ) profile: bool = Field( False, description=\"Display timing data to monitor performance.\", flag_type=\"--\" ) temp_dir: Optional[str] = Field( description=\"Specify a path for the temp files folder.\", flag_type=\"--\", rename_param=\"temp-dir\", ) wait_for_file: conint(gt=-2) = Field( 0, description=\"Wait at most `x` seconds for a file to be created. A value of -1 means wait forever.\", flag_type=\"--\", rename_param=\"wait-for-file\", ) no_image_data: bool = Field( False, description=\"Load only the metadata, no iamges. Can check indexability without high data requirements.\", flag_type=\"--\", rename_param=\"no-image-data\", ) # Peak-finding options # .... # Indexing options indexing: Optional[str] = Field( description=\"Comma-separated list of supported indexing algorithms to use. Default is to automatically detect.\", flag_type=\"--\", ) cell_file: Optional[str] = Field( description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) tolerance: str = Field( \"5,5,5,1.5\", description=( \"Tolerances (in percent) for unit cell comparison. \" \"Comma-separated list a,b,c,angle. Default=5,5,5,1.5\" ), flag_type=\"--\", ) no_check_cell: bool = Field( False, description=\"Do not check cell parameters against unit cell. Replaces '-raw' method.\", flag_type=\"--\", rename_param=\"no-check-cell\", ) no_check_peaks: bool = Field( False, description=\"Do not verify peaks are accounted for by solution.\", flag_type=\"--\", rename_param=\"no-check-peaks\", ) multi: bool = Field( False, description=\"Enable multi-lattice indexing.\", flag_type=\"--\" ) wavelength_estimate: Optional[float] = Field( description=\"Estimate for X-ray wavelength. Required for some methods.\", flag_type=\"--\", rename_param=\"wavelength-estimate\", ) camera_length_estimate: Optional[float] = Field( description=\"Estimate for camera distance. Required for some methods.\", flag_type=\"--\", rename_param=\"camera-length-estimate\", ) max_indexer_threads: Optional[PositiveInt] = Field( # 1, description=\"Some indexing algos can use multiple threads. In addition to image-based.\", flag_type=\"--\", rename_param=\"max-indexer-threads\", ) no_retry: bool = Field( False, description=\"Do not remove weak peaks and try again.\", flag_type=\"--\", rename_param=\"no-retry\", ) no_refine: bool = Field( False, description=\"Skip refinement step.\", flag_type=\"--\", rename_param=\"no-refine\", ) no_revalidate: bool = Field( False, description=\"Skip revalidation step.\", flag_type=\"--\", rename_param=\"no-revalidate\", ) # TakeTwo specific parameters taketwo_member_threshold: Optional[PositiveInt] = Field( # 20, description=\"Minimum number of vectors to consider.\", flag_type=\"--\", rename_param=\"taketwo-member-threshold\", ) taketwo_len_tolerance: Optional[PositiveFloat] = Field( # 0.001, description=\"TakeTwo length tolerance in Angstroms.\", flag_type=\"--\", rename_param=\"taketwo-len-tolerance\", ) taketwo_angle_tolerance: Optional[PositiveFloat] = Field( # 0.6, description=\"TakeTwo angle tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-angle-tolerance\", ) taketwo_trace_tolerance: Optional[PositiveFloat] = Field( # 3, description=\"Matrix trace tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-trace-tolerance\", ) # Felix-specific parameters # felix_domega # felix-fraction-max-visits # felix-max-internal-angle # felix-max-uniqueness # felix-min-completeness # felix-min-visits # felix-num-voxels # felix-sigma # felix-tthrange-max # felix-tthrange-min # XGANDALF-specific parameters xgandalf_sampling_pitch: Optional[NonNegativeInt] = Field( # 6, description=\"Density of reciprocal space sampling.\", flag_type=\"--\", rename_param=\"xgandalf-sampling-pitch\", ) xgandalf_grad_desc_iterations: Optional[NonNegativeInt] = Field( # 4, description=\"Number of gradient descent iterations.\", flag_type=\"--\", rename_param=\"xgandalf-grad-desc-iterations\", ) xgandalf_tolerance: Optional[PositiveFloat] = Field( # 0.02, description=\"Relative tolerance of lattice vectors\", flag_type=\"--\", rename_param=\"xgandalf-tolerance\", ) xgandalf_no_deviation_from_provided_cell: Optional[bool] = Field( description=\"Found unit cell must match provided.\", flag_type=\"--\", rename_param=\"xgandalf-no-deviation-from-provided-cell\", ) xgandalf_min_lattice_vector_length: Optional[PositiveFloat] = Field( # 30, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-min-lattice-vector-length\", ) xgandalf_max_lattice_vector_length: Optional[PositiveFloat] = Field( # 250, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-max-lattice-vector-length\", ) xgandalf_max_peaks: Optional[PositiveInt] = Field( # 250, description=\"Maximum number of peaks to use for indexing.\", flag_type=\"--\", rename_param=\"xgandalf-max-peaks\", ) xgandalf_fast_execution: bool = Field( False, description=\"Shortcut to set sampling-pitch=2, and grad-desc-iterations=3.\", flag_type=\"--\", rename_param=\"xgandalf-fast-execution\", ) # pinkIndexer parameters # ... # asdf_fast: bool = Field(False, description=\"Enable fast mode for asdf. 3x faster for 7% loss in accuracy.\", flag_type=\"--\", rename_param=\"asdf-fast\") # Integration parameters integration: str = Field( \"rings-nocen\", description=\"Method for integrating reflections.\", flag_type=\"--\" ) fix_profile_radius: Optional[float] = Field( description=\"Fix the profile radius (m^{-1})\", flag_type=\"--\", rename_param=\"fix-profile-radius\", ) fix_divergence: Optional[float] = Field( 0, description=\"Fix the divergence (rad, full angle).\", flag_type=\"--\", rename_param=\"fix-divergence\", ) int_radius: str = Field( \"4,5,7\", description=\"Inner, middle, and outer radii for 3-ring integration.\", flag_type=\"--\", rename_param=\"int-radius\", ) int_diag: str = Field( \"none\", description=\"Show detailed information on integration when condition is met.\", flag_type=\"--\", rename_param=\"int-diag\", ) push_res: str = Field( \"infinity\", description=\"Integrate `x` higher than apparent resolution limit (nm-1).\", flag_type=\"--\", rename_param=\"push-res\", ) overpredict: bool = Field( False, description=\"Over-predict reflections. Maybe useful with post-refinement.\", flag_type=\"--\", ) cell_parameters_only: bool = Field( False, description=\"Do not predict refletions at all\", flag_type=\"--\" ) # Output parameters no_non_hits_in_stream: bool = Field( False, description=\"Exclude non-hits from the stream file.\", flag_type=\"--\", rename_param=\"no-non-hits-in-stream\", ) copy_hheader: Optional[str] = Field( description=\"Copy information from header in the image to output stream.\", flag_type=\"--\", rename_param=\"copy-hheader\", ) no_peaks_in_stream: bool = Field( False, description=\"Do not record peaks in stream file.\", flag_type=\"--\", rename_param=\"no-peaks-in-stream\", ) no_refls_in_stream: bool = Field( False, description=\"Do not record reflections in stream.\", flag_type=\"--\", rename_param=\"no-refls-in-stream\", ) serial_offset: Optional[PositiveInt] = Field( description=\"Start numbering at `x` instead of 1.\", flag_type=\"--\", rename_param=\"serial-offset\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": filename: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPyAlgos\", \"out_file\" ) if filename is None: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) tag: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"tag\" ) out_dir: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"outDir\" ) if out_dir is not None: fname: str = f\"{out_dir}/{exp}_{run:04d}\" if tag is not None: fname = f\"{fname}_{tag}\" return f\"{fname}.lst\" else: return filename return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": expmt: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) work_dir: str = values[\"lute_config\"].work_dir fname: str = f\"{expmt}_r{run:04d}.stream\" return f\"{work_dir}/{fname}\" return out_file Config Bases: Config Source code in lute/io/models/sfx_index.py class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result.","title":"sfx_index"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.ConcatenateStreamFilesParameters","text":"Bases: TaskParameters Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. Source code in lute/io/models/sfx_index.py class ConcatenateStreamFilesParameters(TaskParameters): \"\"\"Parameters for stream concatenation. Concatenates the stream file output from CrystFEL indexing for multiple experimental runs. \"\"\" class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" in_file: str = Field( \"\", description=\"Root of directory tree storing stream files to merge.\", ) tag: Optional[str] = Field( \"\", description=\"Tag identifying the stream files to merge.\", ) out_file: str = Field( \"\", description=\"Path to merged output stream file.\", is_result=True ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_dir: str = str(Path(stream_file).parent) return stream_dir return in_file @validator(\"tag\", always=True) def validate_tag(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"out_file\" ) if stream_file: stream_tag: str = Path(stream_file).name.split(\"_\")[0] return stream_tag return tag @validator(\"out_file\", always=True) def validate_out_file(cls, tag: str, values: Dict[str, Any]) -> str: if tag == \"\": stream_out_file: str = str( Path(values[\"in_file\"]).parent / f\"{values['tag'].stream}\" ) return stream_out_file return tag","title":"ConcatenateStreamFilesParameters"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.ConcatenateStreamFilesParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_index.py class Config(TaskParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.ConcatenateStreamFilesParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.IndexCrystFELParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's indexamajig . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html Source code in lute/io/models/sfx_index.py class IndexCrystFELParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `indexamajig`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-indexamajig.html \"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/indexamajig\", description=\"CrystFEL's indexing binary.\", flag_type=\"\", ) # Basic options in_file: Optional[str] = Field( \"\", description=\"Path to input file.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) geometry: str = Field( \"\", description=\"Path to geometry file.\", flag_type=\"-\", rename_param=\"g\" ) zmq_input: Optional[str] = Field( description=\"ZMQ address to receive data over. `input` and `zmq-input` are mutually exclusive\", flag_type=\"--\", rename_param=\"zmq-input\", ) zmq_subscribe: Optional[str] = Field( # Can be used multiple times... description=\"Subscribe to ZMQ message of type `tag`\", flag_type=\"--\", rename_param=\"zmq-subscribe\", ) zmq_request: Optional[AnyUrl] = Field( description=\"Request new data over ZMQ by sending this value\", flag_type=\"--\", rename_param=\"zmq-request\", ) asapo_endpoint: Optional[str] = Field( description=\"ASAP::O endpoint. zmq-input and this are mutually exclusive.\", flag_type=\"--\", rename_param=\"asapo-endpoint\", ) asapo_token: Optional[str] = Field( description=\"ASAP::O authentication token.\", flag_type=\"--\", rename_param=\"asapo-token\", ) asapo_beamtime: Optional[str] = Field( description=\"ASAP::O beatime.\", flag_type=\"--\", rename_param=\"asapo-beamtime\", ) asapo_source: Optional[str] = Field( description=\"ASAP::O data source.\", flag_type=\"--\", rename_param=\"asapo-source\", ) asapo_group: Optional[str] = Field( description=\"ASAP::O consumer group.\", flag_type=\"--\", rename_param=\"asapo-group\", ) asapo_stream: Optional[str] = Field( description=\"ASAP::O stream.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) asapo_wait_for_stream: Optional[str] = Field( description=\"If ASAP::O stream does not exist, wait for it to appear.\", flag_type=\"--\", rename_param=\"asapo-wait-for-stream\", ) data_format: Optional[str] = Field( description=\"Specify format for ZMQ or ASAP::O. `msgpack`, `hdf5` or `seedee`.\", flag_type=\"--\", rename_param=\"data-format\", ) basename: bool = Field( False, description=\"Remove directory parts of filenames. Acts before prefix if prefix also given.\", flag_type=\"--\", ) prefix: Optional[str] = Field( description=\"Add a prefix to the filenames from the infile argument.\", flag_type=\"--\", rename_param=\"asapo-stream\", ) nthreads: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of threads to use. See also `max_indexer_threads`.\", flag_type=\"-\", rename_param=\"j\", ) no_check_prefix: bool = Field( False, description=\"Don't attempt to correct the prefix if it seems incorrect.\", flag_type=\"--\", rename_param=\"no-check-prefix\", ) highres: Optional[float] = Field( description=\"Mark all pixels greater than `x` has bad.\", flag_type=\"--\" ) profile: bool = Field( False, description=\"Display timing data to monitor performance.\", flag_type=\"--\" ) temp_dir: Optional[str] = Field( description=\"Specify a path for the temp files folder.\", flag_type=\"--\", rename_param=\"temp-dir\", ) wait_for_file: conint(gt=-2) = Field( 0, description=\"Wait at most `x` seconds for a file to be created. A value of -1 means wait forever.\", flag_type=\"--\", rename_param=\"wait-for-file\", ) no_image_data: bool = Field( False, description=\"Load only the metadata, no iamges. Can check indexability without high data requirements.\", flag_type=\"--\", rename_param=\"no-image-data\", ) # Peak-finding options # .... # Indexing options indexing: Optional[str] = Field( description=\"Comma-separated list of supported indexing algorithms to use. Default is to automatically detect.\", flag_type=\"--\", ) cell_file: Optional[str] = Field( description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) tolerance: str = Field( \"5,5,5,1.5\", description=( \"Tolerances (in percent) for unit cell comparison. \" \"Comma-separated list a,b,c,angle. Default=5,5,5,1.5\" ), flag_type=\"--\", ) no_check_cell: bool = Field( False, description=\"Do not check cell parameters against unit cell. Replaces '-raw' method.\", flag_type=\"--\", rename_param=\"no-check-cell\", ) no_check_peaks: bool = Field( False, description=\"Do not verify peaks are accounted for by solution.\", flag_type=\"--\", rename_param=\"no-check-peaks\", ) multi: bool = Field( False, description=\"Enable multi-lattice indexing.\", flag_type=\"--\" ) wavelength_estimate: Optional[float] = Field( description=\"Estimate for X-ray wavelength. Required for some methods.\", flag_type=\"--\", rename_param=\"wavelength-estimate\", ) camera_length_estimate: Optional[float] = Field( description=\"Estimate for camera distance. Required for some methods.\", flag_type=\"--\", rename_param=\"camera-length-estimate\", ) max_indexer_threads: Optional[PositiveInt] = Field( # 1, description=\"Some indexing algos can use multiple threads. In addition to image-based.\", flag_type=\"--\", rename_param=\"max-indexer-threads\", ) no_retry: bool = Field( False, description=\"Do not remove weak peaks and try again.\", flag_type=\"--\", rename_param=\"no-retry\", ) no_refine: bool = Field( False, description=\"Skip refinement step.\", flag_type=\"--\", rename_param=\"no-refine\", ) no_revalidate: bool = Field( False, description=\"Skip revalidation step.\", flag_type=\"--\", rename_param=\"no-revalidate\", ) # TakeTwo specific parameters taketwo_member_threshold: Optional[PositiveInt] = Field( # 20, description=\"Minimum number of vectors to consider.\", flag_type=\"--\", rename_param=\"taketwo-member-threshold\", ) taketwo_len_tolerance: Optional[PositiveFloat] = Field( # 0.001, description=\"TakeTwo length tolerance in Angstroms.\", flag_type=\"--\", rename_param=\"taketwo-len-tolerance\", ) taketwo_angle_tolerance: Optional[PositiveFloat] = Field( # 0.6, description=\"TakeTwo angle tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-angle-tolerance\", ) taketwo_trace_tolerance: Optional[PositiveFloat] = Field( # 3, description=\"Matrix trace tolerance in degrees.\", flag_type=\"--\", rename_param=\"taketwo-trace-tolerance\", ) # Felix-specific parameters # felix_domega # felix-fraction-max-visits # felix-max-internal-angle # felix-max-uniqueness # felix-min-completeness # felix-min-visits # felix-num-voxels # felix-sigma # felix-tthrange-max # felix-tthrange-min # XGANDALF-specific parameters xgandalf_sampling_pitch: Optional[NonNegativeInt] = Field( # 6, description=\"Density of reciprocal space sampling.\", flag_type=\"--\", rename_param=\"xgandalf-sampling-pitch\", ) xgandalf_grad_desc_iterations: Optional[NonNegativeInt] = Field( # 4, description=\"Number of gradient descent iterations.\", flag_type=\"--\", rename_param=\"xgandalf-grad-desc-iterations\", ) xgandalf_tolerance: Optional[PositiveFloat] = Field( # 0.02, description=\"Relative tolerance of lattice vectors\", flag_type=\"--\", rename_param=\"xgandalf-tolerance\", ) xgandalf_no_deviation_from_provided_cell: Optional[bool] = Field( description=\"Found unit cell must match provided.\", flag_type=\"--\", rename_param=\"xgandalf-no-deviation-from-provided-cell\", ) xgandalf_min_lattice_vector_length: Optional[PositiveFloat] = Field( # 30, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-min-lattice-vector-length\", ) xgandalf_max_lattice_vector_length: Optional[PositiveFloat] = Field( # 250, description=\"Minimum possible lattice length.\", flag_type=\"--\", rename_param=\"xgandalf-max-lattice-vector-length\", ) xgandalf_max_peaks: Optional[PositiveInt] = Field( # 250, description=\"Maximum number of peaks to use for indexing.\", flag_type=\"--\", rename_param=\"xgandalf-max-peaks\", ) xgandalf_fast_execution: bool = Field( False, description=\"Shortcut to set sampling-pitch=2, and grad-desc-iterations=3.\", flag_type=\"--\", rename_param=\"xgandalf-fast-execution\", ) # pinkIndexer parameters # ... # asdf_fast: bool = Field(False, description=\"Enable fast mode for asdf. 3x faster for 7% loss in accuracy.\", flag_type=\"--\", rename_param=\"asdf-fast\") # Integration parameters integration: str = Field( \"rings-nocen\", description=\"Method for integrating reflections.\", flag_type=\"--\" ) fix_profile_radius: Optional[float] = Field( description=\"Fix the profile radius (m^{-1})\", flag_type=\"--\", rename_param=\"fix-profile-radius\", ) fix_divergence: Optional[float] = Field( 0, description=\"Fix the divergence (rad, full angle).\", flag_type=\"--\", rename_param=\"fix-divergence\", ) int_radius: str = Field( \"4,5,7\", description=\"Inner, middle, and outer radii for 3-ring integration.\", flag_type=\"--\", rename_param=\"int-radius\", ) int_diag: str = Field( \"none\", description=\"Show detailed information on integration when condition is met.\", flag_type=\"--\", rename_param=\"int-diag\", ) push_res: str = Field( \"infinity\", description=\"Integrate `x` higher than apparent resolution limit (nm-1).\", flag_type=\"--\", rename_param=\"push-res\", ) overpredict: bool = Field( False, description=\"Over-predict reflections. Maybe useful with post-refinement.\", flag_type=\"--\", ) cell_parameters_only: bool = Field( False, description=\"Do not predict refletions at all\", flag_type=\"--\" ) # Output parameters no_non_hits_in_stream: bool = Field( False, description=\"Exclude non-hits from the stream file.\", flag_type=\"--\", rename_param=\"no-non-hits-in-stream\", ) copy_hheader: Optional[str] = Field( description=\"Copy information from header in the image to output stream.\", flag_type=\"--\", rename_param=\"copy-hheader\", ) no_peaks_in_stream: bool = Field( False, description=\"Do not record peaks in stream file.\", flag_type=\"--\", rename_param=\"no-peaks-in-stream\", ) no_refls_in_stream: bool = Field( False, description=\"Do not record reflections in stream.\", flag_type=\"--\", rename_param=\"no-refls-in-stream\", ) serial_offset: Optional[PositiveInt] = Field( description=\"Start numbering at `x` instead of 1.\", flag_type=\"--\", rename_param=\"serial-offset\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": filename: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPyAlgos\", \"out_file\" ) if filename is None: exp: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) tag: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"tag\" ) out_dir: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"FindPeaksPsocake\", \"outDir\" ) if out_dir is not None: fname: str = f\"{out_dir}/{exp}_{run:04d}\" if tag is not None: fname = f\"{fname}_{tag}\" return f\"{fname}.lst\" else: return filename return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": expmt: str = values[\"lute_config\"].experiment run: int = int(values[\"lute_config\"].run) work_dir: str = values[\"lute_config\"].work_dir fname: str = f\"{expmt}_r{run:04d}.stream\" return f\"{work_dir}/{fname}\" return out_file","title":"IndexCrystFELParameters"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.IndexCrystFELParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_index.py class Config(ThirdPartyParameters.Config): set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.IndexCrystFELParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/models/sfx_index/#io.models.sfx_index.IndexCrystFELParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_merge/","text":"Models for merging reflections in serial femtosecond crystallography. Classes: Name Description MergePartialatorParameters Perform merging using CrystFEL's partialator . CompareHKLParameters Calculate figures of merit using CrystFEL's compare_hkl . ManipulateHKLParameters Perform transformations on lists of reflections using CrystFEL's get_hkl . CompareHKLParameters Bases: ThirdPartyParameters Parameters for CrystFEL's compare_hkl for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class CompareHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `compare_hkl` for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/compare_hkl\", description=\"CrystFEL's reflection comparison binary.\", flag_type=\"\", ) in_files: Optional[str] = Field( \"\", description=\"Path to input HKLs. Space-separated list of 2. Use output of partialator e.g.\", flag_type=\"\", ) ## Need mechanism to set is_result=True ... symmetry: str = Field(\"\", description=\"Point group symmetry.\", flag_type=\"--\") cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) fom: str = Field( \"Rsplit\", description=\"Specify figure of merit to calculate.\", flag_type=\"--\" ) nshells: int = Field(10, description=\"Use n resolution shells.\", flag_type=\"--\") # NEED A NEW CASE FOR THIS -> Boolean flag, no arg, one hyphen... # fix_unity: bool = Field( # False, # description=\"Fix scale factors to unity.\", # flag_type=\"-\", # rename_param=\"u\", # ) shell_file: str = Field( \"\", description=\"Write the statistics in resolution shells to a file.\", flag_type=\"--\", rename_param=\"shell-file\", is_result=True, ) ignore_negs: bool = Field( False, description=\"Ignore reflections with negative reflections.\", flag_type=\"--\", rename_param=\"ignore-negs\", ) zero_negs: bool = Field( False, description=\"Set negative intensities to 0.\", flag_type=\"--\", rename_param=\"zero-negs\", ) sigma_cutoff: Optional[Union[float, int, str]] = Field( # \"-infinity\", description=\"Discard reflections with I/sigma(I) < n. -infinity means no cutoff.\", flag_type=\"--\", rename_param=\"sigma-cutoff\", ) rmin: Optional[float] = Field( description=\"Low resolution cutoff of 1/d (m-1). Use this or --lowres NOT both.\", flag_type=\"--\", ) lowres: Optional[float] = Field( descirption=\"Low resolution cutoff in Angstroms. Use this or --rmin NOT both.\", flag_type=\"--\", ) rmax: Optional[float] = Field( description=\"High resolution cutoff in 1/d (m-1). Use this or --highres NOT both.\", flag_type=\"--\", ) highres: Optional[float] = Field( description=\"High resolution cutoff in Angstroms. Use this or --rmax NOT both.\", flag_type=\"--\", ) @validator(\"in_files\", always=True) def validate_in_files(cls, in_files: str, values: Dict[str, Any]) -> str: if in_files == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: hkls: str = f\"{partialator_file}1 {partialator_file}2\" return hkls return in_files @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file @validator(\"symmetry\", always=True) def validate_symmetry(cls, symmetry: str, values: Dict[str, Any]) -> str: if symmetry == \"\": partialator_sym: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"symmetry\" ) if partialator_sym: return partialator_sym return symmetry @validator(\"shell_file\", always=True) def validate_shell_file(cls, shell_file: str, values: Dict[str, Any]) -> str: if shell_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: shells_out: str = partialator_file.split(\".\")[0] shells_out = f\"{shells_out}_{values['fom']}_n{values['nshells']}.dat\" return shells_out return shell_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. ManipulateHKLParameters Bases: ThirdPartyParameters Parameters for CrystFEL's get_hkl for manipulating lists of reflections. This Task is predominantly used internally to convert hkl to mtz files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class ManipulateHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `get_hkl` for manipulating lists of reflections. This Task is predominantly used internally to convert `hkl` to `mtz` files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/get_hkl\", description=\"CrystFEL's reflection manipulation binary.\", flag_type=\"\", ) in_file: str = Field( \"\", description=\"Path to input HKL file.\", flag_type=\"-\", rename_param=\"i\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) output_format: str = Field( \"mtz\", description=\"Output format. One of mtz, mtz-bij, or xds. Otherwise CrystFEL format.\", flag_type=\"--\", rename_param=\"output-format\", ) expand: Optional[str] = Field( description=\"Reflections will be expanded to fill asymmetric unit of specified point group.\", flag_type=\"--\", ) # Reducing reflections to higher symmetry twin: Optional[str] = Field( description=\"Reflections equivalent to specified point group will have intensities summed.\", flag_type=\"--\", ) no_need_all_parts: Optional[bool] = Field( description=\"Use with --twin to allow reflections missing a 'twin mate' to be written out.\", flag_type=\"--\", rename_param=\"no-need-all-parts\", ) # Noise - Add to data noise: Optional[bool] = Field( description=\"Generate 10% uniform noise.\", flag_type=\"--\" ) poisson: Optional[bool] = Field( description=\"Generate Poisson noise. Intensities assumed to be A.U.\", flag_type=\"--\", ) adu_per_photon: Optional[int] = Field( description=\"Use with --poisson to convert A.U. to photons.\", flag_type=\"--\", rename_param=\"adu-per-photon\", ) # Remove duplicate reflections trim_centrics: Optional[bool] = Field( description=\"Duplicated reflections (according to symmetry) are removed.\", flag_type=\"--\", ) # Restrict to template file template: Optional[str] = Field( description=\"Only reflections which also appear in specified file are written out.\", flag_type=\"--\", ) # Multiplicity multiplicity: Optional[bool] = Field( description=\"Reflections are multiplied by their symmetric multiplicites.\", flag_type=\"--\", ) # Resolution cutoffs cutoff_angstroms: Optional[Union[str, int, float]] = Field( description=\"Either n, or n1,n2,n3. For n, reflections < n are removed. For n1,n2,n3 anisotropic trunction performed at separate resolution limits for a*, b*, c*.\", flag_type=\"--\", rename_param=\"cutoff-angstroms\", ) lowres: Optional[float] = Field( description=\"Remove reflections with d > n\", flag_type=\"--\" ) highres: Optional[float] = Field( description=\"Synonym for first form of --cutoff-angstroms\" ) reindex: Optional[str] = Field( description=\"Reindex according to specified operator. E.g. k,h,-l.\", flag_type=\"--\", ) # Override input symmetry symmetry: Optional[str] = Field( description=\"Point group symmetry to use to override. Almost always OMIT this option.\", flag_type=\"--\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: return partialator_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: mtz_out: str = partialator_file.split(\".\")[0] mtz_out = f\"{mtz_out}.mtz\" return mtz_out return out_file @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result. MergePartialatorParameters Bases: ThirdPartyParameters Parameters for CrystFEL's partialator . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class MergePartialatorParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `partialator`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/partialator\", description=\"CrystFEL's Partialator binary.\", flag_type=\"\", ) in_file: Optional[str] = Field( \"\", description=\"Path to input stream.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) symmetry: str = Field(description=\"Point group symmetry.\", flag_type=\"--\") niter: Optional[int] = Field( description=\"Number of cycles of scaling and post-refinement.\", flag_type=\"-\", rename_param=\"n\", ) no_scale: Optional[bool] = Field( description=\"Disable scaling.\", flag_type=\"--\", rename_param=\"no-scale\" ) no_Bscale: Optional[bool] = Field( description=\"Disable Debye-Waller part of scaling.\", flag_type=\"--\", rename_param=\"no-Bscale\", ) no_pr: Optional[bool] = Field( description=\"Disable orientation model.\", flag_type=\"--\", rename_param=\"no-pr\" ) no_deltacchalf: Optional[bool] = Field( description=\"Disable rejection based on deltaCC1/2.\", flag_type=\"--\", rename_param=\"no-deltacchalf\", ) model: str = Field( \"unity\", description=\"Partiality model. Options: xsphere, unity, offset, ggpm.\", flag_type=\"--\", ) nthreads: int = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of parallel analyses.\", flag_type=\"-\", rename_param=\"j\", ) polarisation: Optional[str] = Field( description=\"Specification of incident polarisation. Refer to CrystFEL docs for more info.\", flag_type=\"--\", ) no_polarisation: Optional[bool] = Field( description=\"Synonym for --polarisation=none\", flag_type=\"--\", rename_param=\"no-polarisation\", ) max_adu: Optional[float] = Field( description=\"Maximum intensity of reflection to include.\", flag_type=\"--\", rename_param=\"max-adu\", ) min_res: Optional[float] = Field( description=\"Only include crystals diffracting to a minimum resolution.\", flag_type=\"--\", rename_param=\"min-res\", ) min_measurements: int = Field( 2, description=\"Include a reflection only if it appears a minimum number of times.\", flag_type=\"--\", rename_param=\"min-measurements\", ) push_res: Optional[float] = Field( description=\"Merge reflections up to higher than the apparent resolution limit.\", flag_type=\"--\", rename_param=\"push-res\", ) start_after: int = Field( 0, description=\"Ignore the first n crystals.\", flag_type=\"--\", rename_param=\"start-after\", ) stop_after: int = Field( 0, description=\"Stop after processing n crystals. 0 means process all.\", flag_type=\"--\", rename_param=\"stop-after\", ) no_free: Optional[bool] = Field( description=\"Disable cross-validation. Testing ONLY.\", flag_type=\"--\", rename_param=\"no-free\", ) custom_split: Optional[str] = Field( description=\"Read a set of filenames, event and dataset IDs from a filename.\", flag_type=\"--\", rename_param=\"custom-split\", ) max_rel_B: float = Field( 100, description=\"Reject crystals if |relB| > n sq Angstroms.\", flag_type=\"--\", rename_param=\"max-rel-B\", ) output_every_cycle: bool = Field( False, description=\"Write per-crystal params after every refinement cycle.\", flag_type=\"--\", rename_param=\"output-every-cycle\", ) no_logs: bool = Field( False, description=\"Do not write logs needed for plots, maps and graphs.\", flag_type=\"--\", rename_param=\"no-logs\", ) set_symmetry: Optional[str] = Field( description=\"Set the apparent symmetry of the crystals to a point group.\", flag_type=\"-\", rename_param=\"w\", ) operator: Optional[str] = Field( description=\"Specify an ambiguity operator. E.g. k,h,-l.\", flag_type=\"--\" ) force_bandwidth: Optional[float] = Field( description=\"Set X-ray bandwidth. As percent, e.g. 0.0013 (0.13%).\", flag_type=\"--\", rename_param=\"force-bandwidth\", ) force_radius: Optional[float] = Field( description=\"Set the initial profile radius (nm-1).\", flag_type=\"--\", rename_param=\"force-radius\", ) force_lambda: Optional[float] = Field( description=\"Set the wavelength. In Angstroms.\", flag_type=\"--\", rename_param=\"force-lambda\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ConcatenateStreamFiles\", \"out_file\", ) if stream_file: return stream_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": in_file: str = values[\"in_file\"] if in_file: tag: str = in_file.split(\".\")[0] return f\"{tag}.hkl\" else: return \"partialator.hkl\" return out_file Config Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" long_flags_use_eq: bool = True class-attribute instance-attribute Whether long command-line arguments are passed like --long=arg . set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result.","title":"sfx_merge"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.CompareHKLParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's compare_hkl for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class CompareHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `compare_hkl` for calculating figures of merit. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/compare_hkl\", description=\"CrystFEL's reflection comparison binary.\", flag_type=\"\", ) in_files: Optional[str] = Field( \"\", description=\"Path to input HKLs. Space-separated list of 2. Use output of partialator e.g.\", flag_type=\"\", ) ## Need mechanism to set is_result=True ... symmetry: str = Field(\"\", description=\"Point group symmetry.\", flag_type=\"--\") cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) fom: str = Field( \"Rsplit\", description=\"Specify figure of merit to calculate.\", flag_type=\"--\" ) nshells: int = Field(10, description=\"Use n resolution shells.\", flag_type=\"--\") # NEED A NEW CASE FOR THIS -> Boolean flag, no arg, one hyphen... # fix_unity: bool = Field( # False, # description=\"Fix scale factors to unity.\", # flag_type=\"-\", # rename_param=\"u\", # ) shell_file: str = Field( \"\", description=\"Write the statistics in resolution shells to a file.\", flag_type=\"--\", rename_param=\"shell-file\", is_result=True, ) ignore_negs: bool = Field( False, description=\"Ignore reflections with negative reflections.\", flag_type=\"--\", rename_param=\"ignore-negs\", ) zero_negs: bool = Field( False, description=\"Set negative intensities to 0.\", flag_type=\"--\", rename_param=\"zero-negs\", ) sigma_cutoff: Optional[Union[float, int, str]] = Field( # \"-infinity\", description=\"Discard reflections with I/sigma(I) < n. -infinity means no cutoff.\", flag_type=\"--\", rename_param=\"sigma-cutoff\", ) rmin: Optional[float] = Field( description=\"Low resolution cutoff of 1/d (m-1). Use this or --lowres NOT both.\", flag_type=\"--\", ) lowres: Optional[float] = Field( descirption=\"Low resolution cutoff in Angstroms. Use this or --rmin NOT both.\", flag_type=\"--\", ) rmax: Optional[float] = Field( description=\"High resolution cutoff in 1/d (m-1). Use this or --highres NOT both.\", flag_type=\"--\", ) highres: Optional[float] = Field( description=\"High resolution cutoff in Angstroms. Use this or --rmax NOT both.\", flag_type=\"--\", ) @validator(\"in_files\", always=True) def validate_in_files(cls, in_files: str, values: Dict[str, Any]) -> str: if in_files == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: hkls: str = f\"{partialator_file}1 {partialator_file}2\" return hkls return in_files @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file @validator(\"symmetry\", always=True) def validate_symmetry(cls, symmetry: str, values: Dict[str, Any]) -> str: if symmetry == \"\": partialator_sym: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"symmetry\" ) if partialator_sym: return partialator_sym return symmetry @validator(\"shell_file\", always=True) def validate_shell_file(cls, shell_file: str, values: Dict[str, Any]) -> str: if shell_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: shells_out: str = partialator_file.split(\".\")[0] shells_out = f\"{shells_out}_{values['fom']}_n{values['nshells']}.dat\" return shells_out return shell_file","title":"CompareHKLParameters"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.CompareHKLParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.CompareHKLParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.CompareHKLParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.ManipulateHKLParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's get_hkl for manipulating lists of reflections. This Task is predominantly used internally to convert hkl to mtz files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class ManipulateHKLParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `get_hkl` for manipulating lists of reflections. This Task is predominantly used internally to convert `hkl` to `mtz` files. Note that performing multiple manipulations is undefined behaviour. Run the Task with multiple configurations in explicit separate steps. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/get_hkl\", description=\"CrystFEL's reflection manipulation binary.\", flag_type=\"\", ) in_file: str = Field( \"\", description=\"Path to input HKL file.\", flag_type=\"-\", rename_param=\"i\", ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) cell_file: str = Field( \"\", description=\"Path to a file containing unit cell information (PDB or CrystFEL format).\", flag_type=\"-\", rename_param=\"p\", ) output_format: str = Field( \"mtz\", description=\"Output format. One of mtz, mtz-bij, or xds. Otherwise CrystFEL format.\", flag_type=\"--\", rename_param=\"output-format\", ) expand: Optional[str] = Field( description=\"Reflections will be expanded to fill asymmetric unit of specified point group.\", flag_type=\"--\", ) # Reducing reflections to higher symmetry twin: Optional[str] = Field( description=\"Reflections equivalent to specified point group will have intensities summed.\", flag_type=\"--\", ) no_need_all_parts: Optional[bool] = Field( description=\"Use with --twin to allow reflections missing a 'twin mate' to be written out.\", flag_type=\"--\", rename_param=\"no-need-all-parts\", ) # Noise - Add to data noise: Optional[bool] = Field( description=\"Generate 10% uniform noise.\", flag_type=\"--\" ) poisson: Optional[bool] = Field( description=\"Generate Poisson noise. Intensities assumed to be A.U.\", flag_type=\"--\", ) adu_per_photon: Optional[int] = Field( description=\"Use with --poisson to convert A.U. to photons.\", flag_type=\"--\", rename_param=\"adu-per-photon\", ) # Remove duplicate reflections trim_centrics: Optional[bool] = Field( description=\"Duplicated reflections (according to symmetry) are removed.\", flag_type=\"--\", ) # Restrict to template file template: Optional[str] = Field( description=\"Only reflections which also appear in specified file are written out.\", flag_type=\"--\", ) # Multiplicity multiplicity: Optional[bool] = Field( description=\"Reflections are multiplied by their symmetric multiplicites.\", flag_type=\"--\", ) # Resolution cutoffs cutoff_angstroms: Optional[Union[str, int, float]] = Field( description=\"Either n, or n1,n2,n3. For n, reflections < n are removed. For n1,n2,n3 anisotropic trunction performed at separate resolution limits for a*, b*, c*.\", flag_type=\"--\", rename_param=\"cutoff-angstroms\", ) lowres: Optional[float] = Field( description=\"Remove reflections with d > n\", flag_type=\"--\" ) highres: Optional[float] = Field( description=\"Synonym for first form of --cutoff-angstroms\" ) reindex: Optional[str] = Field( description=\"Reindex according to specified operator. E.g. k,h,-l.\", flag_type=\"--\", ) # Override input symmetry symmetry: Optional[str] = Field( description=\"Point group symmetry to use to override. Almost always OMIT this option.\", flag_type=\"--\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: return partialator_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": partialator_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"MergePartialator\", \"out_file\" ) if partialator_file: mtz_out: str = partialator_file.split(\".\")[0] mtz_out = f\"{mtz_out}.mtz\" return mtz_out return out_file @validator(\"cell_file\", always=True) def validate_cell_file(cls, cell_file: str, values: Dict[str, Any]) -> str: if cell_file == \"\": idx_cell_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"IndexCrystFEL\", \"cell_file\", valid_only=False, ) if idx_cell_file: return idx_cell_file return cell_file","title":"ManipulateHKLParameters"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.ManipulateHKLParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.ManipulateHKLParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.ManipulateHKLParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.MergePartialatorParameters","text":"Bases: ThirdPartyParameters Parameters for CrystFEL's partialator . There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html Source code in lute/io/models/sfx_merge.py class MergePartialatorParameters(ThirdPartyParameters): \"\"\"Parameters for CrystFEL's `partialator`. There are many parameters, and many combinations. For more information on usage, please refer to the CrystFEL documentation, here: https://www.desy.de/~twhite/crystfel/manual-partialator.html \"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/crystfel/0.10.2/bin/partialator\", description=\"CrystFEL's Partialator binary.\", flag_type=\"\", ) in_file: Optional[str] = Field( \"\", description=\"Path to input stream.\", flag_type=\"-\", rename_param=\"i\" ) out_file: str = Field( \"\", description=\"Path to output file.\", flag_type=\"-\", rename_param=\"o\", is_result=True, ) symmetry: str = Field(description=\"Point group symmetry.\", flag_type=\"--\") niter: Optional[int] = Field( description=\"Number of cycles of scaling and post-refinement.\", flag_type=\"-\", rename_param=\"n\", ) no_scale: Optional[bool] = Field( description=\"Disable scaling.\", flag_type=\"--\", rename_param=\"no-scale\" ) no_Bscale: Optional[bool] = Field( description=\"Disable Debye-Waller part of scaling.\", flag_type=\"--\", rename_param=\"no-Bscale\", ) no_pr: Optional[bool] = Field( description=\"Disable orientation model.\", flag_type=\"--\", rename_param=\"no-pr\" ) no_deltacchalf: Optional[bool] = Field( description=\"Disable rejection based on deltaCC1/2.\", flag_type=\"--\", rename_param=\"no-deltacchalf\", ) model: str = Field( \"unity\", description=\"Partiality model. Options: xsphere, unity, offset, ggpm.\", flag_type=\"--\", ) nthreads: int = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of parallel analyses.\", flag_type=\"-\", rename_param=\"j\", ) polarisation: Optional[str] = Field( description=\"Specification of incident polarisation. Refer to CrystFEL docs for more info.\", flag_type=\"--\", ) no_polarisation: Optional[bool] = Field( description=\"Synonym for --polarisation=none\", flag_type=\"--\", rename_param=\"no-polarisation\", ) max_adu: Optional[float] = Field( description=\"Maximum intensity of reflection to include.\", flag_type=\"--\", rename_param=\"max-adu\", ) min_res: Optional[float] = Field( description=\"Only include crystals diffracting to a minimum resolution.\", flag_type=\"--\", rename_param=\"min-res\", ) min_measurements: int = Field( 2, description=\"Include a reflection only if it appears a minimum number of times.\", flag_type=\"--\", rename_param=\"min-measurements\", ) push_res: Optional[float] = Field( description=\"Merge reflections up to higher than the apparent resolution limit.\", flag_type=\"--\", rename_param=\"push-res\", ) start_after: int = Field( 0, description=\"Ignore the first n crystals.\", flag_type=\"--\", rename_param=\"start-after\", ) stop_after: int = Field( 0, description=\"Stop after processing n crystals. 0 means process all.\", flag_type=\"--\", rename_param=\"stop-after\", ) no_free: Optional[bool] = Field( description=\"Disable cross-validation. Testing ONLY.\", flag_type=\"--\", rename_param=\"no-free\", ) custom_split: Optional[str] = Field( description=\"Read a set of filenames, event and dataset IDs from a filename.\", flag_type=\"--\", rename_param=\"custom-split\", ) max_rel_B: float = Field( 100, description=\"Reject crystals if |relB| > n sq Angstroms.\", flag_type=\"--\", rename_param=\"max-rel-B\", ) output_every_cycle: bool = Field( False, description=\"Write per-crystal params after every refinement cycle.\", flag_type=\"--\", rename_param=\"output-every-cycle\", ) no_logs: bool = Field( False, description=\"Do not write logs needed for plots, maps and graphs.\", flag_type=\"--\", rename_param=\"no-logs\", ) set_symmetry: Optional[str] = Field( description=\"Set the apparent symmetry of the crystals to a point group.\", flag_type=\"-\", rename_param=\"w\", ) operator: Optional[str] = Field( description=\"Specify an ambiguity operator. E.g. k,h,-l.\", flag_type=\"--\" ) force_bandwidth: Optional[float] = Field( description=\"Set X-ray bandwidth. As percent, e.g. 0.0013 (0.13%).\", flag_type=\"--\", rename_param=\"force-bandwidth\", ) force_radius: Optional[float] = Field( description=\"Set the initial profile radius (nm-1).\", flag_type=\"--\", rename_param=\"force-radius\", ) force_lambda: Optional[float] = Field( description=\"Set the wavelength. In Angstroms.\", flag_type=\"--\", rename_param=\"force-lambda\", ) harvest_file: Optional[str] = Field( description=\"Write parameters to file in JSON format.\", flag_type=\"--\", rename_param=\"harvest-file\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": stream_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ConcatenateStreamFiles\", \"out_file\", ) if stream_file: return stream_file return in_file @validator(\"out_file\", always=True) def validate_out_file(cls, out_file: str, values: Dict[str, Any]) -> str: if out_file == \"\": in_file: str = values[\"in_file\"] if in_file: tag: str = in_file.split(\".\")[0] return f\"{tag}.hkl\" else: return \"partialator.hkl\" return out_file","title":"MergePartialatorParameters"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.MergePartialatorParameters.Config","text":"Bases: Config Source code in lute/io/models/sfx_merge.py class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True \"\"\"Whether long command-line arguments are passed like `--long=arg`.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\"","title":"Config"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.MergePartialatorParameters.Config.long_flags_use_eq","text":"Whether long command-line arguments are passed like --long=arg .","title":"long_flags_use_eq"},{"location":"source/io/models/sfx_merge/#io.models.sfx_merge.MergePartialatorParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/sfx_solve/","text":"Models for structure solution in serial femtosecond crystallography. Classes: Name Description DimpleSolveParameters Perform structure solution using CCP4's dimple (molecular replacement). DimpleSolveParameters Bases: ThirdPartyParameters Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ Source code in lute/io/models/sfx_solve.py class DimpleSolveParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/dimple\", description=\"CCP4 Dimple for solving structures with MR.\", flag_type=\"\", ) # Positional requirements - all required. in_file: str = Field( \"\", description=\"Path to input mtz.\", flag_type=\"\", ) pdb: str = Field(\"\", description=\"Path to a PDB.\", flag_type=\"\") out_dir: str = Field(\"\", description=\"Output DIRECTORY.\", flag_type=\"\") # Most used options mr_thresh: PositiveFloat = Field( 0.4, description=\"Threshold for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-when-r\", ) slow: Optional[bool] = Field( False, description=\"Perform more refinement.\", flag_type=\"--\" ) # Other options (IO) hklout: str = Field( \"final.mtz\", description=\"Output mtz file name.\", flag_type=\"--\" ) xyzout: str = Field( \"final.pdb\", description=\"Output PDB file name.\", flag_type=\"--\" ) icolumn: Optional[str] = Field( # \"IMEAN\", description=\"Name for the I column.\", flag_type=\"--\", ) sigicolumn: Optional[str] = Field( # \"SIG<ICOL>\", description=\"Name for the Sig<I> column.\", flag_type=\"--\", ) fcolumn: Optional[str] = Field( # \"F\", description=\"Name for the F column.\", flag_type=\"--\", ) sigfcolumn: Optional[str] = Field( # \"F\", description=\"Name for the Sig<F> column.\", flag_type=\"--\", ) libin: Optional[str] = Field( description=\"Ligand descriptions for refmac (LIBIN).\", flag_type=\"--\" ) refmac_key: Optional[str] = Field( description=\"Extra Refmac keywords to use in refinement.\", flag_type=\"--\", rename_param=\"refmac-key\", ) free_r_flags: Optional[str] = Field( description=\"Path to a mtz file with freeR flags.\", flag_type=\"--\", rename_param=\"free-r-flags\", ) freecolumn: Optional[Union[int, float]] = Field( # 0, description=\"Refree column with an optional value.\", flag_type=\"--\", ) img_format: Optional[str] = Field( description=\"Format of generated images. (png, jpeg, none).\", flag_type=\"-\", rename_param=\"f\", ) white_bg: bool = Field( False, description=\"Use a white background in Coot and in images.\", flag_type=\"--\", rename_param=\"white-bg\", ) no_cleanup: bool = Field( False, description=\"Retain intermediate files.\", flag_type=\"--\", rename_param=\"no-cleanup\", ) # Calculations no_blob_search: bool = Field( False, description=\"Do not search for unmodelled blobs.\", flag_type=\"--\", rename_param=\"no-blob-search\", ) anode: bool = Field( False, description=\"Use SHELX/AnoDe to find peaks in the anomalous map.\" ) # Run customization no_hetatm: bool = Field( False, description=\"Remove heteroatoms from the given model.\", flag_type=\"--\", rename_param=\"no-hetatm\", ) rigid_cycles: Optional[PositiveInt] = Field( # 10, description=\"Number of cycles of rigid-body refinement to perform.\", flag_type=\"--\", rename_param=\"rigid-cycles\", ) jelly: Optional[PositiveInt] = Field( # 4, description=\"Number of cycles of jelly-body refinement to perform.\", flag_type=\"--\", ) restr_cycles: Optional[PositiveInt] = Field( # 8, description=\"Number of cycles of refmac final refinement to perform.\", flag_type=\"--\", rename_param=\"restr-cycles\", ) lim_resolution: Optional[PositiveFloat] = Field( description=\"Limit the final resolution.\", flag_type=\"--\", rename_param=\"reso\" ) weight: Optional[str] = Field( # \"auto-weight\", description=\"The refmac matrix weight.\", flag_type=\"--\", ) mr_prog: Optional[str] = Field( # \"phaser\", description=\"Molecular replacement program. phaser or molrep.\", flag_type=\"--\", rename_param=\"mr-prog\", ) mr_num: Optional[Union[str, int]] = Field( # \"auto\", description=\"Number of molecules to use for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-num\", ) mr_reso: Optional[PositiveFloat] = Field( # 3.25, description=\"High resolution for molecular replacement. If >10 interpreted as eLLG.\", flag_type=\"--\", rename_param=\"mr-reso\", ) itof_prog: Optional[str] = Field( description=\"Program to calculate amplitudes. truncate, or ctruncate.\", flag_type=\"--\", rename_param=\"ItoF-prog\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return get_hkl_file return in_file @validator(\"out_dir\", always=True) def validate_out_dir(cls, out_dir: str, values: Dict[str, Any]) -> str: if out_dir == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return os.path.dirname(get_hkl_file) return out_dir RunSHELXCParameters Bases: ThirdPartyParameters Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html Source code in lute/io/models/sfx_solve.py class RunSHELXCParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/shelxc\", description=\"CCP4 SHELXC. Generates input files for SHELXD/SHELXE.\", flag_type=\"\", ) placeholder: str = Field( \"xx\", description=\"Placeholder filename stem.\", flag_type=\"\" ) in_file: str = Field( \"\", description=\"Input file for SHELXC with reflections AND proper records.\", flag_type=\"\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": # get_hkl needed to be run to produce an XDS format file... xds_format_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if xds_format_file: in_file = xds_format_file if in_file[0] != \"<\": # Need to add a redirection for this program # Runs like `shelxc xx <input_file.xds` in_file = f\"<{in_file}\" return in_file","title":"sfx_solve"},{"location":"source/io/models/sfx_solve/#io.models.sfx_solve.DimpleSolveParameters","text":"Bases: ThirdPartyParameters Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ Source code in lute/io/models/sfx_solve.py class DimpleSolveParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's dimple program. There are many parameters. For more information on usage, please refer to the CCP4 documentation, here: https://ccp4.github.io/dimple/ \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/dimple\", description=\"CCP4 Dimple for solving structures with MR.\", flag_type=\"\", ) # Positional requirements - all required. in_file: str = Field( \"\", description=\"Path to input mtz.\", flag_type=\"\", ) pdb: str = Field(\"\", description=\"Path to a PDB.\", flag_type=\"\") out_dir: str = Field(\"\", description=\"Output DIRECTORY.\", flag_type=\"\") # Most used options mr_thresh: PositiveFloat = Field( 0.4, description=\"Threshold for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-when-r\", ) slow: Optional[bool] = Field( False, description=\"Perform more refinement.\", flag_type=\"--\" ) # Other options (IO) hklout: str = Field( \"final.mtz\", description=\"Output mtz file name.\", flag_type=\"--\" ) xyzout: str = Field( \"final.pdb\", description=\"Output PDB file name.\", flag_type=\"--\" ) icolumn: Optional[str] = Field( # \"IMEAN\", description=\"Name for the I column.\", flag_type=\"--\", ) sigicolumn: Optional[str] = Field( # \"SIG<ICOL>\", description=\"Name for the Sig<I> column.\", flag_type=\"--\", ) fcolumn: Optional[str] = Field( # \"F\", description=\"Name for the F column.\", flag_type=\"--\", ) sigfcolumn: Optional[str] = Field( # \"F\", description=\"Name for the Sig<F> column.\", flag_type=\"--\", ) libin: Optional[str] = Field( description=\"Ligand descriptions for refmac (LIBIN).\", flag_type=\"--\" ) refmac_key: Optional[str] = Field( description=\"Extra Refmac keywords to use in refinement.\", flag_type=\"--\", rename_param=\"refmac-key\", ) free_r_flags: Optional[str] = Field( description=\"Path to a mtz file with freeR flags.\", flag_type=\"--\", rename_param=\"free-r-flags\", ) freecolumn: Optional[Union[int, float]] = Field( # 0, description=\"Refree column with an optional value.\", flag_type=\"--\", ) img_format: Optional[str] = Field( description=\"Format of generated images. (png, jpeg, none).\", flag_type=\"-\", rename_param=\"f\", ) white_bg: bool = Field( False, description=\"Use a white background in Coot and in images.\", flag_type=\"--\", rename_param=\"white-bg\", ) no_cleanup: bool = Field( False, description=\"Retain intermediate files.\", flag_type=\"--\", rename_param=\"no-cleanup\", ) # Calculations no_blob_search: bool = Field( False, description=\"Do not search for unmodelled blobs.\", flag_type=\"--\", rename_param=\"no-blob-search\", ) anode: bool = Field( False, description=\"Use SHELX/AnoDe to find peaks in the anomalous map.\" ) # Run customization no_hetatm: bool = Field( False, description=\"Remove heteroatoms from the given model.\", flag_type=\"--\", rename_param=\"no-hetatm\", ) rigid_cycles: Optional[PositiveInt] = Field( # 10, description=\"Number of cycles of rigid-body refinement to perform.\", flag_type=\"--\", rename_param=\"rigid-cycles\", ) jelly: Optional[PositiveInt] = Field( # 4, description=\"Number of cycles of jelly-body refinement to perform.\", flag_type=\"--\", ) restr_cycles: Optional[PositiveInt] = Field( # 8, description=\"Number of cycles of refmac final refinement to perform.\", flag_type=\"--\", rename_param=\"restr-cycles\", ) lim_resolution: Optional[PositiveFloat] = Field( description=\"Limit the final resolution.\", flag_type=\"--\", rename_param=\"reso\" ) weight: Optional[str] = Field( # \"auto-weight\", description=\"The refmac matrix weight.\", flag_type=\"--\", ) mr_prog: Optional[str] = Field( # \"phaser\", description=\"Molecular replacement program. phaser or molrep.\", flag_type=\"--\", rename_param=\"mr-prog\", ) mr_num: Optional[Union[str, int]] = Field( # \"auto\", description=\"Number of molecules to use for molecular replacement.\", flag_type=\"--\", rename_param=\"mr-num\", ) mr_reso: Optional[PositiveFloat] = Field( # 3.25, description=\"High resolution for molecular replacement. If >10 interpreted as eLLG.\", flag_type=\"--\", rename_param=\"mr-reso\", ) itof_prog: Optional[str] = Field( description=\"Program to calculate amplitudes. truncate, or ctruncate.\", flag_type=\"--\", rename_param=\"ItoF-prog\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return get_hkl_file return in_file @validator(\"out_dir\", always=True) def validate_out_dir(cls, out_dir: str, values: Dict[str, Any]) -> str: if out_dir == \"\": get_hkl_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if get_hkl_file: return os.path.dirname(get_hkl_file) return out_dir","title":"DimpleSolveParameters"},{"location":"source/io/models/sfx_solve/#io.models.sfx_solve.RunSHELXCParameters","text":"Bases: ThirdPartyParameters Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html Source code in lute/io/models/sfx_solve.py class RunSHELXCParameters(ThirdPartyParameters): \"\"\"Parameters for CCP4's SHELXC program. SHELXC prepares files for SHELXD and SHELXE. For more information please refer to the official documentation: https://www.ccp4.ac.uk/html/crank.html \"\"\" executable: str = Field( \"/sdf/group/lcls/ds/tools/ccp4-8.0/bin/shelxc\", description=\"CCP4 SHELXC. Generates input files for SHELXD/SHELXE.\", flag_type=\"\", ) placeholder: str = Field( \"xx\", description=\"Placeholder filename stem.\", flag_type=\"\" ) in_file: str = Field( \"\", description=\"Input file for SHELXC with reflections AND proper records.\", flag_type=\"\", ) @validator(\"in_file\", always=True) def validate_in_file(cls, in_file: str, values: Dict[str, Any]) -> str: if in_file == \"\": # get_hkl needed to be run to produce an XDS format file... xds_format_file: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", \"ManipulateHKL\", \"out_file\" ) if xds_format_file: in_file = xds_format_file if in_file[0] != \"<\": # Need to add a redirection for this program # Runs like `shelxc xx <input_file.xds` in_file = f\"<{in_file}\" return in_file","title":"RunSHELXCParameters"},{"location":"source/io/models/smd/","text":"Models for smalldata_tools Tasks. Classes: Name Description SubmitSMDParameters Parameters to run smalldata_tools to produce a smalldata HDF5 file. FindOverlapXSSParameters Parameter model for the FindOverlapXSS Task. Used to determine spatial/temporal overlap based on XSS difference signal. FindOverlapXSSParameters Bases: TaskParameters TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. Source code in lute/io/models/smd.py class FindOverlapXSSParameters(TaskParameters): \"\"\"TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. \"\"\" class ExpConfig(BaseModel): det_name: str ipm_var: str scan_var: Union[str, List[str]] class Thresholds(BaseModel): min_Iscat: Union[int, float] min_ipm: Union[int, float] class AnalysisFlags(BaseModel): use_pyfai: bool = True use_asymls: bool = False exp_config: ExpConfig thresholds: Thresholds analysis_flags: AnalysisFlags SubmitSMDParameters Bases: ThirdPartyParameters Parameters for running smalldata to produce reduced HDF5 files. Source code in lute/io/models/smd.py class SubmitSMDParameters(ThirdPartyParameters): \"\"\"Parameters for running smalldata to produce reduced HDF5 files.\"\"\" class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) m: str = Field( \"mpi4py.run\", description=\"Python option to execute a module's contents as __main__ module.\", flag_type=\"-\", ) producer: str = Field( \"\", description=\"Path to the SmallData producer Python script.\", flag_type=\"\" ) run: str = Field( os.environ.get(\"RUN_NUM\", \"\"), description=\"DAQ Run Number.\", flag_type=\"--\" ) experiment: str = Field( os.environ.get(\"EXPERIMENT\", \"\"), description=\"LCLS Experiment Number.\", flag_type=\"--\", ) stn: NonNegativeInt = Field(0, description=\"Hutch endstation.\", flag_type=\"--\") nevents: int = Field( int(1e9), description=\"Number of events to process.\", flag_type=\"--\" ) directory: Optional[str] = Field( None, description=\"Optional output directory. If None, will be in ${EXP_FOLDER}/hdf5/smalldata.\", flag_type=\"--\", ) ## Need mechanism to set result_from_param=True ... gather_interval: PositiveInt = Field( 25, description=\"Number of events to collect at a time.\", flag_type=\"--\" ) norecorder: bool = Field( False, description=\"Whether to ignore recorder streams.\", flag_type=\"--\" ) url: HttpUrl = Field( \"https://pswww.slac.stanford.edu/ws-auth/lgbk\", description=\"Base URL for eLog posting.\", flag_type=\"--\", ) epicsAll: bool = Field( False, description=\"Whether to store all EPICS PVs. Use with care.\", flag_type=\"--\", ) full: bool = Field( False, description=\"Whether to store all data. Use with EXTRA care.\", flag_type=\"--\", ) fullSum: bool = Field( False, description=\"Whether to store sums for all area detector images.\", flag_type=\"--\", ) default: bool = Field( False, description=\"Whether to store only the default minimal set of data.\", flag_type=\"--\", ) image: bool = Field( False, description=\"Whether to save everything as images. Use with care.\", flag_type=\"--\", ) tiff: bool = Field( False, description=\"Whether to save all images as a single TIFF. Use with EXTRA care.\", flag_type=\"--\", ) centerpix: bool = Field( False, description=\"Whether to mask center pixels for Epix10k2M detectors.\", flag_type=\"--\", ) postRuntable: bool = Field( False, description=\"Whether to post run tables. Also used as a trigger for summary jobs.\", flag_type=\"--\", ) wait: bool = Field( False, description=\"Whether to wait for a file to appear.\", flag_type=\"--\" ) xtcav: bool = Field( False, description=\"Whether to add XTCAV processing to the HDF5 generation.\", flag_type=\"--\", ) noarch: bool = Field( False, description=\"Whether to not use archiver data.\", flag_type=\"--\" ) lute_template_cfg: TemplateConfig = TemplateConfig(template_name=\"\", output_path=\"\") @validator(\"producer\", always=True) def validate_producer_path(cls, producer: str) -> str: return producer @validator(\"lute_template_cfg\", always=True) def use_producer( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if not lute_template_cfg.output_path: lute_template_cfg.output_path = values[\"producer\"] return lute_template_cfg @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment hutch: str = exp[:3] run: int = int(values[\"lute_config\"].run) directory: Optional[str] = values[\"directory\"] if directory is None: directory = f\"/sdf/data/lcls/ds/{hutch}/{exp}/hdf5/smalldata\" fname: str = f\"{exp}_Run{run:04d}.h5\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values Config Bases: Config Identical to super-class Config but includes a result. Source code in lute/io/models/smd.py class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" result_from_params: str = '' class-attribute instance-attribute Defines a result from the parameters. Use a validator to do so. set_result: bool = True class-attribute instance-attribute Whether the Executor should mark a specified parameter as a result.","title":"smd"},{"location":"source/io/models/smd/#io.models.smd.FindOverlapXSSParameters","text":"Bases: TaskParameters TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. Source code in lute/io/models/smd.py class FindOverlapXSSParameters(TaskParameters): \"\"\"TaskParameter model for FindOverlapXSS Task. This Task determines spatial or temporal overlap between an optical pulse and the FEL pulse based on difference scattering (XSS) signal. This Task uses SmallData HDF5 files as a source. \"\"\" class ExpConfig(BaseModel): det_name: str ipm_var: str scan_var: Union[str, List[str]] class Thresholds(BaseModel): min_Iscat: Union[int, float] min_ipm: Union[int, float] class AnalysisFlags(BaseModel): use_pyfai: bool = True use_asymls: bool = False exp_config: ExpConfig thresholds: Thresholds analysis_flags: AnalysisFlags","title":"FindOverlapXSSParameters"},{"location":"source/io/models/smd/#io.models.smd.SubmitSMDParameters","text":"Bases: ThirdPartyParameters Parameters for running smalldata to produce reduced HDF5 files. Source code in lute/io/models/smd.py class SubmitSMDParameters(ThirdPartyParameters): \"\"\"Parameters for running smalldata to produce reduced HDF5 files.\"\"\" class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\" executable: str = Field(\"mpirun\", description=\"MPI executable.\", flag_type=\"\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) p_arg1: str = Field( \"python\", description=\"Executable to run with mpi (i.e. python).\", flag_type=\"\" ) u: str = Field( \"\", description=\"Python option for unbuffered output.\", flag_type=\"-\" ) m: str = Field( \"mpi4py.run\", description=\"Python option to execute a module's contents as __main__ module.\", flag_type=\"-\", ) producer: str = Field( \"\", description=\"Path to the SmallData producer Python script.\", flag_type=\"\" ) run: str = Field( os.environ.get(\"RUN_NUM\", \"\"), description=\"DAQ Run Number.\", flag_type=\"--\" ) experiment: str = Field( os.environ.get(\"EXPERIMENT\", \"\"), description=\"LCLS Experiment Number.\", flag_type=\"--\", ) stn: NonNegativeInt = Field(0, description=\"Hutch endstation.\", flag_type=\"--\") nevents: int = Field( int(1e9), description=\"Number of events to process.\", flag_type=\"--\" ) directory: Optional[str] = Field( None, description=\"Optional output directory. If None, will be in ${EXP_FOLDER}/hdf5/smalldata.\", flag_type=\"--\", ) ## Need mechanism to set result_from_param=True ... gather_interval: PositiveInt = Field( 25, description=\"Number of events to collect at a time.\", flag_type=\"--\" ) norecorder: bool = Field( False, description=\"Whether to ignore recorder streams.\", flag_type=\"--\" ) url: HttpUrl = Field( \"https://pswww.slac.stanford.edu/ws-auth/lgbk\", description=\"Base URL for eLog posting.\", flag_type=\"--\", ) epicsAll: bool = Field( False, description=\"Whether to store all EPICS PVs. Use with care.\", flag_type=\"--\", ) full: bool = Field( False, description=\"Whether to store all data. Use with EXTRA care.\", flag_type=\"--\", ) fullSum: bool = Field( False, description=\"Whether to store sums for all area detector images.\", flag_type=\"--\", ) default: bool = Field( False, description=\"Whether to store only the default minimal set of data.\", flag_type=\"--\", ) image: bool = Field( False, description=\"Whether to save everything as images. Use with care.\", flag_type=\"--\", ) tiff: bool = Field( False, description=\"Whether to save all images as a single TIFF. Use with EXTRA care.\", flag_type=\"--\", ) centerpix: bool = Field( False, description=\"Whether to mask center pixels for Epix10k2M detectors.\", flag_type=\"--\", ) postRuntable: bool = Field( False, description=\"Whether to post run tables. Also used as a trigger for summary jobs.\", flag_type=\"--\", ) wait: bool = Field( False, description=\"Whether to wait for a file to appear.\", flag_type=\"--\" ) xtcav: bool = Field( False, description=\"Whether to add XTCAV processing to the HDF5 generation.\", flag_type=\"--\", ) noarch: bool = Field( False, description=\"Whether to not use archiver data.\", flag_type=\"--\" ) lute_template_cfg: TemplateConfig = TemplateConfig(template_name=\"\", output_path=\"\") @validator(\"producer\", always=True) def validate_producer_path(cls, producer: str) -> str: return producer @validator(\"lute_template_cfg\", always=True) def use_producer( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if not lute_template_cfg.output_path: lute_template_cfg.output_path = values[\"producer\"] return lute_template_cfg @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: exp: str = values[\"lute_config\"].experiment hutch: str = exp[:3] run: int = int(values[\"lute_config\"].run) directory: Optional[str] = values[\"directory\"] if directory is None: directory = f\"/sdf/data/lcls/ds/{hutch}/{exp}/hdf5/smalldata\" fname: str = f\"{exp}_Run{run:04d}.h5\" cls.Config.result_from_params = f\"{directory}/{fname}\" return values","title":"SubmitSMDParameters"},{"location":"source/io/models/smd/#io.models.smd.SubmitSMDParameters.Config","text":"Bases: Config Identical to super-class Config but includes a result. Source code in lute/io/models/smd.py class Config(ThirdPartyParameters.Config): \"\"\"Identical to super-class Config but includes a result.\"\"\" set_result: bool = True \"\"\"Whether the Executor should mark a specified parameter as a result.\"\"\" result_from_params: str = \"\" \"\"\"Defines a result from the parameters. Use a validator to do so.\"\"\"","title":"Config"},{"location":"source/io/models/smd/#io.models.smd.SubmitSMDParameters.Config.result_from_params","text":"Defines a result from the parameters. Use a validator to do so.","title":"result_from_params"},{"location":"source/io/models/smd/#io.models.smd.SubmitSMDParameters.Config.set_result","text":"Whether the Executor should mark a specified parameter as a result.","title":"set_result"},{"location":"source/io/models/tests/","text":"Models for all test Tasks. Classes: Name Description TestParameters Model for most basic test case. Single core first-party Task. Uses only communication via pipes. TestBinaryParameters Parameters for a simple multi- threaded binary executable. TestSocketParameters Model for first-party test requiring communication via socket. TestWriteOutputParameters Model for test Task which writes an output file. Location of file is recorded in database. TestReadOutputParameters Model for test Task which locates an output file based on an entry in the database, if no path is provided. TestBinaryErrParameters Bases: ThirdPartyParameters Same as TestBinary, but exits with non-zero code. Source code in lute/io/models/tests.py class TestBinaryErrParameters(ThirdPartyParameters): \"\"\"Same as TestBinary, but exits with non-zero code.\"\"\" executable: str = Field( \"/sdf/home/d/dorlhiac/test_tasks/test_threads_err\", description=\"Multi-threaded tes tbinary with non-zero exit code.\", ) p_arg1: int = Field(1, description=\"Number of threads.\") TestParameters Bases: TaskParameters Parameters for the test Task Test . Source code in lute/io/models/tests.py class TestParameters(TaskParameters): \"\"\"Parameters for the test Task `Test`.\"\"\" float_var: float = Field(0.01, description=\"A floating point number.\") str_var: str = Field(\"test\", description=\"A string.\") class CompoundVar(BaseModel): int_var: int = 1 dict_var: Dict[str, str] = {\"a\": \"b\"} compound_var: CompoundVar = Field( description=( \"A compound parameter - consists of a `int_var` (int) and `dict_var`\" \" (Dict[str, str]).\" ) ) throw_error: bool = Field( False, description=\"If `True`, raise an exception to test error handling.\" )","title":"tests"},{"location":"source/io/models/tests/#io.models.tests.TestBinaryErrParameters","text":"Bases: ThirdPartyParameters Same as TestBinary, but exits with non-zero code. Source code in lute/io/models/tests.py class TestBinaryErrParameters(ThirdPartyParameters): \"\"\"Same as TestBinary, but exits with non-zero code.\"\"\" executable: str = Field( \"/sdf/home/d/dorlhiac/test_tasks/test_threads_err\", description=\"Multi-threaded tes tbinary with non-zero exit code.\", ) p_arg1: int = Field(1, description=\"Number of threads.\")","title":"TestBinaryErrParameters"},{"location":"source/io/models/tests/#io.models.tests.TestParameters","text":"Bases: TaskParameters Parameters for the test Task Test . Source code in lute/io/models/tests.py class TestParameters(TaskParameters): \"\"\"Parameters for the test Task `Test`.\"\"\" float_var: float = Field(0.01, description=\"A floating point number.\") str_var: str = Field(\"test\", description=\"A string.\") class CompoundVar(BaseModel): int_var: int = 1 dict_var: Dict[str, str] = {\"a\": \"b\"} compound_var: CompoundVar = Field( description=( \"A compound parameter - consists of a `int_var` (int) and `dict_var`\" \" (Dict[str, str]).\" ) ) throw_error: bool = Field( False, description=\"If `True`, raise an exception to test error handling.\" )","title":"TestParameters"},{"location":"source/tasks/dataclasses/","text":"Classes for describing Task state and results. Classes: Name Description TaskResult Output of a specific analysis task. TaskStatus Enumeration of possible Task statuses (running, pending, failed, etc.). DescribedAnalysis Executor's description of a Task run (results, parameters, env). DescribedAnalysis dataclass Complete analysis description. Held by an Executor. Source code in lute/tasks/dataclasses.py @dataclass class DescribedAnalysis: \"\"\"Complete analysis description. Held by an Executor.\"\"\" task_result: TaskResult task_parameters: Optional[TaskParameters] task_env: Dict[str, str] poll_interval: float communicator_desc: List[str] ElogSummaryPlots dataclass Holds a graphical summary intended for display in the eLog. Attributes: display_name ( str ) \u2013 This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. display_name = \"scans/my_motor_scan\" will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. Source code in lute/tasks/dataclasses.py @dataclass class ElogSummaryPlots: \"\"\"Holds a graphical summary intended for display in the eLog. Attributes: display_name (str): This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. `display_name = \"scans/my_motor_scan\"` will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. \"\"\" display_name: str figures: Union[pn.Tabs, hv.Image, plt.Figure] TaskResult dataclass Class for storing the result of a Task's execution with metadata. Attributes: task_name ( str ) \u2013 Name of the associated task which produced it. task_status ( TaskStatus ) \u2013 Status of associated task. summary ( str ) \u2013 Short message/summary associated with the result. payload ( Any ) \u2013 Actual result. May be data in any format. impl_schemas ( Optional [ str ] ) \u2013 A string listing Task schemas implemented by the associated Task . Schemas define the category and expected output of the Task . An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" Source code in lute/tasks/dataclasses.py @dataclass class TaskResult: \"\"\"Class for storing the result of a Task's execution with metadata. Attributes: task_name (str): Name of the associated task which produced it. task_status (TaskStatus): Status of associated task. summary (str): Short message/summary associated with the result. payload (Any): Actual result. May be data in any format. impl_schemas (Optional[str]): A string listing `Task` schemas implemented by the associated `Task`. Schemas define the category and expected output of the `Task`. An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" \"\"\" task_name: str task_status: TaskStatus summary: str payload: Any impl_schemas: Optional[str] = None TaskStatus Bases: Enum Possible Task statuses. Source code in lute/tasks/dataclasses.py class TaskStatus(Enum): \"\"\"Possible Task statuses.\"\"\" PENDING = 0 \"\"\" Task has yet to run. Is Queued, or waiting for prior tasks. \"\"\" RUNNING = 1 \"\"\" Task is in the process of execution. \"\"\" COMPLETED = 2 \"\"\" Task has completed without fatal errors. \"\"\" FAILED = 3 \"\"\" Task encountered a fatal error. \"\"\" STOPPED = 4 \"\"\" Task was, potentially temporarily, stopped/suspended. \"\"\" CANCELLED = 5 \"\"\" Task was cancelled prior to completion or failure. \"\"\" TIMEDOUT = 6 \"\"\" Task did not reach completion due to timeout. \"\"\" CANCELLED = 5 class-attribute instance-attribute Task was cancelled prior to completion or failure. COMPLETED = 2 class-attribute instance-attribute Task has completed without fatal errors. FAILED = 3 class-attribute instance-attribute Task encountered a fatal error. PENDING = 0 class-attribute instance-attribute Task has yet to run. Is Queued, or waiting for prior tasks. RUNNING = 1 class-attribute instance-attribute Task is in the process of execution. STOPPED = 4 class-attribute instance-attribute Task was, potentially temporarily, stopped/suspended. TIMEDOUT = 6 class-attribute instance-attribute Task did not reach completion due to timeout.","title":"dataclasses"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.DescribedAnalysis","text":"Complete analysis description. Held by an Executor. Source code in lute/tasks/dataclasses.py @dataclass class DescribedAnalysis: \"\"\"Complete analysis description. Held by an Executor.\"\"\" task_result: TaskResult task_parameters: Optional[TaskParameters] task_env: Dict[str, str] poll_interval: float communicator_desc: List[str]","title":"DescribedAnalysis"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.ElogSummaryPlots","text":"Holds a graphical summary intended for display in the eLog. Attributes: display_name ( str ) \u2013 This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. display_name = \"scans/my_motor_scan\" will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. Source code in lute/tasks/dataclasses.py @dataclass class ElogSummaryPlots: \"\"\"Holds a graphical summary intended for display in the eLog. Attributes: display_name (str): This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. `display_name = \"scans/my_motor_scan\"` will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. \"\"\" display_name: str figures: Union[pn.Tabs, hv.Image, plt.Figure]","title":"ElogSummaryPlots"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskResult","text":"Class for storing the result of a Task's execution with metadata. Attributes: task_name ( str ) \u2013 Name of the associated task which produced it. task_status ( TaskStatus ) \u2013 Status of associated task. summary ( str ) \u2013 Short message/summary associated with the result. payload ( Any ) \u2013 Actual result. May be data in any format. impl_schemas ( Optional [ str ] ) \u2013 A string listing Task schemas implemented by the associated Task . Schemas define the category and expected output of the Task . An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" Source code in lute/tasks/dataclasses.py @dataclass class TaskResult: \"\"\"Class for storing the result of a Task's execution with metadata. Attributes: task_name (str): Name of the associated task which produced it. task_status (TaskStatus): Status of associated task. summary (str): Short message/summary associated with the result. payload (Any): Actual result. May be data in any format. impl_schemas (Optional[str]): A string listing `Task` schemas implemented by the associated `Task`. Schemas define the category and expected output of the `Task`. An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" \"\"\" task_name: str task_status: TaskStatus summary: str payload: Any impl_schemas: Optional[str] = None","title":"TaskResult"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus","text":"Bases: Enum Possible Task statuses. Source code in lute/tasks/dataclasses.py class TaskStatus(Enum): \"\"\"Possible Task statuses.\"\"\" PENDING = 0 \"\"\" Task has yet to run. Is Queued, or waiting for prior tasks. \"\"\" RUNNING = 1 \"\"\" Task is in the process of execution. \"\"\" COMPLETED = 2 \"\"\" Task has completed without fatal errors. \"\"\" FAILED = 3 \"\"\" Task encountered a fatal error. \"\"\" STOPPED = 4 \"\"\" Task was, potentially temporarily, stopped/suspended. \"\"\" CANCELLED = 5 \"\"\" Task was cancelled prior to completion or failure. \"\"\" TIMEDOUT = 6 \"\"\" Task did not reach completion due to timeout. \"\"\"","title":"TaskStatus"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.CANCELLED","text":"Task was cancelled prior to completion or failure.","title":"CANCELLED"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.COMPLETED","text":"Task has completed without fatal errors.","title":"COMPLETED"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.FAILED","text":"Task encountered a fatal error.","title":"FAILED"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.PENDING","text":"Task has yet to run. Is Queued, or waiting for prior tasks.","title":"PENDING"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.RUNNING","text":"Task is in the process of execution.","title":"RUNNING"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.STOPPED","text":"Task was, potentially temporarily, stopped/suspended.","title":"STOPPED"},{"location":"source/tasks/dataclasses/#tasks.dataclasses.TaskStatus.TIMEDOUT","text":"Task did not reach completion due to timeout.","title":"TIMEDOUT"},{"location":"source/tasks/sfx_find_peaks/","text":"Classes for peak finding tasks in SFX. Classes: Name Description CxiWriter utility class for writing peak finding results to CXI files. FindPeaksPyAlgos peak finding using psana's PyAlgos algorithm. Optional data compression and decompression with libpressio for data reduction tests. CxiWriter Source code in lute/tasks/sfx_find_peaks.py class CxiWriter: def __init__( self, outdir: str, rank: int, exp: str, run: int, n_events: int, det_shape: Tuple[int, ...], min_peaks: int, max_peaks: int, i_x: Any, # Not typed becomes it comes from psana i_y: Any, # Not typed becomes it comes from psana ipx: Any, # Not typed becomes it comes from psana ipy: Any, # Not typed becomes it comes from psana tag: str, ): \"\"\" Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. \"\"\" self._det_shape: Tuple[int, ...] = det_shape self._i_x: Any = i_x self._i_y: Any = i_y self._ipx: Any = ipx self._ipy: Any = ipy self._index: int = 0 # Create and open the HDF5 file fname: str = f\"{exp}_r{run:0>4}_{rank}{tag}.cxi\" Path(outdir).mkdir(exist_ok=True) self._outh5: Any = h5py.File(Path(outdir) / fname, \"w\") # Entry_1 entry for processing with CrystFEL entry_1: Any = self._outh5.create_group(\"entry_1\") keys: List[str] = [ \"nPeaks\", \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ] ds_expId: Any = entry_1.create_dataset( \"experimental_identifier\", (n_events,), maxshape=(None,), dtype=int ) ds_expId.attrs[\"axes\"] = \"experiment_identifier\" data_1: Any = entry_1.create_dataset( \"/entry_1/data_1/data\", (n_events, det_shape[0], det_shape[1]), chunks=(1, det_shape[0], det_shape[1]), maxshape=(None, det_shape[0], det_shape[1]), dtype=numpy.float32, ) data_1.attrs[\"axes\"] = \"experiment_identifier\" key: str for key in [\"powderHits\", \"powderMisses\", \"mask\"]: entry_1.create_dataset( f\"/entry_1/data_1/{key}\", (det_shape[0], det_shape[1]), chunks=(det_shape[0], det_shape[1]), maxshape=(det_shape[0], det_shape[1]), dtype=float, ) # Peak-related entries for key in keys: if key == \"nPeaks\": ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events,), maxshape=(None,), dtype=int, ) ds_x.attrs[\"minPeaks\"] = min_peaks ds_x.attrs[\"maxPeaks\"] = max_peaks else: ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events, max_peaks), maxshape=(None, max_peaks), chunks=(1, max_peaks), dtype=float, ) ds_x.attrs[\"axes\"] = \"experiment_identifier:peaks\" # Timestamp entries lcls_1: Any = self._outh5.create_group(\"LCLS\") keys: List[str] = [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"photon_energy_eV\", ] key: str for key in keys: if key == \"photon_energy_eV\": ds_x: Any = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=float ) else: ds_x = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=int ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" ds_x = self._outh5.create_dataset( \"/LCLS/detector_1/EncoderValue\", (n_events,), maxshape=(None,), dtype=float ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" def write_event( self, img: NDArray[numpy.float_], peaks: Any, # Not typed becomes it comes from psana timestamp_seconds: int, timestamp_nanoseconds: int, timestamp_fiducials: int, photon_energy: float, ): \"\"\" Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event \"\"\" ch_rows: NDArray[numpy.float_] = peaks[:, 0] * self._det_shape[1] + peaks[:, 1] ch_cols: NDArray[numpy.float_] = peaks[:, 2] # Entry_1 entry for processing with CrystFEL self._outh5[\"/entry_1/data_1/data\"][self._index, :, :] = img.reshape( -1, img.shape[-1] ) self._outh5[\"/entry_1/result_1/nPeaks\"][self._index] = peaks.shape[0] self._outh5[\"/entry_1/result_1/peakXPosRaw\"][self._index, : peaks.shape[0]] = ( ch_cols.astype(\"int\") ) self._outh5[\"/entry_1/result_1/peakYPosRaw\"][self._index, : peaks.shape[0]] = ( ch_rows.astype(\"int\") ) self._outh5[\"/entry_1/result_1/rcent\"][self._index, : peaks.shape[0]] = peaks[ :, 6 ] self._outh5[\"/entry_1/result_1/ccent\"][self._index, : peaks.shape[0]] = peaks[ :, 7 ] self._outh5[\"/entry_1/result_1/rmin\"][self._index, : peaks.shape[0]] = peaks[ :, 10 ] self._outh5[\"/entry_1/result_1/rmax\"][self._index, : peaks.shape[0]] = peaks[ :, 11 ] self._outh5[\"/entry_1/result_1/cmin\"][self._index, : peaks.shape[0]] = peaks[ :, 12 ] self._outh5[\"/entry_1/result_1/cmax\"][self._index, : peaks.shape[0]] = peaks[ :, 13 ] self._outh5[\"/entry_1/result_1/peakTotalIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 5] self._outh5[\"/entry_1/result_1/peakMaxIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 4] # Calculate and write pixel radius peaks_cenx: NDArray[numpy.float_] = ( self._i_x[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipx ) peaks_ceny: NDArray[numpy.float_] = ( self._i_y[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipy ) peak_radius: NDArray[numpy.float_] = numpy.sqrt( (peaks_cenx**2) + (peaks_ceny**2) ) self._outh5[\"/entry_1/result_1/peakRadius\"][ self._index, : peaks.shape[0] ] = peak_radius # LCLS entry dataset self._outh5[\"/LCLS/machineTime\"][self._index] = timestamp_seconds self._outh5[\"/LCLS/machineTimeNanoSeconds\"][self._index] = timestamp_nanoseconds self._outh5[\"/LCLS/fiducial\"][self._index] = timestamp_fiducials self._outh5[\"/LCLS/photon_energy_eV\"][self._index] = photon_energy self._index += 1 def write_non_event_data( self, powder_hits: NDArray[numpy.float_], powder_misses: NDArray[numpy.float_], mask: NDArray[numpy.uint16], clen: float, ): \"\"\" Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file \"\"\" # Add powders and mask to files, reshaping them to match the crystfel # convention self._outh5[\"/entry_1/data_1/powderHits\"][:] = powder_hits.reshape( -1, powder_hits.shape[-1] ) self._outh5[\"/entry_1/data_1/powderMisses\"][:] = powder_misses.reshape( -1, powder_misses.shape[-1] ) self._outh5[\"/entry_1/data_1/mask\"][:] = (1 - mask).reshape( -1, mask.shape[-1] ) # Crystfel expects inverted values # Add clen distance self._outh5[\"/LCLS/detector_1/EncoderValue\"][:] = clen def optimize_and_close_file( self, num_hits: int, max_peaks: int, ): \"\"\" Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file \"\"\" # Resize the entry_1 entry data_shape: Tuple[int, ...] = self._outh5[\"/entry_1/data_1/data\"].shape self._outh5[\"/entry_1/data_1/data\"].resize( (num_hits, data_shape[1], data_shape[2]) ) self._outh5[f\"/entry_1/result_1/nPeaks\"].resize((num_hits,)) key: str for key in [ \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ]: self._outh5[f\"/entry_1/result_1/{key}\"].resize((num_hits, max_peaks)) # Resize LCLS entry for key in [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"detector_1/EncoderValue\", \"photon_energy_eV\", ]: self._outh5[f\"/LCLS/{key}\"].resize((num_hits,)) self._outh5.close() __init__(outdir, rank, exp, run, n_events, det_shape, min_peaks, max_peaks, i_x, i_y, ipx, ipy, tag) Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. Source code in lute/tasks/sfx_find_peaks.py def __init__( self, outdir: str, rank: int, exp: str, run: int, n_events: int, det_shape: Tuple[int, ...], min_peaks: int, max_peaks: int, i_x: Any, # Not typed becomes it comes from psana i_y: Any, # Not typed becomes it comes from psana ipx: Any, # Not typed becomes it comes from psana ipy: Any, # Not typed becomes it comes from psana tag: str, ): \"\"\" Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. \"\"\" self._det_shape: Tuple[int, ...] = det_shape self._i_x: Any = i_x self._i_y: Any = i_y self._ipx: Any = ipx self._ipy: Any = ipy self._index: int = 0 # Create and open the HDF5 file fname: str = f\"{exp}_r{run:0>4}_{rank}{tag}.cxi\" Path(outdir).mkdir(exist_ok=True) self._outh5: Any = h5py.File(Path(outdir) / fname, \"w\") # Entry_1 entry for processing with CrystFEL entry_1: Any = self._outh5.create_group(\"entry_1\") keys: List[str] = [ \"nPeaks\", \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ] ds_expId: Any = entry_1.create_dataset( \"experimental_identifier\", (n_events,), maxshape=(None,), dtype=int ) ds_expId.attrs[\"axes\"] = \"experiment_identifier\" data_1: Any = entry_1.create_dataset( \"/entry_1/data_1/data\", (n_events, det_shape[0], det_shape[1]), chunks=(1, det_shape[0], det_shape[1]), maxshape=(None, det_shape[0], det_shape[1]), dtype=numpy.float32, ) data_1.attrs[\"axes\"] = \"experiment_identifier\" key: str for key in [\"powderHits\", \"powderMisses\", \"mask\"]: entry_1.create_dataset( f\"/entry_1/data_1/{key}\", (det_shape[0], det_shape[1]), chunks=(det_shape[0], det_shape[1]), maxshape=(det_shape[0], det_shape[1]), dtype=float, ) # Peak-related entries for key in keys: if key == \"nPeaks\": ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events,), maxshape=(None,), dtype=int, ) ds_x.attrs[\"minPeaks\"] = min_peaks ds_x.attrs[\"maxPeaks\"] = max_peaks else: ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events, max_peaks), maxshape=(None, max_peaks), chunks=(1, max_peaks), dtype=float, ) ds_x.attrs[\"axes\"] = \"experiment_identifier:peaks\" # Timestamp entries lcls_1: Any = self._outh5.create_group(\"LCLS\") keys: List[str] = [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"photon_energy_eV\", ] key: str for key in keys: if key == \"photon_energy_eV\": ds_x: Any = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=float ) else: ds_x = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=int ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" ds_x = self._outh5.create_dataset( \"/LCLS/detector_1/EncoderValue\", (n_events,), maxshape=(None,), dtype=float ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" optimize_and_close_file(num_hits, max_peaks) Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file Source code in lute/tasks/sfx_find_peaks.py def optimize_and_close_file( self, num_hits: int, max_peaks: int, ): \"\"\" Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file \"\"\" # Resize the entry_1 entry data_shape: Tuple[int, ...] = self._outh5[\"/entry_1/data_1/data\"].shape self._outh5[\"/entry_1/data_1/data\"].resize( (num_hits, data_shape[1], data_shape[2]) ) self._outh5[f\"/entry_1/result_1/nPeaks\"].resize((num_hits,)) key: str for key in [ \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ]: self._outh5[f\"/entry_1/result_1/{key}\"].resize((num_hits, max_peaks)) # Resize LCLS entry for key in [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"detector_1/EncoderValue\", \"photon_energy_eV\", ]: self._outh5[f\"/LCLS/{key}\"].resize((num_hits,)) self._outh5.close() write_event(img, peaks, timestamp_seconds, timestamp_nanoseconds, timestamp_fiducials, photon_energy) Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event Source code in lute/tasks/sfx_find_peaks.py def write_event( self, img: NDArray[numpy.float_], peaks: Any, # Not typed becomes it comes from psana timestamp_seconds: int, timestamp_nanoseconds: int, timestamp_fiducials: int, photon_energy: float, ): \"\"\" Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event \"\"\" ch_rows: NDArray[numpy.float_] = peaks[:, 0] * self._det_shape[1] + peaks[:, 1] ch_cols: NDArray[numpy.float_] = peaks[:, 2] # Entry_1 entry for processing with CrystFEL self._outh5[\"/entry_1/data_1/data\"][self._index, :, :] = img.reshape( -1, img.shape[-1] ) self._outh5[\"/entry_1/result_1/nPeaks\"][self._index] = peaks.shape[0] self._outh5[\"/entry_1/result_1/peakXPosRaw\"][self._index, : peaks.shape[0]] = ( ch_cols.astype(\"int\") ) self._outh5[\"/entry_1/result_1/peakYPosRaw\"][self._index, : peaks.shape[0]] = ( ch_rows.astype(\"int\") ) self._outh5[\"/entry_1/result_1/rcent\"][self._index, : peaks.shape[0]] = peaks[ :, 6 ] self._outh5[\"/entry_1/result_1/ccent\"][self._index, : peaks.shape[0]] = peaks[ :, 7 ] self._outh5[\"/entry_1/result_1/rmin\"][self._index, : peaks.shape[0]] = peaks[ :, 10 ] self._outh5[\"/entry_1/result_1/rmax\"][self._index, : peaks.shape[0]] = peaks[ :, 11 ] self._outh5[\"/entry_1/result_1/cmin\"][self._index, : peaks.shape[0]] = peaks[ :, 12 ] self._outh5[\"/entry_1/result_1/cmax\"][self._index, : peaks.shape[0]] = peaks[ :, 13 ] self._outh5[\"/entry_1/result_1/peakTotalIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 5] self._outh5[\"/entry_1/result_1/peakMaxIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 4] # Calculate and write pixel radius peaks_cenx: NDArray[numpy.float_] = ( self._i_x[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipx ) peaks_ceny: NDArray[numpy.float_] = ( self._i_y[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipy ) peak_radius: NDArray[numpy.float_] = numpy.sqrt( (peaks_cenx**2) + (peaks_ceny**2) ) self._outh5[\"/entry_1/result_1/peakRadius\"][ self._index, : peaks.shape[0] ] = peak_radius # LCLS entry dataset self._outh5[\"/LCLS/machineTime\"][self._index] = timestamp_seconds self._outh5[\"/LCLS/machineTimeNanoSeconds\"][self._index] = timestamp_nanoseconds self._outh5[\"/LCLS/fiducial\"][self._index] = timestamp_fiducials self._outh5[\"/LCLS/photon_energy_eV\"][self._index] = photon_energy self._index += 1 write_non_event_data(powder_hits, powder_misses, mask, clen) Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file Source code in lute/tasks/sfx_find_peaks.py def write_non_event_data( self, powder_hits: NDArray[numpy.float_], powder_misses: NDArray[numpy.float_], mask: NDArray[numpy.uint16], clen: float, ): \"\"\" Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file \"\"\" # Add powders and mask to files, reshaping them to match the crystfel # convention self._outh5[\"/entry_1/data_1/powderHits\"][:] = powder_hits.reshape( -1, powder_hits.shape[-1] ) self._outh5[\"/entry_1/data_1/powderMisses\"][:] = powder_misses.reshape( -1, powder_misses.shape[-1] ) self._outh5[\"/entry_1/data_1/mask\"][:] = (1 - mask).reshape( -1, mask.shape[-1] ) # Crystfel expects inverted values # Add clen distance self._outh5[\"/LCLS/detector_1/EncoderValue\"][:] = clen FindPeaksPyAlgos Bases: Task Task that performs peak finding using the PyAlgos peak finding algorithms and writes the peak information to CXI files. Source code in lute/tasks/sfx_find_peaks.py class FindPeaksPyAlgos(Task): \"\"\" Task that performs peak finding using the PyAlgos peak finding algorithms and writes the peak information to CXI files. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: ds: Any = MPIDataSource( f\"exp={self._task_parameters.lute_config.experiment}:\" f\"run={self._task_parameters.lute_config.run}:smd\" ) if self._task_parameters.n_events != 0: ds.break_after(self._task_parameters.n_events) det: Any = Detector(self._task_parameters.det_name) det.do_reshape_2d_to_3d(flag=True) evr: Any = Detector(self._task_parameters.event_receiver) i_x: Any = det.indexes_x(self._task_parameters.lute_config.run).astype( numpy.int64 ) i_y: Any = det.indexes_y(self._task_parameters.lute_config.run).astype( numpy.int64 ) ipx: Any ipy: Any ipx, ipy = det.point_indexes( self._task_parameters.lute_config.run, pxy_um=(0, 0) ) alg: Any = None num_hits: int = 0 num_events: int = 0 num_empty_images: int = 0 tag: str = self._task_parameters.tag if (tag != \"\") and (tag[0] != \"_\"): tag = \"_\" + tag evt: Any for evt in ds.events(): evt_id: Any = evt.get(EventId) timestamp_seconds: int = evt_id.time()[0] timestamp_nanoseconds: int = evt_id.time()[1] timestamp_fiducials: int = evt_id.fiducials() event_codes: Any = evr.eventCodes(evt) if isinstance(self._task_parameters.pv_camera_length, float): clen: float = self._task_parameters.pv_camera_length else: clen = ( ds.env().epicsStore().value(self._task_parameters.pv_camera_length) ) if self._task_parameters.event_logic: if not self._task_parameters.event_code in event_codes: continue img: Any = det.calib(evt) if img is None: num_empty_images += 1 continue if alg is None: det_shape: Tuple[int, ...] = img.shape if len(det_shape) == 3: det_shape = (det_shape[0] * det_shape[1], det_shape[2]) else: det_shape = img.shape mask: NDArray[numpy.uint16] = numpy.ones(det_shape).astype(numpy.uint16) if self._task_parameters.psana_mask: mask = det.mask( self.task_parameters.run, calib=False, status=True, edges=False, centra=False, unbond=False, unbondnbrs=False, ).astype(numpy.uint16) hdffh: Any if self._task_parameters.mask_file is not None: with h5py.File(self._task_parameters.mask_file, \"r\") as hdffh: loaded_mask: NDArray[numpy.int] = hdffh[\"entry_1/data_1/mask\"][ : ] mask *= loaded_mask.astype(numpy.uint16) file_writer: CxiWriter = CxiWriter( outdir=self._task_parameters.outdir, rank=ds.rank, exp=self._task_parameters.lute_config.experiment, run=self._task_parameters.lute_config.run, n_events=self._task_parameters.n_events, det_shape=det_shape, i_x=i_x, i_y=i_y, ipx=ipx, ipy=ipy, min_peaks=self._task_parameters.min_peaks, max_peaks=self._task_parameters.max_peaks, tag=tag, ) alg: Any = PyAlgos(mask=mask, pbits=0) # pbits controls verbosity alg.set_peak_selection_pars( npix_min=self._task_parameters.npix_min, npix_max=self._task_parameters.npix_max, amax_thr=self._task_parameters.amax_thr, atot_thr=self._task_parameters.atot_thr, son_min=self._task_parameters.son_min, ) if self._task_parameters.compression is not None: libpressio_config = generate_libpressio_configuration( compressor=self._task_parameters.compression.compressor, roi_window_size=self._task_parameters.compression.roi_window_size, bin_size=self._task_parameters.compression.bin_size, abs_error=self._task_parameters.compression.abs_error, libpressio_mask=mask, ) powder_hits: NDArray[numpy.float_] = numpy.zeros(det_shape) powder_misses: NDArray[numpy.float_] = numpy.zeros(det_shape) peaks: Any = alg.peak_finder_v3r3( img, rank=self._task_parameters.peak_rank, r0=self._task_parameters.r0, dr=self._task_parameters.dr, # nsigm=self._task_parameters.nsigm, ) num_events += 1 if (peaks.shape[0] >= self._task_parameters.min_peaks) and ( peaks.shape[0] <= self._task_parameters.max_peaks ): if self._task_parameters.compression is not None: libpressio_config_with_peaks = ( add_peaks_to_libpressio_configuration(libpressio_config, peaks) ) compressor = PressioCompressor.from_config( libpressio_config_with_peaks ) compressed_img = compressor.encode(img) decompressed_img = numpy.zeros_like(img) decompressed = compressor.decode(compressed_img, decompressed_img) img = decompressed_img try: photon_energy: float = ( Detector(\"EBeam\").get(evt).ebeamPhotonEnergy() ) except AttributeError: photon_energy = ( 1.23984197386209e-06 / ds.env().epicsStore().value(\"SIOC:SYS0:ML00:AO192\") / 1.0e9 ) file_writer.write_event( img=img, peaks=peaks, timestamp_seconds=timestamp_seconds, timestamp_nanoseconds=timestamp_nanoseconds, timestamp_fiducials=timestamp_fiducials, photon_energy=photon_energy, ) num_hits += 1 # TODO: Fix bug here # generate / update powders if peaks.shape[0] >= self._task_parameters.min_peaks: powder_hits = numpy.maximum(powder_hits, img) else: powder_misses = numpy.maximum(powder_misses, img) if num_empty_images != 0: msg: Message = Message( contents=f\"Rank {ds.rank} encountered {num_empty_images} empty images.\" ) self._report_to_executor(msg) file_writer.write_non_event_data( powder_hits=powder_hits, powder_misses=powder_misses, mask=mask, clen=clen, ) file_writer.optimize_and_close_file( num_hits=num_hits, max_peaks=self._task_parameters.max_peaks ) COMM_WORLD.Barrier() num_hits_per_rank: List[int] = COMM_WORLD.gather(num_hits, root=0) num_hits_total: int = COMM_WORLD.reduce(num_hits, SUM) num_events_per_rank: List[int] = COMM_WORLD.gather(num_events, root=0) if ds.rank == 0: master_fname: Path = write_master_file( mpi_size=ds.size, outdir=self._task_parameters.outdir, exp=self._task_parameters.lute_config.experiment, run=self._task_parameters.lute_config.run, tag=tag, n_hits_per_rank=num_hits_per_rank, n_hits_total=num_hits_total, ) # Write final summary file f: TextIO with open( Path(self._task_parameters.outdir) / f\"peakfinding{tag}.summary\", \"w\" ) as f: print(f\"Number of events processed: {num_events_per_rank[-1]}\", file=f) print(f\"Number of hits found: {num_hits_total}\", file=f) print( \"Fractional hit rate: \" f\"{(num_hits_total/num_events_per_rank[-1]):.2f}\", file=f, ) print(f\"No. hits per rank: {num_hits_per_rank}\", file=f) with open(Path(self._task_parameters.out_file), \"w\") as f: print(f\"{master_fname}\", file=f) # Write out_file def _post_run(self) -> None: super()._post_run() self._result.task_status = TaskStatus.COMPLETED add_peaks_to_libpressio_configuration(lp_json, peaks) Add peak infromation to libpressio configuration Parameters: lp_json: Dictionary storing the configuration JSON structure for the libpressio library. peaks (Any): Peak information as returned by psana. Returns: lp_json: Updated configuration JSON structure for the libpressio library. Source code in lute/tasks/sfx_find_peaks.py def add_peaks_to_libpressio_configuration(lp_json, peaks) -> Dict[str, Any]: \"\"\" Add peak infromation to libpressio configuration Parameters: lp_json: Dictionary storing the configuration JSON structure for the libpressio library. peaks (Any): Peak information as returned by psana. Returns: lp_json: Updated configuration JSON structure for the libpressio library. \"\"\" lp_json[\"compressor_config\"][\"pressio\"][\"roibin\"][\"roibin:centers\"] = ( numpy.ascontiguousarray(numpy.uint64(peaks[:, [2, 1, 0]])) ) return lp_json generate_libpressio_configuration(compressor, roi_window_size, bin_size, abs_error, libpressio_mask) Create the configuration JSON for the libpressio library Parameters: compressor (Literal[\"sz3\", \"qoz\"]): Compression algorithm to use (\"qoz\" or \"sz3\"). abs_error (float): Bound value for the absolute error. bin_size (int): Bining Size. roi_window_size (int): Default size of the ROI window. libpressio_mask (NDArray): mask to be applied to the data. Returns: lp_json (Dict[str, Any]): Dictionary storing the JSON configuration structure for the libpressio library Source code in lute/tasks/sfx_find_peaks.py def generate_libpressio_configuration( compressor: Literal[\"sz3\", \"qoz\"], roi_window_size: int, bin_size: int, abs_error: float, libpressio_mask, ) -> Dict[str, Any]: \"\"\" Create the configuration JSON for the libpressio library Parameters: compressor (Literal[\"sz3\", \"qoz\"]): Compression algorithm to use (\"qoz\" or \"sz3\"). abs_error (float): Bound value for the absolute error. bin_size (int): Bining Size. roi_window_size (int): Default size of the ROI window. libpressio_mask (NDArray): mask to be applied to the data. Returns: lp_json (Dict[str, Any]): Dictionary storing the JSON configuration structure for the libpressio library \"\"\" if compressor == \"qoz\": pressio_opts: Dict[str, Any] = { \"pressio:abs\": abs_error, \"qoz\": {\"qoz:stride\": 8}, } elif compressor == \"sz3\": pressio_opts = {\"pressio:abs\": abs_error} lp_json = { \"compressor_id\": \"pressio\", \"early_config\": { \"pressio\": { \"pressio:compressor\": \"roibin\", \"roibin\": { \"roibin:metric\": \"composite\", \"roibin:background\": \"mask_binning\", \"roibin:roi\": \"fpzip\", \"background\": { \"binning:compressor\": \"pressio\", \"mask_binning:compressor\": \"pressio\", \"pressio\": {\"pressio:compressor\": compressor}, }, \"composite\": { \"composite:plugins\": [ \"size\", \"time\", \"input_stats\", \"error_stat\", ] }, }, } }, \"compressor_config\": { \"pressio\": { \"roibin\": { \"roibin:roi_size\": [roi_window_size, roi_window_size, 0], \"roibin:centers\": None, # \"roibin:roi_strategy\": \"coordinates\", \"roibin:nthreads\": 4, \"roi\": {\"fpzip:prec\": 0}, \"background\": { \"mask_binning:mask\": None, \"mask_binning:shape\": [bin_size, bin_size, 1], \"mask_binning:nthreads\": 4, \"pressio\": pressio_opts, }, } } }, \"name\": \"pressio\", } lp_json[\"compressor_config\"][\"pressio\"][\"roibin\"][\"background\"][ \"mask_binning:mask\" ] = (1 - libpressio_mask) return lp_json write_master_file(mpi_size, outdir, exp, run, tag, n_hits_per_rank, n_hits_total) Generate a virtual dataset to map all individual files for this run. Parameters: mpi_size (int): Number of ranks in the MPI pool. outdir (str): Output directory for cxi file. exp (str): Experiment string. run (int): Experimental run. tag (str): Tag to append to cxi file names. n_hits_per_rank (List[int]): Array containing the number of hits found on each node processing data. n_hits_total (int): Total number of hits found across all nodes. Returns: The path to the the written master file Source code in lute/tasks/sfx_find_peaks.py def write_master_file( mpi_size: int, outdir: str, exp: str, run: int, tag: str, n_hits_per_rank: List[int], n_hits_total: int, ) -> Path: \"\"\" Generate a virtual dataset to map all individual files for this run. Parameters: mpi_size (int): Number of ranks in the MPI pool. outdir (str): Output directory for cxi file. exp (str): Experiment string. run (int): Experimental run. tag (str): Tag to append to cxi file names. n_hits_per_rank (List[int]): Array containing the number of hits found on each node processing data. n_hits_total (int): Total number of hits found across all nodes. Returns: The path to the the written master file \"\"\" # Retrieve paths to the files containing data fnames: List[Path] = [] fi: int for fi in range(mpi_size): if n_hits_per_rank[fi] > 0: fnames.append(Path(outdir) / f\"{exp}_r{run:0>4}_{fi}{tag}.cxi\") if len(fnames) == 0: sys.exit(\"No hits found\") # Retrieve list of entries to populate in the virtual hdf5 file dname_list, key_list, shape_list, dtype_list = [], [], [], [] datasets = [\"/entry_1/result_1\", \"/LCLS/detector_1\", \"/LCLS\", \"/entry_1/data_1\"] f = h5py.File(fnames[0], \"r\") for dname in datasets: dset = f[dname] for key in dset.keys(): if f\"{dname}/{key}\" not in datasets: dname_list.append(dname) key_list.append(key) shape_list.append(dset[key].shape) dtype_list.append(dset[key].dtype) f.close() # Compute cumulative powder hits and misses for all files powder_hits, powder_misses = None, None for fn in fnames: f = h5py.File(fn, \"r\") if powder_hits is None: powder_hits = f[\"entry_1/data_1/powderHits\"][:].copy() powder_misses = f[\"entry_1/data_1/powderMisses\"][:].copy() else: powder_hits = numpy.maximum( powder_hits, f[\"entry_1/data_1/powderHits\"][:].copy() ) powder_misses = numpy.maximum( powder_misses, f[\"entry_1/data_1/powderMisses\"][:].copy() ) f.close() vfname: Path = Path(outdir) / f\"{exp}_r{run:0>4}{tag}.cxi\" with h5py.File(vfname, \"w\") as vdf: # Write the virtual hdf5 file for dnum in range(len(dname_list)): dname = f\"{dname_list[dnum]}/{key_list[dnum]}\" if key_list[dnum] not in [\"mask\", \"powderHits\", \"powderMisses\"]: layout = h5py.VirtualLayout( shape=(n_hits_total,) + shape_list[dnum][1:], dtype=dtype_list[dnum] ) cursor = 0 for i, fn in enumerate(fnames): vsrc = h5py.VirtualSource( fn, dname, shape=(n_hits_per_rank[i],) + shape_list[dnum][1:] ) if len(shape_list[dnum]) == 1: layout[cursor : cursor + n_hits_per_rank[i]] = vsrc else: layout[cursor : cursor + n_hits_per_rank[i], :] = vsrc cursor += n_hits_per_rank[i] vdf.create_virtual_dataset(dname, layout, fillvalue=-1) vdf[\"entry_1/data_1/powderHits\"] = powder_hits vdf[\"entry_1/data_1/powderMisses\"] = powder_misses return vfname","title":"sfx_find_peaks"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.CxiWriter","text":"Source code in lute/tasks/sfx_find_peaks.py class CxiWriter: def __init__( self, outdir: str, rank: int, exp: str, run: int, n_events: int, det_shape: Tuple[int, ...], min_peaks: int, max_peaks: int, i_x: Any, # Not typed becomes it comes from psana i_y: Any, # Not typed becomes it comes from psana ipx: Any, # Not typed becomes it comes from psana ipy: Any, # Not typed becomes it comes from psana tag: str, ): \"\"\" Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. \"\"\" self._det_shape: Tuple[int, ...] = det_shape self._i_x: Any = i_x self._i_y: Any = i_y self._ipx: Any = ipx self._ipy: Any = ipy self._index: int = 0 # Create and open the HDF5 file fname: str = f\"{exp}_r{run:0>4}_{rank}{tag}.cxi\" Path(outdir).mkdir(exist_ok=True) self._outh5: Any = h5py.File(Path(outdir) / fname, \"w\") # Entry_1 entry for processing with CrystFEL entry_1: Any = self._outh5.create_group(\"entry_1\") keys: List[str] = [ \"nPeaks\", \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ] ds_expId: Any = entry_1.create_dataset( \"experimental_identifier\", (n_events,), maxshape=(None,), dtype=int ) ds_expId.attrs[\"axes\"] = \"experiment_identifier\" data_1: Any = entry_1.create_dataset( \"/entry_1/data_1/data\", (n_events, det_shape[0], det_shape[1]), chunks=(1, det_shape[0], det_shape[1]), maxshape=(None, det_shape[0], det_shape[1]), dtype=numpy.float32, ) data_1.attrs[\"axes\"] = \"experiment_identifier\" key: str for key in [\"powderHits\", \"powderMisses\", \"mask\"]: entry_1.create_dataset( f\"/entry_1/data_1/{key}\", (det_shape[0], det_shape[1]), chunks=(det_shape[0], det_shape[1]), maxshape=(det_shape[0], det_shape[1]), dtype=float, ) # Peak-related entries for key in keys: if key == \"nPeaks\": ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events,), maxshape=(None,), dtype=int, ) ds_x.attrs[\"minPeaks\"] = min_peaks ds_x.attrs[\"maxPeaks\"] = max_peaks else: ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events, max_peaks), maxshape=(None, max_peaks), chunks=(1, max_peaks), dtype=float, ) ds_x.attrs[\"axes\"] = \"experiment_identifier:peaks\" # Timestamp entries lcls_1: Any = self._outh5.create_group(\"LCLS\") keys: List[str] = [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"photon_energy_eV\", ] key: str for key in keys: if key == \"photon_energy_eV\": ds_x: Any = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=float ) else: ds_x = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=int ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" ds_x = self._outh5.create_dataset( \"/LCLS/detector_1/EncoderValue\", (n_events,), maxshape=(None,), dtype=float ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" def write_event( self, img: NDArray[numpy.float_], peaks: Any, # Not typed becomes it comes from psana timestamp_seconds: int, timestamp_nanoseconds: int, timestamp_fiducials: int, photon_energy: float, ): \"\"\" Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event \"\"\" ch_rows: NDArray[numpy.float_] = peaks[:, 0] * self._det_shape[1] + peaks[:, 1] ch_cols: NDArray[numpy.float_] = peaks[:, 2] # Entry_1 entry for processing with CrystFEL self._outh5[\"/entry_1/data_1/data\"][self._index, :, :] = img.reshape( -1, img.shape[-1] ) self._outh5[\"/entry_1/result_1/nPeaks\"][self._index] = peaks.shape[0] self._outh5[\"/entry_1/result_1/peakXPosRaw\"][self._index, : peaks.shape[0]] = ( ch_cols.astype(\"int\") ) self._outh5[\"/entry_1/result_1/peakYPosRaw\"][self._index, : peaks.shape[0]] = ( ch_rows.astype(\"int\") ) self._outh5[\"/entry_1/result_1/rcent\"][self._index, : peaks.shape[0]] = peaks[ :, 6 ] self._outh5[\"/entry_1/result_1/ccent\"][self._index, : peaks.shape[0]] = peaks[ :, 7 ] self._outh5[\"/entry_1/result_1/rmin\"][self._index, : peaks.shape[0]] = peaks[ :, 10 ] self._outh5[\"/entry_1/result_1/rmax\"][self._index, : peaks.shape[0]] = peaks[ :, 11 ] self._outh5[\"/entry_1/result_1/cmin\"][self._index, : peaks.shape[0]] = peaks[ :, 12 ] self._outh5[\"/entry_1/result_1/cmax\"][self._index, : peaks.shape[0]] = peaks[ :, 13 ] self._outh5[\"/entry_1/result_1/peakTotalIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 5] self._outh5[\"/entry_1/result_1/peakMaxIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 4] # Calculate and write pixel radius peaks_cenx: NDArray[numpy.float_] = ( self._i_x[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipx ) peaks_ceny: NDArray[numpy.float_] = ( self._i_y[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipy ) peak_radius: NDArray[numpy.float_] = numpy.sqrt( (peaks_cenx**2) + (peaks_ceny**2) ) self._outh5[\"/entry_1/result_1/peakRadius\"][ self._index, : peaks.shape[0] ] = peak_radius # LCLS entry dataset self._outh5[\"/LCLS/machineTime\"][self._index] = timestamp_seconds self._outh5[\"/LCLS/machineTimeNanoSeconds\"][self._index] = timestamp_nanoseconds self._outh5[\"/LCLS/fiducial\"][self._index] = timestamp_fiducials self._outh5[\"/LCLS/photon_energy_eV\"][self._index] = photon_energy self._index += 1 def write_non_event_data( self, powder_hits: NDArray[numpy.float_], powder_misses: NDArray[numpy.float_], mask: NDArray[numpy.uint16], clen: float, ): \"\"\" Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file \"\"\" # Add powders and mask to files, reshaping them to match the crystfel # convention self._outh5[\"/entry_1/data_1/powderHits\"][:] = powder_hits.reshape( -1, powder_hits.shape[-1] ) self._outh5[\"/entry_1/data_1/powderMisses\"][:] = powder_misses.reshape( -1, powder_misses.shape[-1] ) self._outh5[\"/entry_1/data_1/mask\"][:] = (1 - mask).reshape( -1, mask.shape[-1] ) # Crystfel expects inverted values # Add clen distance self._outh5[\"/LCLS/detector_1/EncoderValue\"][:] = clen def optimize_and_close_file( self, num_hits: int, max_peaks: int, ): \"\"\" Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file \"\"\" # Resize the entry_1 entry data_shape: Tuple[int, ...] = self._outh5[\"/entry_1/data_1/data\"].shape self._outh5[\"/entry_1/data_1/data\"].resize( (num_hits, data_shape[1], data_shape[2]) ) self._outh5[f\"/entry_1/result_1/nPeaks\"].resize((num_hits,)) key: str for key in [ \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ]: self._outh5[f\"/entry_1/result_1/{key}\"].resize((num_hits, max_peaks)) # Resize LCLS entry for key in [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"detector_1/EncoderValue\", \"photon_energy_eV\", ]: self._outh5[f\"/LCLS/{key}\"].resize((num_hits,)) self._outh5.close()","title":"CxiWriter"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.CxiWriter.__init__","text":"Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. Source code in lute/tasks/sfx_find_peaks.py def __init__( self, outdir: str, rank: int, exp: str, run: int, n_events: int, det_shape: Tuple[int, ...], min_peaks: int, max_peaks: int, i_x: Any, # Not typed becomes it comes from psana i_y: Any, # Not typed becomes it comes from psana ipx: Any, # Not typed becomes it comes from psana ipy: Any, # Not typed becomes it comes from psana tag: str, ): \"\"\" Set up the CXI files to which peak finding results will be saved. Parameters: outdir (str): Output directory for cxi file. rank (int): MPI rank of the caller. exp (str): Experiment string. run (int): Experimental run. n_events (int): Number of events to process. det_shape (Tuple[int, int]): Shape of the numpy array storing the detector data. This must be aCheetah-stile 2D array. min_peaks (int): Minimum number of peaks per image. max_peaks (int): Maximum number of peaks per image. i_x (Any): Array of pixel indexes along x i_y (Any): Array of pixel indexes along y ipx (Any): Pixel indexes with respect to detector origin (x component) ipy (Any): Pixel indexes with respect to detector origin (y component) tag (str): Tag to append to cxi file names. \"\"\" self._det_shape: Tuple[int, ...] = det_shape self._i_x: Any = i_x self._i_y: Any = i_y self._ipx: Any = ipx self._ipy: Any = ipy self._index: int = 0 # Create and open the HDF5 file fname: str = f\"{exp}_r{run:0>4}_{rank}{tag}.cxi\" Path(outdir).mkdir(exist_ok=True) self._outh5: Any = h5py.File(Path(outdir) / fname, \"w\") # Entry_1 entry for processing with CrystFEL entry_1: Any = self._outh5.create_group(\"entry_1\") keys: List[str] = [ \"nPeaks\", \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ] ds_expId: Any = entry_1.create_dataset( \"experimental_identifier\", (n_events,), maxshape=(None,), dtype=int ) ds_expId.attrs[\"axes\"] = \"experiment_identifier\" data_1: Any = entry_1.create_dataset( \"/entry_1/data_1/data\", (n_events, det_shape[0], det_shape[1]), chunks=(1, det_shape[0], det_shape[1]), maxshape=(None, det_shape[0], det_shape[1]), dtype=numpy.float32, ) data_1.attrs[\"axes\"] = \"experiment_identifier\" key: str for key in [\"powderHits\", \"powderMisses\", \"mask\"]: entry_1.create_dataset( f\"/entry_1/data_1/{key}\", (det_shape[0], det_shape[1]), chunks=(det_shape[0], det_shape[1]), maxshape=(det_shape[0], det_shape[1]), dtype=float, ) # Peak-related entries for key in keys: if key == \"nPeaks\": ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events,), maxshape=(None,), dtype=int, ) ds_x.attrs[\"minPeaks\"] = min_peaks ds_x.attrs[\"maxPeaks\"] = max_peaks else: ds_x: Any = self._outh5.create_dataset( f\"/entry_1/result_1/{key}\", (n_events, max_peaks), maxshape=(None, max_peaks), chunks=(1, max_peaks), dtype=float, ) ds_x.attrs[\"axes\"] = \"experiment_identifier:peaks\" # Timestamp entries lcls_1: Any = self._outh5.create_group(\"LCLS\") keys: List[str] = [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"photon_energy_eV\", ] key: str for key in keys: if key == \"photon_energy_eV\": ds_x: Any = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=float ) else: ds_x = lcls_1.create_dataset( f\"{key}\", (n_events,), maxshape=(None,), dtype=int ) ds_x.attrs[\"axes\"] = \"experiment_identifier\" ds_x = self._outh5.create_dataset( \"/LCLS/detector_1/EncoderValue\", (n_events,), maxshape=(None,), dtype=float ) ds_x.attrs[\"axes\"] = \"experiment_identifier\"","title":"__init__"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.CxiWriter.optimize_and_close_file","text":"Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file Source code in lute/tasks/sfx_find_peaks.py def optimize_and_close_file( self, num_hits: int, max_peaks: int, ): \"\"\" Resize data blocks and write additional information to the file Parameters: num_hits (int): Number of hits for which information has been saved to the file max_peaks (int): Maximum number of peaks (per event) for which information can be written into the file \"\"\" # Resize the entry_1 entry data_shape: Tuple[int, ...] = self._outh5[\"/entry_1/data_1/data\"].shape self._outh5[\"/entry_1/data_1/data\"].resize( (num_hits, data_shape[1], data_shape[2]) ) self._outh5[f\"/entry_1/result_1/nPeaks\"].resize((num_hits,)) key: str for key in [ \"peakXPosRaw\", \"peakYPosRaw\", \"rcent\", \"ccent\", \"rmin\", \"rmax\", \"cmin\", \"cmax\", \"peakTotalIntensity\", \"peakMaxIntensity\", \"peakRadius\", ]: self._outh5[f\"/entry_1/result_1/{key}\"].resize((num_hits, max_peaks)) # Resize LCLS entry for key in [ \"eventNumber\", \"machineTime\", \"machineTimeNanoSeconds\", \"fiducial\", \"detector_1/EncoderValue\", \"photon_energy_eV\", ]: self._outh5[f\"/LCLS/{key}\"].resize((num_hits,)) self._outh5.close()","title":"optimize_and_close_file"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.CxiWriter.write_event","text":"Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event Source code in lute/tasks/sfx_find_peaks.py def write_event( self, img: NDArray[numpy.float_], peaks: Any, # Not typed becomes it comes from psana timestamp_seconds: int, timestamp_nanoseconds: int, timestamp_fiducials: int, photon_energy: float, ): \"\"\" Write peak finding results for an event into the HDF5 file. Parameters: img (NDArray[numpy.float_]): Detector data for the event peaks: (Any): Peak information for the event, as recovered from the PyAlgos algorithm timestamp_seconds (int): Second part of the event's timestamp information timestamp_nanoseconds (int): Nanosecond part of the event's timestamp information timestamp_fiducials (int): Fiducials part of the event's timestamp information photon_energy (float): Photon energy for the event \"\"\" ch_rows: NDArray[numpy.float_] = peaks[:, 0] * self._det_shape[1] + peaks[:, 1] ch_cols: NDArray[numpy.float_] = peaks[:, 2] # Entry_1 entry for processing with CrystFEL self._outh5[\"/entry_1/data_1/data\"][self._index, :, :] = img.reshape( -1, img.shape[-1] ) self._outh5[\"/entry_1/result_1/nPeaks\"][self._index] = peaks.shape[0] self._outh5[\"/entry_1/result_1/peakXPosRaw\"][self._index, : peaks.shape[0]] = ( ch_cols.astype(\"int\") ) self._outh5[\"/entry_1/result_1/peakYPosRaw\"][self._index, : peaks.shape[0]] = ( ch_rows.astype(\"int\") ) self._outh5[\"/entry_1/result_1/rcent\"][self._index, : peaks.shape[0]] = peaks[ :, 6 ] self._outh5[\"/entry_1/result_1/ccent\"][self._index, : peaks.shape[0]] = peaks[ :, 7 ] self._outh5[\"/entry_1/result_1/rmin\"][self._index, : peaks.shape[0]] = peaks[ :, 10 ] self._outh5[\"/entry_1/result_1/rmax\"][self._index, : peaks.shape[0]] = peaks[ :, 11 ] self._outh5[\"/entry_1/result_1/cmin\"][self._index, : peaks.shape[0]] = peaks[ :, 12 ] self._outh5[\"/entry_1/result_1/cmax\"][self._index, : peaks.shape[0]] = peaks[ :, 13 ] self._outh5[\"/entry_1/result_1/peakTotalIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 5] self._outh5[\"/entry_1/result_1/peakMaxIntensity\"][ self._index, : peaks.shape[0] ] = peaks[:, 4] # Calculate and write pixel radius peaks_cenx: NDArray[numpy.float_] = ( self._i_x[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipx ) peaks_ceny: NDArray[numpy.float_] = ( self._i_y[ numpy.array(peaks[:, 0], dtype=numpy.int64), numpy.array(peaks[:, 1], dtype=numpy.int64), numpy.array(peaks[:, 2], dtype=numpy.int64), ] + 0.5 - self._ipy ) peak_radius: NDArray[numpy.float_] = numpy.sqrt( (peaks_cenx**2) + (peaks_ceny**2) ) self._outh5[\"/entry_1/result_1/peakRadius\"][ self._index, : peaks.shape[0] ] = peak_radius # LCLS entry dataset self._outh5[\"/LCLS/machineTime\"][self._index] = timestamp_seconds self._outh5[\"/LCLS/machineTimeNanoSeconds\"][self._index] = timestamp_nanoseconds self._outh5[\"/LCLS/fiducial\"][self._index] = timestamp_fiducials self._outh5[\"/LCLS/photon_energy_eV\"][self._index] = photon_energy self._index += 1","title":"write_event"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.CxiWriter.write_non_event_data","text":"Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file Source code in lute/tasks/sfx_find_peaks.py def write_non_event_data( self, powder_hits: NDArray[numpy.float_], powder_misses: NDArray[numpy.float_], mask: NDArray[numpy.uint16], clen: float, ): \"\"\" Write to the file data that is not related to a specific event (masks, powders) Parameters: powder_hits (NDArray[numpy.float_]): Virtual powder pattern from hits powder_misses (NDArray[numpy.float_]): Virtual powder pattern from hits mask: (NDArray[numpy.uint16]): Pixel ask to write into the file \"\"\" # Add powders and mask to files, reshaping them to match the crystfel # convention self._outh5[\"/entry_1/data_1/powderHits\"][:] = powder_hits.reshape( -1, powder_hits.shape[-1] ) self._outh5[\"/entry_1/data_1/powderMisses\"][:] = powder_misses.reshape( -1, powder_misses.shape[-1] ) self._outh5[\"/entry_1/data_1/mask\"][:] = (1 - mask).reshape( -1, mask.shape[-1] ) # Crystfel expects inverted values # Add clen distance self._outh5[\"/LCLS/detector_1/EncoderValue\"][:] = clen","title":"write_non_event_data"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.FindPeaksPyAlgos","text":"Bases: Task Task that performs peak finding using the PyAlgos peak finding algorithms and writes the peak information to CXI files. Source code in lute/tasks/sfx_find_peaks.py class FindPeaksPyAlgos(Task): \"\"\" Task that performs peak finding using the PyAlgos peak finding algorithms and writes the peak information to CXI files. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: ds: Any = MPIDataSource( f\"exp={self._task_parameters.lute_config.experiment}:\" f\"run={self._task_parameters.lute_config.run}:smd\" ) if self._task_parameters.n_events != 0: ds.break_after(self._task_parameters.n_events) det: Any = Detector(self._task_parameters.det_name) det.do_reshape_2d_to_3d(flag=True) evr: Any = Detector(self._task_parameters.event_receiver) i_x: Any = det.indexes_x(self._task_parameters.lute_config.run).astype( numpy.int64 ) i_y: Any = det.indexes_y(self._task_parameters.lute_config.run).astype( numpy.int64 ) ipx: Any ipy: Any ipx, ipy = det.point_indexes( self._task_parameters.lute_config.run, pxy_um=(0, 0) ) alg: Any = None num_hits: int = 0 num_events: int = 0 num_empty_images: int = 0 tag: str = self._task_parameters.tag if (tag != \"\") and (tag[0] != \"_\"): tag = \"_\" + tag evt: Any for evt in ds.events(): evt_id: Any = evt.get(EventId) timestamp_seconds: int = evt_id.time()[0] timestamp_nanoseconds: int = evt_id.time()[1] timestamp_fiducials: int = evt_id.fiducials() event_codes: Any = evr.eventCodes(evt) if isinstance(self._task_parameters.pv_camera_length, float): clen: float = self._task_parameters.pv_camera_length else: clen = ( ds.env().epicsStore().value(self._task_parameters.pv_camera_length) ) if self._task_parameters.event_logic: if not self._task_parameters.event_code in event_codes: continue img: Any = det.calib(evt) if img is None: num_empty_images += 1 continue if alg is None: det_shape: Tuple[int, ...] = img.shape if len(det_shape) == 3: det_shape = (det_shape[0] * det_shape[1], det_shape[2]) else: det_shape = img.shape mask: NDArray[numpy.uint16] = numpy.ones(det_shape).astype(numpy.uint16) if self._task_parameters.psana_mask: mask = det.mask( self.task_parameters.run, calib=False, status=True, edges=False, centra=False, unbond=False, unbondnbrs=False, ).astype(numpy.uint16) hdffh: Any if self._task_parameters.mask_file is not None: with h5py.File(self._task_parameters.mask_file, \"r\") as hdffh: loaded_mask: NDArray[numpy.int] = hdffh[\"entry_1/data_1/mask\"][ : ] mask *= loaded_mask.astype(numpy.uint16) file_writer: CxiWriter = CxiWriter( outdir=self._task_parameters.outdir, rank=ds.rank, exp=self._task_parameters.lute_config.experiment, run=self._task_parameters.lute_config.run, n_events=self._task_parameters.n_events, det_shape=det_shape, i_x=i_x, i_y=i_y, ipx=ipx, ipy=ipy, min_peaks=self._task_parameters.min_peaks, max_peaks=self._task_parameters.max_peaks, tag=tag, ) alg: Any = PyAlgos(mask=mask, pbits=0) # pbits controls verbosity alg.set_peak_selection_pars( npix_min=self._task_parameters.npix_min, npix_max=self._task_parameters.npix_max, amax_thr=self._task_parameters.amax_thr, atot_thr=self._task_parameters.atot_thr, son_min=self._task_parameters.son_min, ) if self._task_parameters.compression is not None: libpressio_config = generate_libpressio_configuration( compressor=self._task_parameters.compression.compressor, roi_window_size=self._task_parameters.compression.roi_window_size, bin_size=self._task_parameters.compression.bin_size, abs_error=self._task_parameters.compression.abs_error, libpressio_mask=mask, ) powder_hits: NDArray[numpy.float_] = numpy.zeros(det_shape) powder_misses: NDArray[numpy.float_] = numpy.zeros(det_shape) peaks: Any = alg.peak_finder_v3r3( img, rank=self._task_parameters.peak_rank, r0=self._task_parameters.r0, dr=self._task_parameters.dr, # nsigm=self._task_parameters.nsigm, ) num_events += 1 if (peaks.shape[0] >= self._task_parameters.min_peaks) and ( peaks.shape[0] <= self._task_parameters.max_peaks ): if self._task_parameters.compression is not None: libpressio_config_with_peaks = ( add_peaks_to_libpressio_configuration(libpressio_config, peaks) ) compressor = PressioCompressor.from_config( libpressio_config_with_peaks ) compressed_img = compressor.encode(img) decompressed_img = numpy.zeros_like(img) decompressed = compressor.decode(compressed_img, decompressed_img) img = decompressed_img try: photon_energy: float = ( Detector(\"EBeam\").get(evt).ebeamPhotonEnergy() ) except AttributeError: photon_energy = ( 1.23984197386209e-06 / ds.env().epicsStore().value(\"SIOC:SYS0:ML00:AO192\") / 1.0e9 ) file_writer.write_event( img=img, peaks=peaks, timestamp_seconds=timestamp_seconds, timestamp_nanoseconds=timestamp_nanoseconds, timestamp_fiducials=timestamp_fiducials, photon_energy=photon_energy, ) num_hits += 1 # TODO: Fix bug here # generate / update powders if peaks.shape[0] >= self._task_parameters.min_peaks: powder_hits = numpy.maximum(powder_hits, img) else: powder_misses = numpy.maximum(powder_misses, img) if num_empty_images != 0: msg: Message = Message( contents=f\"Rank {ds.rank} encountered {num_empty_images} empty images.\" ) self._report_to_executor(msg) file_writer.write_non_event_data( powder_hits=powder_hits, powder_misses=powder_misses, mask=mask, clen=clen, ) file_writer.optimize_and_close_file( num_hits=num_hits, max_peaks=self._task_parameters.max_peaks ) COMM_WORLD.Barrier() num_hits_per_rank: List[int] = COMM_WORLD.gather(num_hits, root=0) num_hits_total: int = COMM_WORLD.reduce(num_hits, SUM) num_events_per_rank: List[int] = COMM_WORLD.gather(num_events, root=0) if ds.rank == 0: master_fname: Path = write_master_file( mpi_size=ds.size, outdir=self._task_parameters.outdir, exp=self._task_parameters.lute_config.experiment, run=self._task_parameters.lute_config.run, tag=tag, n_hits_per_rank=num_hits_per_rank, n_hits_total=num_hits_total, ) # Write final summary file f: TextIO with open( Path(self._task_parameters.outdir) / f\"peakfinding{tag}.summary\", \"w\" ) as f: print(f\"Number of events processed: {num_events_per_rank[-1]}\", file=f) print(f\"Number of hits found: {num_hits_total}\", file=f) print( \"Fractional hit rate: \" f\"{(num_hits_total/num_events_per_rank[-1]):.2f}\", file=f, ) print(f\"No. hits per rank: {num_hits_per_rank}\", file=f) with open(Path(self._task_parameters.out_file), \"w\") as f: print(f\"{master_fname}\", file=f) # Write out_file def _post_run(self) -> None: super()._post_run() self._result.task_status = TaskStatus.COMPLETED","title":"FindPeaksPyAlgos"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.add_peaks_to_libpressio_configuration","text":"Add peak infromation to libpressio configuration Parameters: lp_json: Dictionary storing the configuration JSON structure for the libpressio library. peaks (Any): Peak information as returned by psana. Returns: lp_json: Updated configuration JSON structure for the libpressio library. Source code in lute/tasks/sfx_find_peaks.py def add_peaks_to_libpressio_configuration(lp_json, peaks) -> Dict[str, Any]: \"\"\" Add peak infromation to libpressio configuration Parameters: lp_json: Dictionary storing the configuration JSON structure for the libpressio library. peaks (Any): Peak information as returned by psana. Returns: lp_json: Updated configuration JSON structure for the libpressio library. \"\"\" lp_json[\"compressor_config\"][\"pressio\"][\"roibin\"][\"roibin:centers\"] = ( numpy.ascontiguousarray(numpy.uint64(peaks[:, [2, 1, 0]])) ) return lp_json","title":"add_peaks_to_libpressio_configuration"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.generate_libpressio_configuration","text":"Create the configuration JSON for the libpressio library Parameters: compressor (Literal[\"sz3\", \"qoz\"]): Compression algorithm to use (\"qoz\" or \"sz3\"). abs_error (float): Bound value for the absolute error. bin_size (int): Bining Size. roi_window_size (int): Default size of the ROI window. libpressio_mask (NDArray): mask to be applied to the data. Returns: lp_json (Dict[str, Any]): Dictionary storing the JSON configuration structure for the libpressio library Source code in lute/tasks/sfx_find_peaks.py def generate_libpressio_configuration( compressor: Literal[\"sz3\", \"qoz\"], roi_window_size: int, bin_size: int, abs_error: float, libpressio_mask, ) -> Dict[str, Any]: \"\"\" Create the configuration JSON for the libpressio library Parameters: compressor (Literal[\"sz3\", \"qoz\"]): Compression algorithm to use (\"qoz\" or \"sz3\"). abs_error (float): Bound value for the absolute error. bin_size (int): Bining Size. roi_window_size (int): Default size of the ROI window. libpressio_mask (NDArray): mask to be applied to the data. Returns: lp_json (Dict[str, Any]): Dictionary storing the JSON configuration structure for the libpressio library \"\"\" if compressor == \"qoz\": pressio_opts: Dict[str, Any] = { \"pressio:abs\": abs_error, \"qoz\": {\"qoz:stride\": 8}, } elif compressor == \"sz3\": pressio_opts = {\"pressio:abs\": abs_error} lp_json = { \"compressor_id\": \"pressio\", \"early_config\": { \"pressio\": { \"pressio:compressor\": \"roibin\", \"roibin\": { \"roibin:metric\": \"composite\", \"roibin:background\": \"mask_binning\", \"roibin:roi\": \"fpzip\", \"background\": { \"binning:compressor\": \"pressio\", \"mask_binning:compressor\": \"pressio\", \"pressio\": {\"pressio:compressor\": compressor}, }, \"composite\": { \"composite:plugins\": [ \"size\", \"time\", \"input_stats\", \"error_stat\", ] }, }, } }, \"compressor_config\": { \"pressio\": { \"roibin\": { \"roibin:roi_size\": [roi_window_size, roi_window_size, 0], \"roibin:centers\": None, # \"roibin:roi_strategy\": \"coordinates\", \"roibin:nthreads\": 4, \"roi\": {\"fpzip:prec\": 0}, \"background\": { \"mask_binning:mask\": None, \"mask_binning:shape\": [bin_size, bin_size, 1], \"mask_binning:nthreads\": 4, \"pressio\": pressio_opts, }, } } }, \"name\": \"pressio\", } lp_json[\"compressor_config\"][\"pressio\"][\"roibin\"][\"background\"][ \"mask_binning:mask\" ] = (1 - libpressio_mask) return lp_json","title":"generate_libpressio_configuration"},{"location":"source/tasks/sfx_find_peaks/#tasks.sfx_find_peaks.write_master_file","text":"Generate a virtual dataset to map all individual files for this run. Parameters: mpi_size (int): Number of ranks in the MPI pool. outdir (str): Output directory for cxi file. exp (str): Experiment string. run (int): Experimental run. tag (str): Tag to append to cxi file names. n_hits_per_rank (List[int]): Array containing the number of hits found on each node processing data. n_hits_total (int): Total number of hits found across all nodes. Returns: The path to the the written master file Source code in lute/tasks/sfx_find_peaks.py def write_master_file( mpi_size: int, outdir: str, exp: str, run: int, tag: str, n_hits_per_rank: List[int], n_hits_total: int, ) -> Path: \"\"\" Generate a virtual dataset to map all individual files for this run. Parameters: mpi_size (int): Number of ranks in the MPI pool. outdir (str): Output directory for cxi file. exp (str): Experiment string. run (int): Experimental run. tag (str): Tag to append to cxi file names. n_hits_per_rank (List[int]): Array containing the number of hits found on each node processing data. n_hits_total (int): Total number of hits found across all nodes. Returns: The path to the the written master file \"\"\" # Retrieve paths to the files containing data fnames: List[Path] = [] fi: int for fi in range(mpi_size): if n_hits_per_rank[fi] > 0: fnames.append(Path(outdir) / f\"{exp}_r{run:0>4}_{fi}{tag}.cxi\") if len(fnames) == 0: sys.exit(\"No hits found\") # Retrieve list of entries to populate in the virtual hdf5 file dname_list, key_list, shape_list, dtype_list = [], [], [], [] datasets = [\"/entry_1/result_1\", \"/LCLS/detector_1\", \"/LCLS\", \"/entry_1/data_1\"] f = h5py.File(fnames[0], \"r\") for dname in datasets: dset = f[dname] for key in dset.keys(): if f\"{dname}/{key}\" not in datasets: dname_list.append(dname) key_list.append(key) shape_list.append(dset[key].shape) dtype_list.append(dset[key].dtype) f.close() # Compute cumulative powder hits and misses for all files powder_hits, powder_misses = None, None for fn in fnames: f = h5py.File(fn, \"r\") if powder_hits is None: powder_hits = f[\"entry_1/data_1/powderHits\"][:].copy() powder_misses = f[\"entry_1/data_1/powderMisses\"][:].copy() else: powder_hits = numpy.maximum( powder_hits, f[\"entry_1/data_1/powderHits\"][:].copy() ) powder_misses = numpy.maximum( powder_misses, f[\"entry_1/data_1/powderMisses\"][:].copy() ) f.close() vfname: Path = Path(outdir) / f\"{exp}_r{run:0>4}{tag}.cxi\" with h5py.File(vfname, \"w\") as vdf: # Write the virtual hdf5 file for dnum in range(len(dname_list)): dname = f\"{dname_list[dnum]}/{key_list[dnum]}\" if key_list[dnum] not in [\"mask\", \"powderHits\", \"powderMisses\"]: layout = h5py.VirtualLayout( shape=(n_hits_total,) + shape_list[dnum][1:], dtype=dtype_list[dnum] ) cursor = 0 for i, fn in enumerate(fnames): vsrc = h5py.VirtualSource( fn, dname, shape=(n_hits_per_rank[i],) + shape_list[dnum][1:] ) if len(shape_list[dnum]) == 1: layout[cursor : cursor + n_hits_per_rank[i]] = vsrc else: layout[cursor : cursor + n_hits_per_rank[i], :] = vsrc cursor += n_hits_per_rank[i] vdf.create_virtual_dataset(dname, layout, fillvalue=-1) vdf[\"entry_1/data_1/powderHits\"] = powder_hits vdf[\"entry_1/data_1/powderMisses\"] = powder_misses return vfname","title":"write_master_file"},{"location":"source/tasks/sfx_index/","text":"Classes for indexing tasks in SFX. Classes: Name Description ConcatenateStreamFIles task that merges multiple stream files into a single file. ConcatenateStreamFiles Bases: Task Task that merges stream files located within a directory tree. Source code in lute/tasks/sfx_index.py class ConcatenateStreamFiles(Task): \"\"\" Task that merges stream files located within a directory tree. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: stream_file_path: Path = Path(self._task_parameters.in_file) stream_file_list: List[Path] = list( stream_file_path.rglob(f\"{self._task_parameters.tag}_*.stream\") ) processed_file_list = [str(stream_file) for stream_file in stream_file_list] msg: Message = Message( contents=f\"Merging following stream files: {processed_file_list} into \" f\"{self._task_parameters.out_file}\", ) self._report_to_executor(msg) wfd: BinaryIO with open(self._task_parameters.out_file, \"wb\") as wfd: infile: Path for infile in stream_file_list: fd: BinaryIO with open(infile, \"rb\") as fd: shutil.copyfileobj(fd, wfd)","title":"sfx_index"},{"location":"source/tasks/sfx_index/#tasks.sfx_index.ConcatenateStreamFiles","text":"Bases: Task Task that merges stream files located within a directory tree. Source code in lute/tasks/sfx_index.py class ConcatenateStreamFiles(Task): \"\"\" Task that merges stream files located within a directory tree. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: stream_file_path: Path = Path(self._task_parameters.in_file) stream_file_list: List[Path] = list( stream_file_path.rglob(f\"{self._task_parameters.tag}_*.stream\") ) processed_file_list = [str(stream_file) for stream_file in stream_file_list] msg: Message = Message( contents=f\"Merging following stream files: {processed_file_list} into \" f\"{self._task_parameters.out_file}\", ) self._report_to_executor(msg) wfd: BinaryIO with open(self._task_parameters.out_file, \"wb\") as wfd: infile: Path for infile in stream_file_list: fd: BinaryIO with open(infile, \"rb\") as fd: shutil.copyfileobj(fd, wfd)","title":"ConcatenateStreamFiles"},{"location":"source/tasks/task/","text":"Base classes for implementing analysis tasks. Classes: Name Description Task Abstract base class from which all analysis tasks are derived. ThirdPartyTask Class to run a third-party executable binary as a Task . DescribedAnalysis dataclass Complete analysis description. Held by an Executor. Source code in lute/tasks/dataclasses.py @dataclass class DescribedAnalysis: \"\"\"Complete analysis description. Held by an Executor.\"\"\" task_result: TaskResult task_parameters: Optional[TaskParameters] task_env: Dict[str, str] poll_interval: float communicator_desc: List[str] ElogSummaryPlots dataclass Holds a graphical summary intended for display in the eLog. Attributes: display_name ( str ) \u2013 This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. display_name = \"scans/my_motor_scan\" will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. Source code in lute/tasks/dataclasses.py @dataclass class ElogSummaryPlots: \"\"\"Holds a graphical summary intended for display in the eLog. Attributes: display_name (str): This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. `display_name = \"scans/my_motor_scan\"` will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. \"\"\" display_name: str figures: Union[pn.Tabs, hv.Image, plt.Figure] Task Bases: ABC Abstract base class for analysis tasks. Attributes: name ( str ) \u2013 The name of the Task. Source code in lute/tasks/task.py class Task(ABC): \"\"\"Abstract base class for analysis tasks. Attributes: name (str): The name of the Task. \"\"\" def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. \"\"\" self.name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] self._result: TaskResult = TaskResult( task_name=self.name, task_status=TaskStatus.PENDING, summary=\"PENDING\", payload=\"\", ) self._task_parameters: TaskParameters = params timeout: int = self._task_parameters.lute_config.task_timeout signal.setitimer(signal.ITIMER_REAL, timeout) run_directory: Optional[str] = self._task_parameters.Config.run_directory if run_directory is not None: try: os.chdir(run_directory) except FileNotFoundError: warnings.warn( ( f\"Attempt to change to {run_directory}, but it is not found!\\n\" f\"Will attempt to run from {os.getcwd()}. It may fail!\" ), category=UserWarning, ) def run(self) -> None: \"\"\"Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. \"\"\" self._signal_start() self._pre_run() self._run() self._post_run() self._signal_result() @abstractmethod def _run(self) -> None: \"\"\"Actual analysis to run. Overridden by subclasses. Separating the calling API from the implementation allows `run` to have pre and post task functionality embedded easily into a single function call. \"\"\" ... def _pre_run(self) -> None: \"\"\"Code to run BEFORE the main analysis takes place. This function may, or may not, be employed by subclasses. \"\"\" ... def _post_run(self) -> None: \"\"\"Code to run AFTER the main analysis takes place. This function may, or may not, be employed by subclasses. \"\"\" ... @property def result(self) -> TaskResult: \"\"\"TaskResult: Read-only Task Result information.\"\"\" return self._result def __call__(self) -> None: self.run() def _signal_start(self) -> None: \"\"\"Send the signal that the Task will begin shortly.\"\"\" start_msg: Message = Message( contents=self._task_parameters, signal=\"TASK_STARTED\" ) self._result.task_status = TaskStatus.RUNNING self._report_to_executor(start_msg) def _signal_result(self) -> None: \"\"\"Send the signal that results are ready along with the results.\"\"\" signal: str = \"TASK_RESULT\" results_msg: Message = Message(contents=self.result, signal=signal) self._report_to_executor(results_msg) time.sleep(0.1) def _report_to_executor(self, msg: Message) -> None: \"\"\"Send a message to the Executor. Details of `Communicator` choice are hidden from the caller. This method may be overriden by subclasses with specialized functionality. Args: msg (Message): The message object to send. \"\"\" communicator: Communicator if isinstance(msg.contents, str) or msg.contents is None: communicator = PipeCommunicator() else: communicator = SocketCommunicator() communicator.write(msg) def clean_up_timeout(self) -> None: \"\"\"Perform any necessary cleanup actions before exit if timing out.\"\"\" ... result: TaskResult property TaskResult: Read-only Task Result information. __init__(*, params) Initialize a Task. Parameters: params ( TaskParameters ) \u2013 Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. Source code in lute/tasks/task.py def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. \"\"\" self.name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] self._result: TaskResult = TaskResult( task_name=self.name, task_status=TaskStatus.PENDING, summary=\"PENDING\", payload=\"\", ) self._task_parameters: TaskParameters = params timeout: int = self._task_parameters.lute_config.task_timeout signal.setitimer(signal.ITIMER_REAL, timeout) run_directory: Optional[str] = self._task_parameters.Config.run_directory if run_directory is not None: try: os.chdir(run_directory) except FileNotFoundError: warnings.warn( ( f\"Attempt to change to {run_directory}, but it is not found!\\n\" f\"Will attempt to run from {os.getcwd()}. It may fail!\" ), category=UserWarning, ) clean_up_timeout() Perform any necessary cleanup actions before exit if timing out. Source code in lute/tasks/task.py def clean_up_timeout(self) -> None: \"\"\"Perform any necessary cleanup actions before exit if timing out.\"\"\" ... run() Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. Source code in lute/tasks/task.py def run(self) -> None: \"\"\"Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. \"\"\" self._signal_start() self._pre_run() self._run() self._post_run() self._signal_result() TaskResult dataclass Class for storing the result of a Task's execution with metadata. Attributes: task_name ( str ) \u2013 Name of the associated task which produced it. task_status ( TaskStatus ) \u2013 Status of associated task. summary ( str ) \u2013 Short message/summary associated with the result. payload ( Any ) \u2013 Actual result. May be data in any format. impl_schemas ( Optional [ str ] ) \u2013 A string listing Task schemas implemented by the associated Task . Schemas define the category and expected output of the Task . An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" Source code in lute/tasks/dataclasses.py @dataclass class TaskResult: \"\"\"Class for storing the result of a Task's execution with metadata. Attributes: task_name (str): Name of the associated task which produced it. task_status (TaskStatus): Status of associated task. summary (str): Short message/summary associated with the result. payload (Any): Actual result. May be data in any format. impl_schemas (Optional[str]): A string listing `Task` schemas implemented by the associated `Task`. Schemas define the category and expected output of the `Task`. An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" \"\"\" task_name: str task_status: TaskStatus summary: str payload: Any impl_schemas: Optional[str] = None TaskStatus Bases: Enum Possible Task statuses. Source code in lute/tasks/dataclasses.py class TaskStatus(Enum): \"\"\"Possible Task statuses.\"\"\" PENDING = 0 \"\"\" Task has yet to run. Is Queued, or waiting for prior tasks. \"\"\" RUNNING = 1 \"\"\" Task is in the process of execution. \"\"\" COMPLETED = 2 \"\"\" Task has completed without fatal errors. \"\"\" FAILED = 3 \"\"\" Task encountered a fatal error. \"\"\" STOPPED = 4 \"\"\" Task was, potentially temporarily, stopped/suspended. \"\"\" CANCELLED = 5 \"\"\" Task was cancelled prior to completion or failure. \"\"\" TIMEDOUT = 6 \"\"\" Task did not reach completion due to timeout. \"\"\" CANCELLED = 5 class-attribute instance-attribute Task was cancelled prior to completion or failure. COMPLETED = 2 class-attribute instance-attribute Task has completed without fatal errors. FAILED = 3 class-attribute instance-attribute Task encountered a fatal error. PENDING = 0 class-attribute instance-attribute Task has yet to run. Is Queued, or waiting for prior tasks. RUNNING = 1 class-attribute instance-attribute Task is in the process of execution. STOPPED = 4 class-attribute instance-attribute Task was, potentially temporarily, stopped/suspended. TIMEDOUT = 6 class-attribute instance-attribute Task did not reach completion due to timeout. ThirdPartyTask Bases: Task A Task interface to analysis with binary executables. Source code in lute/tasks/task.py class ThirdPartyTask(Task): \"\"\"A `Task` interface to analysis with binary executables.\"\"\" def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. `Task`s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter `executable`. All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * `arg_name = 3` is converted to `--arg_name 3` Positional arguments can be included with `p_argN` where `N` is any integer: * `p_arg1 = 3` is converted to `3` \"\"\" super().__init__(params=params) self._cmd = self._task_parameters.executable self._args_list: List[str] = [self._cmd] self._template_context: Dict[str, Any] = {} def _add_to_jinja_context(self, param_name: str, value: Any) -> None: \"\"\"Store a parameter as a Jinja template variable. Variables are stored in a dictionary which is used to fill in a premade Jinja template for a third party configuration file. Args: param_name (str): Name to store the variable as. This should be the name defined in the corresponding pydantic model. This name MUST match the name used in the Jinja Template! value (Any): The value to store. If possible, large chunks of the template should be represented as a single dictionary for simplicity; however, any type can be stored as needed. \"\"\" context_update: Dict[str, Any] = {param_name: value} if __debug__: msg: Message = Message(contents=f\"TemplateParameters: {context_update}\") self._report_to_executor(msg) self._template_context.update(context_update) def _template_to_config_file(self) -> None: \"\"\"Convert a template file into a valid configuration file. Uses Jinja to fill in a provided template file with variables supplied through the LUTE config file. This facilitates parameter modification for third party tasks which use a separate configuration, in addition to, or instead of, command-line arguments. \"\"\" from jinja2 import Environment, FileSystemLoader, Template out_file: str = self._task_parameters.lute_template_cfg.output_path template_name: str = self._task_parameters.lute_template_cfg.template_name lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") template_dir: str if lute_path is None: warnings.warn( \"LUTE_PATH is None in Task process! Using relative path for templates!\", category=UserWarning, ) template_dir: str = \"../../config/templates\" else: template_dir = f\"{lute_path}/config/templates\" environment: Environment = Environment(loader=FileSystemLoader(template_dir)) template: Template = environment.get_template(template_name) with open(out_file, \"w\", encoding=\"utf-8\") as cfg_out: cfg_out.write(template.render(self._template_context)) def _pre_run(self) -> None: \"\"\"Parse the parameters into an appropriate argument list. Arguments are identified by a `flag_type` attribute, defined in the pydantic model, which indicates how to pass the parameter and its argument on the command-line. This method parses flag:value pairs into an appropriate list to be used to call the executable. Note: ThirdPartyParameter objects are returned by custom model validators. Objects of this type are assumed to be used for a templated config file used by the third party executable for configuration. The parsing of these parameters is performed separately by a template file used as an input to Jinja. This method solely identifies the necessary objects and passes them all along. Refer to the template files and pydantic models for more information on how these parameters are defined and identified. \"\"\" super()._pre_run() full_schema: Dict[str, Union[str, Dict[str, Any]]] = ( self._task_parameters.schema() ) short_flags_use_eq: bool long_flags_use_eq: bool if hasattr(self._task_parameters.Config, \"short_flags_use_eq\"): short_flags_use_eq: bool = self._task_parameters.Config.short_flags_use_eq long_flags_use_eq: bool = self._task_parameters.Config.long_flags_use_eq else: short_flags_use_eq = False long_flags_use_eq = False for param, value in self._task_parameters.dict().items(): # Clunky test with __dict__[param] because compound model-types are # converted to `dict`. E.g. type(value) = dict not AnalysisHeader if ( param == \"executable\" or value is None # Cannot have empty values in argument list for execvp or value == \"\" # But do want to include, e.g. 0 or isinstance(self._task_parameters.__dict__[param], TemplateConfig) or isinstance(self._task_parameters.__dict__[param], AnalysisHeader) ): continue if isinstance(self._task_parameters.__dict__[param], TemplateParameters): # TemplateParameters objects have a single parameter `params` self._add_to_jinja_context(param_name=param, value=value.params) continue param_attributes: Dict[str, Any] = full_schema[\"properties\"][param] # Some model params do not match the commnad-line parameter names param_repr: str if \"rename_param\" in param_attributes: param_repr = param_attributes[\"rename_param\"] else: param_repr = param if \"flag_type\" in param_attributes: flag: str = param_attributes[\"flag_type\"] if flag: # \"-\" or \"--\" flags if flag == \"--\" and isinstance(value, bool) and not value: continue constructed_flag: str = f\"{flag}{param_repr}\" if flag == \"--\" and isinstance(value, bool) and value: # On/off flag, e.g. something like --verbose: No Arg self._args_list.append(f\"{constructed_flag}\") continue if (flag == \"-\" and short_flags_use_eq) or ( flag == \"--\" and long_flags_use_eq ): # Must come after above check! Otherwise you get --param=True # Flags following --param=value or -param=value constructed_flag = f\"{constructed_flag}={value}\" self._args_list.append(f\"{constructed_flag}\") continue self._args_list.append(f\"{constructed_flag}\") else: warnings.warn( ( f\"Model parameters should be defined using Field(...,flag_type='')\" f\" in the future. Parameter: {param}\" ), category=PendingDeprecationWarning, ) if len(param) == 1: # Single-dash flags if short_flags_use_eq: self._args_list.append(f\"-{param_repr}={value}\") continue self._args_list.append(f\"-{param_repr}\") elif \"p_arg\" in param: # Positional arguments pass else: # Double-dash flags if isinstance(value, bool) and not value: continue if long_flags_use_eq: self._args_list.append(f\"--{param_repr}={value}\") continue self._args_list.append(f\"--{param_repr}\") if isinstance(value, bool) and value: continue if isinstance(value, str) and \" \" in value: for val in value.split(): self._args_list.append(f\"{val}\") else: self._args_list.append(f\"{value}\") if ( hasattr(self._task_parameters, \"lute_template_cfg\") and self._template_context ): self._template_to_config_file() def _run(self) -> None: \"\"\"Execute the new program by replacing the current process.\"\"\" if __debug__: time.sleep(0.1) msg: Message = Message(contents=self._formatted_command()) self._report_to_executor(msg) LUTE_DEBUG_EXIT(\"LUTE_DEBUG_BEFORE_TPP_EXEC\") os.execvp(file=self._cmd, args=self._args_list) def _formatted_command(self) -> str: \"\"\"Returns the command as it would passed on the command-line.\"\"\" formatted_cmd: str = \"\".join(f\"{arg} \" for arg in self._args_list) return formatted_cmd def _signal_start(self) -> None: \"\"\"Override start signal method to switch communication methods.\"\"\" super()._signal_start() time.sleep(0.05) signal: str = \"NO_PICKLE_MODE\" msg: Message = Message(signal=signal) self._report_to_executor(msg) __init__(*, params) Initialize a Task. Parameters: params ( TaskParameters ) \u2013 Parameters needed to properly configure the analysis task. Task s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter executable . All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * arg_name = 3 is converted to --arg_name 3 Positional arguments can be included with p_argN where N is any integer: * p_arg1 = 3 is converted to 3 Source code in lute/tasks/task.py def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. `Task`s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter `executable`. All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * `arg_name = 3` is converted to `--arg_name 3` Positional arguments can be included with `p_argN` where `N` is any integer: * `p_arg1 = 3` is converted to `3` \"\"\" super().__init__(params=params) self._cmd = self._task_parameters.executable self._args_list: List[str] = [self._cmd] self._template_context: Dict[str, Any] = {}","title":"task"},{"location":"source/tasks/task/#tasks.task.DescribedAnalysis","text":"Complete analysis description. Held by an Executor. Source code in lute/tasks/dataclasses.py @dataclass class DescribedAnalysis: \"\"\"Complete analysis description. Held by an Executor.\"\"\" task_result: TaskResult task_parameters: Optional[TaskParameters] task_env: Dict[str, str] poll_interval: float communicator_desc: List[str]","title":"DescribedAnalysis"},{"location":"source/tasks/task/#tasks.task.ElogSummaryPlots","text":"Holds a graphical summary intended for display in the eLog. Attributes: display_name ( str ) \u2013 This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. display_name = \"scans/my_motor_scan\" will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. Source code in lute/tasks/dataclasses.py @dataclass class ElogSummaryPlots: \"\"\"Holds a graphical summary intended for display in the eLog. Attributes: display_name (str): This represents both a path and how the result will be displayed in the eLog. Can include \"/\" characters. E.g. `display_name = \"scans/my_motor_scan\"` will have plots shown on a \"my_motor_scan\" page, under a \"scans\" tab. This format mirrors how the file is stored on disk as well. \"\"\" display_name: str figures: Union[pn.Tabs, hv.Image, plt.Figure]","title":"ElogSummaryPlots"},{"location":"source/tasks/task/#tasks.task.Task","text":"Bases: ABC Abstract base class for analysis tasks. Attributes: name ( str ) \u2013 The name of the Task. Source code in lute/tasks/task.py class Task(ABC): \"\"\"Abstract base class for analysis tasks. Attributes: name (str): The name of the Task. \"\"\" def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. \"\"\" self.name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] self._result: TaskResult = TaskResult( task_name=self.name, task_status=TaskStatus.PENDING, summary=\"PENDING\", payload=\"\", ) self._task_parameters: TaskParameters = params timeout: int = self._task_parameters.lute_config.task_timeout signal.setitimer(signal.ITIMER_REAL, timeout) run_directory: Optional[str] = self._task_parameters.Config.run_directory if run_directory is not None: try: os.chdir(run_directory) except FileNotFoundError: warnings.warn( ( f\"Attempt to change to {run_directory}, but it is not found!\\n\" f\"Will attempt to run from {os.getcwd()}. It may fail!\" ), category=UserWarning, ) def run(self) -> None: \"\"\"Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. \"\"\" self._signal_start() self._pre_run() self._run() self._post_run() self._signal_result() @abstractmethod def _run(self) -> None: \"\"\"Actual analysis to run. Overridden by subclasses. Separating the calling API from the implementation allows `run` to have pre and post task functionality embedded easily into a single function call. \"\"\" ... def _pre_run(self) -> None: \"\"\"Code to run BEFORE the main analysis takes place. This function may, or may not, be employed by subclasses. \"\"\" ... def _post_run(self) -> None: \"\"\"Code to run AFTER the main analysis takes place. This function may, or may not, be employed by subclasses. \"\"\" ... @property def result(self) -> TaskResult: \"\"\"TaskResult: Read-only Task Result information.\"\"\" return self._result def __call__(self) -> None: self.run() def _signal_start(self) -> None: \"\"\"Send the signal that the Task will begin shortly.\"\"\" start_msg: Message = Message( contents=self._task_parameters, signal=\"TASK_STARTED\" ) self._result.task_status = TaskStatus.RUNNING self._report_to_executor(start_msg) def _signal_result(self) -> None: \"\"\"Send the signal that results are ready along with the results.\"\"\" signal: str = \"TASK_RESULT\" results_msg: Message = Message(contents=self.result, signal=signal) self._report_to_executor(results_msg) time.sleep(0.1) def _report_to_executor(self, msg: Message) -> None: \"\"\"Send a message to the Executor. Details of `Communicator` choice are hidden from the caller. This method may be overriden by subclasses with specialized functionality. Args: msg (Message): The message object to send. \"\"\" communicator: Communicator if isinstance(msg.contents, str) or msg.contents is None: communicator = PipeCommunicator() else: communicator = SocketCommunicator() communicator.write(msg) def clean_up_timeout(self) -> None: \"\"\"Perform any necessary cleanup actions before exit if timing out.\"\"\" ...","title":"Task"},{"location":"source/tasks/task/#tasks.task.Task.result","text":"TaskResult: Read-only Task Result information.","title":"result"},{"location":"source/tasks/task/#tasks.task.Task.__init__","text":"Initialize a Task. Parameters: params ( TaskParameters ) \u2013 Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. Source code in lute/tasks/task.py def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. These are NOT related to execution parameters (number of cores, etc), except, potentially, in case of binary executable sub-classes. \"\"\" self.name: str = str(type(self)).split(\"'\")[1].split(\".\")[-1] self._result: TaskResult = TaskResult( task_name=self.name, task_status=TaskStatus.PENDING, summary=\"PENDING\", payload=\"\", ) self._task_parameters: TaskParameters = params timeout: int = self._task_parameters.lute_config.task_timeout signal.setitimer(signal.ITIMER_REAL, timeout) run_directory: Optional[str] = self._task_parameters.Config.run_directory if run_directory is not None: try: os.chdir(run_directory) except FileNotFoundError: warnings.warn( ( f\"Attempt to change to {run_directory}, but it is not found!\\n\" f\"Will attempt to run from {os.getcwd()}. It may fail!\" ), category=UserWarning, )","title":"__init__"},{"location":"source/tasks/task/#tasks.task.Task.clean_up_timeout","text":"Perform any necessary cleanup actions before exit if timing out. Source code in lute/tasks/task.py def clean_up_timeout(self) -> None: \"\"\"Perform any necessary cleanup actions before exit if timing out.\"\"\" ...","title":"clean_up_timeout"},{"location":"source/tasks/task/#tasks.task.Task.run","text":"Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. Source code in lute/tasks/task.py def run(self) -> None: \"\"\"Calls the analysis routines and any pre/post task functions. This method is part of the public API and should not need to be modified in any subclasses. \"\"\" self._signal_start() self._pre_run() self._run() self._post_run() self._signal_result()","title":"run"},{"location":"source/tasks/task/#tasks.task.TaskResult","text":"Class for storing the result of a Task's execution with metadata. Attributes: task_name ( str ) \u2013 Name of the associated task which produced it. task_status ( TaskStatus ) \u2013 Status of associated task. summary ( str ) \u2013 Short message/summary associated with the result. payload ( Any ) \u2013 Actual result. May be data in any format. impl_schemas ( Optional [ str ] ) \u2013 A string listing Task schemas implemented by the associated Task . Schemas define the category and expected output of the Task . An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" Source code in lute/tasks/dataclasses.py @dataclass class TaskResult: \"\"\"Class for storing the result of a Task's execution with metadata. Attributes: task_name (str): Name of the associated task which produced it. task_status (TaskStatus): Status of associated task. summary (str): Short message/summary associated with the result. payload (Any): Actual result. May be data in any format. impl_schemas (Optional[str]): A string listing `Task` schemas implemented by the associated `Task`. Schemas define the category and expected output of the `Task`. An individual task may implement/conform to multiple schemas. Multiple schemas are separated by ';', e.g. * impl_schemas = \"schema1;schema2\" \"\"\" task_name: str task_status: TaskStatus summary: str payload: Any impl_schemas: Optional[str] = None","title":"TaskResult"},{"location":"source/tasks/task/#tasks.task.TaskStatus","text":"Bases: Enum Possible Task statuses. Source code in lute/tasks/dataclasses.py class TaskStatus(Enum): \"\"\"Possible Task statuses.\"\"\" PENDING = 0 \"\"\" Task has yet to run. Is Queued, or waiting for prior tasks. \"\"\" RUNNING = 1 \"\"\" Task is in the process of execution. \"\"\" COMPLETED = 2 \"\"\" Task has completed without fatal errors. \"\"\" FAILED = 3 \"\"\" Task encountered a fatal error. \"\"\" STOPPED = 4 \"\"\" Task was, potentially temporarily, stopped/suspended. \"\"\" CANCELLED = 5 \"\"\" Task was cancelled prior to completion or failure. \"\"\" TIMEDOUT = 6 \"\"\" Task did not reach completion due to timeout. \"\"\"","title":"TaskStatus"},{"location":"source/tasks/task/#tasks.task.TaskStatus.CANCELLED","text":"Task was cancelled prior to completion or failure.","title":"CANCELLED"},{"location":"source/tasks/task/#tasks.task.TaskStatus.COMPLETED","text":"Task has completed without fatal errors.","title":"COMPLETED"},{"location":"source/tasks/task/#tasks.task.TaskStatus.FAILED","text":"Task encountered a fatal error.","title":"FAILED"},{"location":"source/tasks/task/#tasks.task.TaskStatus.PENDING","text":"Task has yet to run. Is Queued, or waiting for prior tasks.","title":"PENDING"},{"location":"source/tasks/task/#tasks.task.TaskStatus.RUNNING","text":"Task is in the process of execution.","title":"RUNNING"},{"location":"source/tasks/task/#tasks.task.TaskStatus.STOPPED","text":"Task was, potentially temporarily, stopped/suspended.","title":"STOPPED"},{"location":"source/tasks/task/#tasks.task.TaskStatus.TIMEDOUT","text":"Task did not reach completion due to timeout.","title":"TIMEDOUT"},{"location":"source/tasks/task/#tasks.task.ThirdPartyTask","text":"Bases: Task A Task interface to analysis with binary executables. Source code in lute/tasks/task.py class ThirdPartyTask(Task): \"\"\"A `Task` interface to analysis with binary executables.\"\"\" def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. `Task`s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter `executable`. All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * `arg_name = 3` is converted to `--arg_name 3` Positional arguments can be included with `p_argN` where `N` is any integer: * `p_arg1 = 3` is converted to `3` \"\"\" super().__init__(params=params) self._cmd = self._task_parameters.executable self._args_list: List[str] = [self._cmd] self._template_context: Dict[str, Any] = {} def _add_to_jinja_context(self, param_name: str, value: Any) -> None: \"\"\"Store a parameter as a Jinja template variable. Variables are stored in a dictionary which is used to fill in a premade Jinja template for a third party configuration file. Args: param_name (str): Name to store the variable as. This should be the name defined in the corresponding pydantic model. This name MUST match the name used in the Jinja Template! value (Any): The value to store. If possible, large chunks of the template should be represented as a single dictionary for simplicity; however, any type can be stored as needed. \"\"\" context_update: Dict[str, Any] = {param_name: value} if __debug__: msg: Message = Message(contents=f\"TemplateParameters: {context_update}\") self._report_to_executor(msg) self._template_context.update(context_update) def _template_to_config_file(self) -> None: \"\"\"Convert a template file into a valid configuration file. Uses Jinja to fill in a provided template file with variables supplied through the LUTE config file. This facilitates parameter modification for third party tasks which use a separate configuration, in addition to, or instead of, command-line arguments. \"\"\" from jinja2 import Environment, FileSystemLoader, Template out_file: str = self._task_parameters.lute_template_cfg.output_path template_name: str = self._task_parameters.lute_template_cfg.template_name lute_path: Optional[str] = os.getenv(\"LUTE_PATH\") template_dir: str if lute_path is None: warnings.warn( \"LUTE_PATH is None in Task process! Using relative path for templates!\", category=UserWarning, ) template_dir: str = \"../../config/templates\" else: template_dir = f\"{lute_path}/config/templates\" environment: Environment = Environment(loader=FileSystemLoader(template_dir)) template: Template = environment.get_template(template_name) with open(out_file, \"w\", encoding=\"utf-8\") as cfg_out: cfg_out.write(template.render(self._template_context)) def _pre_run(self) -> None: \"\"\"Parse the parameters into an appropriate argument list. Arguments are identified by a `flag_type` attribute, defined in the pydantic model, which indicates how to pass the parameter and its argument on the command-line. This method parses flag:value pairs into an appropriate list to be used to call the executable. Note: ThirdPartyParameter objects are returned by custom model validators. Objects of this type are assumed to be used for a templated config file used by the third party executable for configuration. The parsing of these parameters is performed separately by a template file used as an input to Jinja. This method solely identifies the necessary objects and passes them all along. Refer to the template files and pydantic models for more information on how these parameters are defined and identified. \"\"\" super()._pre_run() full_schema: Dict[str, Union[str, Dict[str, Any]]] = ( self._task_parameters.schema() ) short_flags_use_eq: bool long_flags_use_eq: bool if hasattr(self._task_parameters.Config, \"short_flags_use_eq\"): short_flags_use_eq: bool = self._task_parameters.Config.short_flags_use_eq long_flags_use_eq: bool = self._task_parameters.Config.long_flags_use_eq else: short_flags_use_eq = False long_flags_use_eq = False for param, value in self._task_parameters.dict().items(): # Clunky test with __dict__[param] because compound model-types are # converted to `dict`. E.g. type(value) = dict not AnalysisHeader if ( param == \"executable\" or value is None # Cannot have empty values in argument list for execvp or value == \"\" # But do want to include, e.g. 0 or isinstance(self._task_parameters.__dict__[param], TemplateConfig) or isinstance(self._task_parameters.__dict__[param], AnalysisHeader) ): continue if isinstance(self._task_parameters.__dict__[param], TemplateParameters): # TemplateParameters objects have a single parameter `params` self._add_to_jinja_context(param_name=param, value=value.params) continue param_attributes: Dict[str, Any] = full_schema[\"properties\"][param] # Some model params do not match the commnad-line parameter names param_repr: str if \"rename_param\" in param_attributes: param_repr = param_attributes[\"rename_param\"] else: param_repr = param if \"flag_type\" in param_attributes: flag: str = param_attributes[\"flag_type\"] if flag: # \"-\" or \"--\" flags if flag == \"--\" and isinstance(value, bool) and not value: continue constructed_flag: str = f\"{flag}{param_repr}\" if flag == \"--\" and isinstance(value, bool) and value: # On/off flag, e.g. something like --verbose: No Arg self._args_list.append(f\"{constructed_flag}\") continue if (flag == \"-\" and short_flags_use_eq) or ( flag == \"--\" and long_flags_use_eq ): # Must come after above check! Otherwise you get --param=True # Flags following --param=value or -param=value constructed_flag = f\"{constructed_flag}={value}\" self._args_list.append(f\"{constructed_flag}\") continue self._args_list.append(f\"{constructed_flag}\") else: warnings.warn( ( f\"Model parameters should be defined using Field(...,flag_type='')\" f\" in the future. Parameter: {param}\" ), category=PendingDeprecationWarning, ) if len(param) == 1: # Single-dash flags if short_flags_use_eq: self._args_list.append(f\"-{param_repr}={value}\") continue self._args_list.append(f\"-{param_repr}\") elif \"p_arg\" in param: # Positional arguments pass else: # Double-dash flags if isinstance(value, bool) and not value: continue if long_flags_use_eq: self._args_list.append(f\"--{param_repr}={value}\") continue self._args_list.append(f\"--{param_repr}\") if isinstance(value, bool) and value: continue if isinstance(value, str) and \" \" in value: for val in value.split(): self._args_list.append(f\"{val}\") else: self._args_list.append(f\"{value}\") if ( hasattr(self._task_parameters, \"lute_template_cfg\") and self._template_context ): self._template_to_config_file() def _run(self) -> None: \"\"\"Execute the new program by replacing the current process.\"\"\" if __debug__: time.sleep(0.1) msg: Message = Message(contents=self._formatted_command()) self._report_to_executor(msg) LUTE_DEBUG_EXIT(\"LUTE_DEBUG_BEFORE_TPP_EXEC\") os.execvp(file=self._cmd, args=self._args_list) def _formatted_command(self) -> str: \"\"\"Returns the command as it would passed on the command-line.\"\"\" formatted_cmd: str = \"\".join(f\"{arg} \" for arg in self._args_list) return formatted_cmd def _signal_start(self) -> None: \"\"\"Override start signal method to switch communication methods.\"\"\" super()._signal_start() time.sleep(0.05) signal: str = \"NO_PICKLE_MODE\" msg: Message = Message(signal=signal) self._report_to_executor(msg)","title":"ThirdPartyTask"},{"location":"source/tasks/task/#tasks.task.ThirdPartyTask.__init__","text":"Initialize a Task. Parameters: params ( TaskParameters ) \u2013 Parameters needed to properly configure the analysis task. Task s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter executable . All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * arg_name = 3 is converted to --arg_name 3 Positional arguments can be included with p_argN where N is any integer: * p_arg1 = 3 is converted to 3 Source code in lute/tasks/task.py def __init__(self, *, params: TaskParameters) -> None: \"\"\"Initialize a Task. Args: params (TaskParameters): Parameters needed to properly configure the analysis task. `Task`s of this type MUST include the name of a binary to run and any arguments which should be passed to it (as would be done via command line). The binary is included with the parameter `executable`. All other parameter names are assumed to be the long/extended names of the flag passed on the command line: * `arg_name = 3` is converted to `--arg_name 3` Positional arguments can be included with `p_argN` where `N` is any integer: * `p_arg1 = 3` is converted to `3` \"\"\" super().__init__(params=params) self._cmd = self._task_parameters.executable self._args_list: List[str] = [self._cmd] self._template_context: Dict[str, Any] = {}","title":"__init__"},{"location":"source/tasks/test/","text":"Basic test Tasks for testing functionality. Classes: Name Description Test Simplest test Task - runs a 10 iteration loop and returns a result. TestSocket Test Task which sends larger data to test socket IPC. TestWriteOutput Test Task which writes an output file. TestReadOutput Test Task which reads in a file. Can be used to test database access. Test Bases: Task Simple test Task to ensure subprocess and pipe-based IPC work. Source code in lute/tasks/test.py class Test(Task): \"\"\"Simple test Task to ensure subprocess and pipe-based IPC work.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(10): time.sleep(1) msg: Message = Message(contents=f\"Test message {i}\") self._report_to_executor(msg) if self._task_parameters.throw_error: raise RuntimeError(\"Testing Error!\") def _post_run(self) -> None: self._result.summary = \"Test Finished.\" self._result.task_status = TaskStatus.COMPLETED time.sleep(0.1) TestReadOutput Bases: Task Simple test Task to read in output from the test Task above. Its pydantic model relies on a database access to retrieve the output file. Source code in lute/tasks/test.py class TestReadOutput(Task): \"\"\"Simple test Task to read in output from the test Task above. Its pydantic model relies on a database access to retrieve the output file. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: array: np.ndarray = np.loadtxt(self._task_parameters.in_file, delimiter=\",\") self._report_to_executor(msg=Message(contents=\"Successfully loaded data!\")) for i in range(5): time.sleep(1) def _post_run(self) -> None: super()._post_run() self._result.summary = \"Was able to load data.\" self._result.payload = \"This Task produces no output.\" self._result.task_status = TaskStatus.COMPLETED TestSocket Bases: Task Simple test Task to ensure basic IPC over Unix sockets works. Source code in lute/tasks/test.py class TestSocket(Task): \"\"\"Simple test Task to ensure basic IPC over Unix sockets works.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(self._task_parameters.num_arrays): msg: Message = Message(contents=f\"Sending array {i}\") self._report_to_executor(msg) time.sleep(0.05) msg: Message = Message( contents=np.random.rand(self._task_parameters.array_size) ) self._report_to_executor(msg) def _post_run(self) -> None: super()._post_run() self._result.summary = f\"Sent {self._task_parameters.num_arrays} arrays\" self._result.payload = np.random.rand(self._task_parameters.array_size) self._result.task_status = TaskStatus.COMPLETED TestWriteOutput Bases: Task Simple test Task to write output other Tasks depend on. Source code in lute/tasks/test.py class TestWriteOutput(Task): \"\"\"Simple test Task to write output other Tasks depend on.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(self._task_parameters.num_vals): # Doing some calculations... time.sleep(0.05) if i % 10 == 0: msg: Message = Message(contents=f\"Processed {i+1} values!\") self._report_to_executor(msg) def _post_run(self) -> None: super()._post_run() work_dir: str = self._task_parameters.lute_config.work_dir out_file: str = f\"{work_dir}/{self._task_parameters.outfile_name}\" array: np.ndarray = np.random.rand(self._task_parameters.num_vals) np.savetxt(out_file, array, delimiter=\",\") self._result.summary = \"Completed task successfully.\" self._result.payload = out_file self._result.task_status = TaskStatus.COMPLETED","title":"test"},{"location":"source/tasks/test/#tasks.test.Test","text":"Bases: Task Simple test Task to ensure subprocess and pipe-based IPC work. Source code in lute/tasks/test.py class Test(Task): \"\"\"Simple test Task to ensure subprocess and pipe-based IPC work.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(10): time.sleep(1) msg: Message = Message(contents=f\"Test message {i}\") self._report_to_executor(msg) if self._task_parameters.throw_error: raise RuntimeError(\"Testing Error!\") def _post_run(self) -> None: self._result.summary = \"Test Finished.\" self._result.task_status = TaskStatus.COMPLETED time.sleep(0.1)","title":"Test"},{"location":"source/tasks/test/#tasks.test.TestReadOutput","text":"Bases: Task Simple test Task to read in output from the test Task above. Its pydantic model relies on a database access to retrieve the output file. Source code in lute/tasks/test.py class TestReadOutput(Task): \"\"\"Simple test Task to read in output from the test Task above. Its pydantic model relies on a database access to retrieve the output file. \"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: array: np.ndarray = np.loadtxt(self._task_parameters.in_file, delimiter=\",\") self._report_to_executor(msg=Message(contents=\"Successfully loaded data!\")) for i in range(5): time.sleep(1) def _post_run(self) -> None: super()._post_run() self._result.summary = \"Was able to load data.\" self._result.payload = \"This Task produces no output.\" self._result.task_status = TaskStatus.COMPLETED","title":"TestReadOutput"},{"location":"source/tasks/test/#tasks.test.TestSocket","text":"Bases: Task Simple test Task to ensure basic IPC over Unix sockets works. Source code in lute/tasks/test.py class TestSocket(Task): \"\"\"Simple test Task to ensure basic IPC over Unix sockets works.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(self._task_parameters.num_arrays): msg: Message = Message(contents=f\"Sending array {i}\") self._report_to_executor(msg) time.sleep(0.05) msg: Message = Message( contents=np.random.rand(self._task_parameters.array_size) ) self._report_to_executor(msg) def _post_run(self) -> None: super()._post_run() self._result.summary = f\"Sent {self._task_parameters.num_arrays} arrays\" self._result.payload = np.random.rand(self._task_parameters.array_size) self._result.task_status = TaskStatus.COMPLETED","title":"TestSocket"},{"location":"source/tasks/test/#tasks.test.TestWriteOutput","text":"Bases: Task Simple test Task to write output other Tasks depend on. Source code in lute/tasks/test.py class TestWriteOutput(Task): \"\"\"Simple test Task to write output other Tasks depend on.\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) def _run(self) -> None: for i in range(self._task_parameters.num_vals): # Doing some calculations... time.sleep(0.05) if i % 10 == 0: msg: Message = Message(contents=f\"Processed {i+1} values!\") self._report_to_executor(msg) def _post_run(self) -> None: super()._post_run() work_dir: str = self._task_parameters.lute_config.work_dir out_file: str = f\"{work_dir}/{self._task_parameters.outfile_name}\" array: np.ndarray = np.random.rand(self._task_parameters.num_vals) np.savetxt(out_file, array, delimiter=\",\") self._result.summary = \"Completed task successfully.\" self._result.payload = out_file self._result.task_status = TaskStatus.COMPLETED","title":"TestWriteOutput"},{"location":"tutorial/creating_workflows/","text":"Workflows with Airflow Note: Airflow uses the term DAG , or directed acyclic graph, to describe workflows of tasks with defined (and acyclic) connectivities. This page will use the terms workflow and DAG interchangeably. Relevant Components In addition to the core LUTE package, a number of components are generally involved to run a workflow. The current set of scripts and objects are used to interface with Airflow, and the SLURM job scheduler. The core LUTE library can also be used to run workflows using different backends, and in the future these may be supported. For building and running workflows using SLURM and Airflow, the following components are necessary, and will be described in more detail below: - Airflow launch script: launch_airflow.py - This has a wrapper batch submission script: submit_launch_airflow.sh . When running using the ARP (from the eLog), you MUST use this wrapper script instead of the Python script directly. - SLURM submission script: submit_slurm.sh - Airflow operators: - JIDSlurmOperator Launch/Submission Scripts launch_airflow.py Sends a request to an Airflow instance to submit a specific DAG (workflow). This script prepares an HTTP request with the appropriate parameters in a specific format. A request involves the following information, most of which is retrieved automatically: dag_run_data: Dict[str, Union[str, Dict[str, Union[str, int, List[str]]]]] = { \"dag_run_id\": str(uuid.uuid4()), \"conf\": { \"experiment\": os.environ.get(\"EXPERIMENT\"), \"run_id\": f\"{os.environ.get('RUN_NUM')}{datetime.datetime.utcnow().isoformat()}\", \"JID_UPDATE_COUNTERS\": os.environ.get(\"JID_UPDATE_COUNTERS\"), \"ARP_ROOT_JOB_ID\": os.environ.get(\"ARP_JOB_ID\"), \"ARP_LOCATION\": os.environ.get(\"ARP_LOCATION\", \"S3DF\"), \"Authorization\": os.environ.get(\"Authorization\"), \"user\": getpass.getuser(), \"lute_params\": params, \"slurm_params\": extra_args, }, } Note that the environment variables are used to fill in the appropriate information because this script is intended to be launched primarily from the ARP (which passes these variables). The ARP allows for the launch job to be defined in the experiment eLog and submitted automatically for each new DAQ run. The environment variables EXPERIMENT and RUN can alternatively be defined prior to submitting the script on the command-line. The script takes a number of parameters: launch_airflow.py -c <path_to_config_yaml> -w <workflow_name> [--debug] [--test] -c refers to the path of the configuration YAML that contains the parameters for each managed Task in the requested workflow. -w is the name of the DAG (workflow) to run. By convention each DAG is named by the Python file it is defined in. (See below). --debug is an optional flag to run all steps of the workflow in debug mode for verbose logging and output. --test is an optional flag which will use the test Airflow instance. By default the script will make requests of the standard production Airflow instance. Lifetime This script will run for the entire duration of the workflow (DAG) . After making the initial request of Airflow to launch the DAG, it will enter a status update loop which will keep track of each individual job (each job runs one managed Task ) submitted by Airflow. At the end of each job it will collect the log file, in addition to providing a few other status updates/debugging messages, and append it to its own log. This allows all logging for the entire workflow (DAG) to be inspected from an individual file. This is particularly useful when running via the eLog, because only a single log file is displayed. submit_launch_airflow.sh This script is only necessary when running from the eLog using the ARP. The initial job submitted by the ARP can not have a duration of longer than 30 seconds, as it will then time out. As the launch_airflow.py job will live for the entire duration of the workflow, which is often much longer than 30 seconds, the solution was to have a wrapper which submits the launch_airflow.py script to run on the S3DF batch nodes. Usage of this script is identical to launch_airflow.py . All the arguments are passed transparently to the underlying Python script. The wrapper will simply launch a batch job using minimal resources (1 core). submit_slurm.sh Launches a job on the S3DF batch nodes using the SLURM job scheduler. This script launches a single managed Task at a time. The usage is as follows: submit_slurm.sh -c <path_to_config_yaml> -t <MANAGED_task_name> [--debug] [--SLURM_ARGS ...] As a reminder the managed Task refers to the Executor - Task combination. The script does not parse any SLURM specific parameters, and instead passes them transparently to SLURM. At least the following two SLURM arguments must be provided: --partition=<...> # Usually partition=milano --account=<...> # Usually account=lcls:$EXPERIMENT Generally, resource requests will also be included, such as the number of cores to use. A complete call may look like the following: submit_slurm.sh -c /sdf/data/lcls/ds/hutch/experiment/scratch/config.yaml -t Tester --partition=milano --account=lcls:experiment --ntasks=100 [...] When running a workflow using the launch_airflow.py script, each step of the workflow will be submitted using this script. Operators Operator s are the objects submitted as individual steps of a DAG by Airflow. They are conceptually linked to the idea of a task in that each task of a workflow is generally an operator. Care should be taken, not to confuse them with LUTE Task s or managed Task s though. There is, however, usually a one-to-one correspondance between a Task and an Operator . Airflow runs on a K8S cluster which has no access to the experiment data. When we ask Airflow to run a DAG, it will launch an Operator for each step of the DAG. However, the Operator itself cannot perform productive analysis without access to the data. The solution employed by LUTE is to have a limited set of Operator s which do not perform analysis, but instead request that a LUTE managed Task s be submitted on the batch nodes where it can access the data. There may be small differences between how the various provided Operator s do this, but in general they will all make a request to the job interface daemon (JID) that a new SLURM job be scheduled using the submit_slurm.sh script described above. Therefore, running a typical Airflow DAG involves the following steps: 1. launch_airflow.py script is submitted, usually from a definition in the eLog. 2. The launch_airflow script requests that Airflow run a specific DAG. 3. The Airflow instance begins submitting the Operator s that makeup the DAG definition. 4. Each Operator sends a request to the JID to submit a job. 5. The JID submits the elog_submit.sh script with the appropriate managed Task . 6. The managed Task runs on the batch nodes, while the Operator , requesting updates from the JID on job status, waits for it to complete. 7. Once a managed Task completes, the Operator will receieve this information and tell the Airflow server whether the job completed successfully or resulted in failure. 8. The Airflow server will then launch the next step of the DAG, and so on, until every step has been executed. Currently, the following Operator s are maintained: - JIDSlurmOperator : The standard Operator . Each instance has a one-to-one correspondance with a LUTE managed Task . JIDSlurmOperator arguments task_id : This is nominally the name of the task on the Airflow side. However, for simplicity this is used 1-1 to match the name of a managed Task defined in LUTE's managed_tasks.py module. I.e., it should the name of an Executor(\"Task\") object which will run the specific Task of interest. This must match the name of a defined managed Task. max_cores : Used to cap the maximum number of cores which should be requested of SLURM. By default all jobs will run with the same number of cores, which should be specified when running the launch_airflow.py script (either from the ARP, or by hand). This behaviour was chosen because in general we want to increase or decrease the core-count for all Task s uniformly, and we don't want to have to specify core number arguments for each job individually. Nonetheless, on occassion it may be necessary to cap the number of cores a specific job will use. E.g. if the default value specified when launching the Airflow DAG is multiple cores, and one job is single threaded, the core count can be capped for that single job to 1, while the rest run with multiple cores. max_nodes : Similar to the above. This will make sure the Task is distributed across no more than a maximum number of nodes. This feature is useful for, e.g., multi-threaded software which does not make use of tools like MPI . So, the Task can run on multiple cores, but only within a single node. Creating a new workflow Defining a new workflow involves creating a new module (Python file) in the directory workflows/airflow , creating a number of Operator instances within the module, and then drawing the connectivity between them. At the top of the file an Airflow DAG is created and given a name. By convention all LUTE workflows use the name of the file as the name of the DAG. The following code can be copied exactly into the file: from datetime import datetime import os from airflow import DAG from lute.operators.jidoperators import JIDSlurmOperator # Import other operators if needed dag_id: str = f\"lute_{os.path.splitext(os.path.basename(__file__))[0]}\" description: str = ( \"Run SFX processing using PyAlgos peak finding and experimental phasing\" ) dag: DAG = DAG( dag_id=dag_id, start_date=datetime(2024, 3, 18), schedule_interval=None, description=description, ) Once the DAG has been created, a number of Operator s must be created to run the various LUTE analysis operations. As an example consider a partial SFX processing workflow which includes steps for peak finding, indexing, merging, and calculating figures of merit. Each of the 4 steps will have an Operator instance which will launch a corresponding LUTE managed Task , for example: # Using only the JIDSlurmOperator # syntax: JIDSlurmOperator(task_id=\"LuteManagedTaskName\", dag=dag) # optionally, max_cores=123) peak_finder: JIDSlurmOperator = JIDSlurmOperator(task_id=\"PeakFinderPyAlgos\", dag=dag) # We specify a maximum number of cores for the rest of the jobs. indexer: JIDSlurmOperator = JIDSlurmOperator( max_cores=120, task_id=\"CrystFELIndexer\", dag=dag ) # Merge merger: JIDSlurmOperator = JIDSlurmOperator( max_cores=120, task_id=\"PartialatorMerger\", dag=dag ) # Figures of merit hkl_comparer: JIDSlurmOperator = JIDSlurmOperator( max_cores=8, task_id=\"HKLComparer\", dag=dag ) Finally, the dependencies between the Operator s are \"drawn\", defining the execution order of the various steps. The >> operator has been overloaded for the Operator class, allowing it to be used to specify the next step in the DAG. In this case, a completely linear DAG is drawn as: peak_finder >> indexer >> merger >> hkl_comparer Parallel execution can be added by using the >> operator multiple times. Consider a task1 which upon successful completion starts a task2 and task3 in parallel. This dependency can be added to the DAG using: #task1: JIDSlurmOperator = JIDSlurmOperator(...) #task2 ... task1 >> task2 task1 >> task3 As each DAG is defined in pure Python, standard control structures (loops, if statements, etc.) can be used to create more complex workflow arrangements.","title":"Creating a new Workflow"},{"location":"tutorial/creating_workflows/#workflows-with-airflow","text":"Note: Airflow uses the term DAG , or directed acyclic graph, to describe workflows of tasks with defined (and acyclic) connectivities. This page will use the terms workflow and DAG interchangeably.","title":"Workflows with Airflow"},{"location":"tutorial/creating_workflows/#relevant-components","text":"In addition to the core LUTE package, a number of components are generally involved to run a workflow. The current set of scripts and objects are used to interface with Airflow, and the SLURM job scheduler. The core LUTE library can also be used to run workflows using different backends, and in the future these may be supported. For building and running workflows using SLURM and Airflow, the following components are necessary, and will be described in more detail below: - Airflow launch script: launch_airflow.py - This has a wrapper batch submission script: submit_launch_airflow.sh . When running using the ARP (from the eLog), you MUST use this wrapper script instead of the Python script directly. - SLURM submission script: submit_slurm.sh - Airflow operators: - JIDSlurmOperator","title":"Relevant Components"},{"location":"tutorial/creating_workflows/#launchsubmission-scripts","text":"","title":"Launch/Submission Scripts"},{"location":"tutorial/creating_workflows/#launch_airflowpy","text":"Sends a request to an Airflow instance to submit a specific DAG (workflow). This script prepares an HTTP request with the appropriate parameters in a specific format. A request involves the following information, most of which is retrieved automatically: dag_run_data: Dict[str, Union[str, Dict[str, Union[str, int, List[str]]]]] = { \"dag_run_id\": str(uuid.uuid4()), \"conf\": { \"experiment\": os.environ.get(\"EXPERIMENT\"), \"run_id\": f\"{os.environ.get('RUN_NUM')}{datetime.datetime.utcnow().isoformat()}\", \"JID_UPDATE_COUNTERS\": os.environ.get(\"JID_UPDATE_COUNTERS\"), \"ARP_ROOT_JOB_ID\": os.environ.get(\"ARP_JOB_ID\"), \"ARP_LOCATION\": os.environ.get(\"ARP_LOCATION\", \"S3DF\"), \"Authorization\": os.environ.get(\"Authorization\"), \"user\": getpass.getuser(), \"lute_params\": params, \"slurm_params\": extra_args, }, } Note that the environment variables are used to fill in the appropriate information because this script is intended to be launched primarily from the ARP (which passes these variables). The ARP allows for the launch job to be defined in the experiment eLog and submitted automatically for each new DAQ run. The environment variables EXPERIMENT and RUN can alternatively be defined prior to submitting the script on the command-line. The script takes a number of parameters: launch_airflow.py -c <path_to_config_yaml> -w <workflow_name> [--debug] [--test] -c refers to the path of the configuration YAML that contains the parameters for each managed Task in the requested workflow. -w is the name of the DAG (workflow) to run. By convention each DAG is named by the Python file it is defined in. (See below). --debug is an optional flag to run all steps of the workflow in debug mode for verbose logging and output. --test is an optional flag which will use the test Airflow instance. By default the script will make requests of the standard production Airflow instance. Lifetime This script will run for the entire duration of the workflow (DAG) . After making the initial request of Airflow to launch the DAG, it will enter a status update loop which will keep track of each individual job (each job runs one managed Task ) submitted by Airflow. At the end of each job it will collect the log file, in addition to providing a few other status updates/debugging messages, and append it to its own log. This allows all logging for the entire workflow (DAG) to be inspected from an individual file. This is particularly useful when running via the eLog, because only a single log file is displayed.","title":"launch_airflow.py"},{"location":"tutorial/creating_workflows/#submit_launch_airflowsh","text":"This script is only necessary when running from the eLog using the ARP. The initial job submitted by the ARP can not have a duration of longer than 30 seconds, as it will then time out. As the launch_airflow.py job will live for the entire duration of the workflow, which is often much longer than 30 seconds, the solution was to have a wrapper which submits the launch_airflow.py script to run on the S3DF batch nodes. Usage of this script is identical to launch_airflow.py . All the arguments are passed transparently to the underlying Python script. The wrapper will simply launch a batch job using minimal resources (1 core).","title":"submit_launch_airflow.sh"},{"location":"tutorial/creating_workflows/#submit_slurmsh","text":"Launches a job on the S3DF batch nodes using the SLURM job scheduler. This script launches a single managed Task at a time. The usage is as follows: submit_slurm.sh -c <path_to_config_yaml> -t <MANAGED_task_name> [--debug] [--SLURM_ARGS ...] As a reminder the managed Task refers to the Executor - Task combination. The script does not parse any SLURM specific parameters, and instead passes them transparently to SLURM. At least the following two SLURM arguments must be provided: --partition=<...> # Usually partition=milano --account=<...> # Usually account=lcls:$EXPERIMENT Generally, resource requests will also be included, such as the number of cores to use. A complete call may look like the following: submit_slurm.sh -c /sdf/data/lcls/ds/hutch/experiment/scratch/config.yaml -t Tester --partition=milano --account=lcls:experiment --ntasks=100 [...] When running a workflow using the launch_airflow.py script, each step of the workflow will be submitted using this script.","title":"submit_slurm.sh"},{"location":"tutorial/creating_workflows/#operators","text":"Operator s are the objects submitted as individual steps of a DAG by Airflow. They are conceptually linked to the idea of a task in that each task of a workflow is generally an operator. Care should be taken, not to confuse them with LUTE Task s or managed Task s though. There is, however, usually a one-to-one correspondance between a Task and an Operator . Airflow runs on a K8S cluster which has no access to the experiment data. When we ask Airflow to run a DAG, it will launch an Operator for each step of the DAG. However, the Operator itself cannot perform productive analysis without access to the data. The solution employed by LUTE is to have a limited set of Operator s which do not perform analysis, but instead request that a LUTE managed Task s be submitted on the batch nodes where it can access the data. There may be small differences between how the various provided Operator s do this, but in general they will all make a request to the job interface daemon (JID) that a new SLURM job be scheduled using the submit_slurm.sh script described above. Therefore, running a typical Airflow DAG involves the following steps: 1. launch_airflow.py script is submitted, usually from a definition in the eLog. 2. The launch_airflow script requests that Airflow run a specific DAG. 3. The Airflow instance begins submitting the Operator s that makeup the DAG definition. 4. Each Operator sends a request to the JID to submit a job. 5. The JID submits the elog_submit.sh script with the appropriate managed Task . 6. The managed Task runs on the batch nodes, while the Operator , requesting updates from the JID on job status, waits for it to complete. 7. Once a managed Task completes, the Operator will receieve this information and tell the Airflow server whether the job completed successfully or resulted in failure. 8. The Airflow server will then launch the next step of the DAG, and so on, until every step has been executed. Currently, the following Operator s are maintained: - JIDSlurmOperator : The standard Operator . Each instance has a one-to-one correspondance with a LUTE managed Task .","title":"Operators"},{"location":"tutorial/creating_workflows/#jidslurmoperator-arguments","text":"task_id : This is nominally the name of the task on the Airflow side. However, for simplicity this is used 1-1 to match the name of a managed Task defined in LUTE's managed_tasks.py module. I.e., it should the name of an Executor(\"Task\") object which will run the specific Task of interest. This must match the name of a defined managed Task. max_cores : Used to cap the maximum number of cores which should be requested of SLURM. By default all jobs will run with the same number of cores, which should be specified when running the launch_airflow.py script (either from the ARP, or by hand). This behaviour was chosen because in general we want to increase or decrease the core-count for all Task s uniformly, and we don't want to have to specify core number arguments for each job individually. Nonetheless, on occassion it may be necessary to cap the number of cores a specific job will use. E.g. if the default value specified when launching the Airflow DAG is multiple cores, and one job is single threaded, the core count can be capped for that single job to 1, while the rest run with multiple cores. max_nodes : Similar to the above. This will make sure the Task is distributed across no more than a maximum number of nodes. This feature is useful for, e.g., multi-threaded software which does not make use of tools like MPI . So, the Task can run on multiple cores, but only within a single node.","title":"JIDSlurmOperator arguments"},{"location":"tutorial/creating_workflows/#creating-a-new-workflow","text":"Defining a new workflow involves creating a new module (Python file) in the directory workflows/airflow , creating a number of Operator instances within the module, and then drawing the connectivity between them. At the top of the file an Airflow DAG is created and given a name. By convention all LUTE workflows use the name of the file as the name of the DAG. The following code can be copied exactly into the file: from datetime import datetime import os from airflow import DAG from lute.operators.jidoperators import JIDSlurmOperator # Import other operators if needed dag_id: str = f\"lute_{os.path.splitext(os.path.basename(__file__))[0]}\" description: str = ( \"Run SFX processing using PyAlgos peak finding and experimental phasing\" ) dag: DAG = DAG( dag_id=dag_id, start_date=datetime(2024, 3, 18), schedule_interval=None, description=description, ) Once the DAG has been created, a number of Operator s must be created to run the various LUTE analysis operations. As an example consider a partial SFX processing workflow which includes steps for peak finding, indexing, merging, and calculating figures of merit. Each of the 4 steps will have an Operator instance which will launch a corresponding LUTE managed Task , for example: # Using only the JIDSlurmOperator # syntax: JIDSlurmOperator(task_id=\"LuteManagedTaskName\", dag=dag) # optionally, max_cores=123) peak_finder: JIDSlurmOperator = JIDSlurmOperator(task_id=\"PeakFinderPyAlgos\", dag=dag) # We specify a maximum number of cores for the rest of the jobs. indexer: JIDSlurmOperator = JIDSlurmOperator( max_cores=120, task_id=\"CrystFELIndexer\", dag=dag ) # Merge merger: JIDSlurmOperator = JIDSlurmOperator( max_cores=120, task_id=\"PartialatorMerger\", dag=dag ) # Figures of merit hkl_comparer: JIDSlurmOperator = JIDSlurmOperator( max_cores=8, task_id=\"HKLComparer\", dag=dag ) Finally, the dependencies between the Operator s are \"drawn\", defining the execution order of the various steps. The >> operator has been overloaded for the Operator class, allowing it to be used to specify the next step in the DAG. In this case, a completely linear DAG is drawn as: peak_finder >> indexer >> merger >> hkl_comparer Parallel execution can be added by using the >> operator multiple times. Consider a task1 which upon successful completion starts a task2 and task3 in parallel. This dependency can be added to the DAG using: #task1: JIDSlurmOperator = JIDSlurmOperator(...) #task2 ... task1 >> task2 task1 >> task3 As each DAG is defined in pure Python, standard control structures (loops, if statements, etc.) can be used to create more complex workflow arrangements.","title":"Creating a new workflow"},{"location":"tutorial/new_task/","text":"Integrating a New Task Task s can be broadly categorized into two types: - \"First-party\" - where the analysis or executed code is maintained within this library. - \"Third-party\" - where the analysis, code, or program is maintained elsewhere and is simply called by a wrapping Task . Creating a new Task of either type generally involves the same steps, although for first-party Task s, the analysis code must of course also be written. Due to this difference, as well as additional considerations for parameter handling when dealing with \"third-party\" Task s, the \"first-party\" and \"third-party\" Task integration cases will be considered separately. Creating a \"Third-party\" Task There are two required steps for third-party Task integration, and one additional step which is optional, and may not be applicable to all possible third-party Task s. Generally, Task integration requires: 1. Defining a TaskParameters (pydantic) model which fully parameterizes the Task . This involves specifying a path to a binary, and all the required command-line arguments to run the binary. 2. Creating a managed Task by specifying an Executor for the new third-party Task . At this stage, any additional environment variables can be added which are required for the execution environment. 3. (Optional/Maybe applicable) Create a template for a third-party configuration file. If the new Task has its own configuration file, specifying a template will allow that file to be parameterized from the singular LUTE yaml configuration file. A couple of minor additions to the pydantic model specified in 1. are required to support template usage. Each of these stages will be discussed in detail below. The vast majority of the work is completed in step 1. Specifying a TaskParameters Model for your Task A brief overview of parameters objects will be provided below. The following information goes into detail only about specifics related to LUTE configuration. An in depth description of pydantic is beyond the scope of this tutorial; please refer to the official documentation for more information. Please note that due to environment constraints pydantic is currently pinned to version 1.10! Make sure to read the appropriate documentation for this version as many things are different compared to the newer releases. At the end this document there will be an example highlighting some supported behaviour as well as a FAQ to address some common integration considerations. Task s and TaskParameter s All Task s have a corresponding TaskParameters object. These objects are linked exclusively by a named relationship. For a Task named MyThirdPartyTask , the parameters object must be named MyThirdPartyTaskParameters . For third-party Task s there are a number of additional requirements: - The model must inherit from a base class called ThirdPartyParameters . - The model must have one field specified called executable . The presence of this field indicates that the Task is a third-party Task and the specified executable must be called. This allows all third-party Task s to be defined exclusively by their parameters model. A single ThirdPartyTask class handles execution of all third-party Task s. All models are stored in lute/io/models . For any given Task , a new model can be added to an existing module contained in this directory or to a new module. If creating a new module, make sure to add an import statement to lute.io.models.__init__ . Defining TaskParameter s When specifying parameters the default behaviour is to provide a one-to-one correspondance between the Python attribute specified in the parameter model, and the parameter specified on the command-line. Single-letter attributes are assumed to be passed using - , e.g. n will be passed as -n when the executable is launched. Longer attributes are passed using -- , e.g. by default a model attribute named my_arg will be passed on the command-line as --my_arg . Positional arguments are specified using p_argX where X is a number. All parameters are passed in the order that they are specified in the model. However, because the number of possible command-line combinations is large, relying on the default behaviour above is NOT recommended . It is provided solely as a fallback. Instead, there are a number of configuration knobs which can be tuned to achieve the desired behaviour. The two main mechanisms for controlling behaviour are specification of model-wide configuration under the Config class within the model's definition, and parameter-by-parameter configuration using field attributes. For the latter, we define all parameters as Field objects. This allows parameters to have their own attributes, which are parsed by LUTE's task-layer. Given this, the preferred starting template for a TaskParameters model is the following - we assume we are integrating a new Task called RunTask : from pydantic import Field, validator # Also include any pydantic type specifications - Pydantic has many custom # validation types already, e.g. types for constrained numberic values, URL handling, etc. from .base import ThirdPartyParameters # Change class name as necessary class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for RunTask...\"\"\" class Config(ThirdPartyParameters.Config): # MUST be exactly as written here. ... # Model-wide configuration will go here executable: str = Field(\"/path/to/executable\", description=\"...\") ... # Additional params. # param1: param1Type = Field(\"default\", description=\"\", ...) Config settings and options Under the class definition for Config in the model, we can modify global options for all the parameters. In addition, there are a number of configuration options related to specifying what the outputs/results from the associated Task are, and a number of options to modify runtime behaviour. Currently, the available configuration options are: Config Parameter Meaning Default Value ThirdPartyTask-specific? run_directory If provided, can be used to specify the directory from which a Task is run. None (not provided) NO set_result bool . If True search the model definition for a parameter that indicates what the result is. False NO result_from_params If set_result is True can define a result using this option and a validator. See also is_result below. None (not provided) NO short_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s long_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s These configuration options modify how the parameter models are parsed and passed along on the command-line, as well as what we consider results and where a Task can run. The default behaviour is that parameters are assumed to be passed as -p arg and --param arg , the Task will be run in the current working directory (or scratch if submitted with the ARP), and we have no information about Task results . Setting the above options can modify this behaviour. By setting short_flags_use_eq and/or long_flags_use_eq to True parameters are instead passed as -p=arg and --param=arg . By setting run_directory to a valid path, we can force a Task to be run in a specific directory. By default the Task will be run from the directory you submit the job in, or from your scratch folder ( /sdf/scratch/... ) if you submit from the eLog. Some ThirdPartyTask s rely on searching the correct working directory in order run properly. By setting set_result to True we indicate that the TaskParameters model will provide information on what the TaskResult is. This setting must be used with one of two options, either the result_from_params Config option, described below, or the Field attribute is_result described in the next sub-section ( Field Attributes ). result_from_params is a Config option that can be used when set_result==True . In conjunction with a validator (described a sections down) we can use this option to specify a result from all the information contained in the model. E.g. if you have a Task that has parameters for an output_directory and a output_filename , you can set result_from_params==f\"{output_directory}/{output_filename}\" . Field attributes In addition to the global configuration options there are a couple of ways to specify individual parameters. The following Field attributes are used when parsing the model: Field Attribute Meaning Default Value Example flag_type Specify the type of flag for passing this argument. One of \"-\" , \"--\" , or \"\" N/A p_arg1 = Field(..., flag_type=\"\") rename_param Change the name of the parameter as passed on the command-line. N/A my_arg = Field(..., rename_param=\"my-arg\") description Documentation of the parameter's usage or purpose. N/A arg = Field(..., description=\"Argument for...\") is_result bool . If the set_result Config option is True , we can set this to True to indicate a result. N/A output_result = Field(..., is_result=true) The flag_type attribute allows us to specify whether the parameter corresponds to a positional ( \"\" ) command line argument, requires a single hyphen ( \"-\" ), or a double hyphen ( \"--\" ). By default, the parameter name is passed as-is on the command-line. However, command-line arguments can have characters which would not be valid in Python variable names. In particular, hyphens are frequently used. To handle this case, the rename_param attribute can be used to specify an alternative spelling of the parameter when it is passed on the command-line. This also allows for using more descriptive variable names internally than those used on the command-line. A description can also be provided for each Field to document the usage and purpose of that particular parameter. As an example, we can again consider defining a model for a RunTask Task . Consider an executable which would normally be called from the command-line as follows: /sdf/group/lcls/ds/tools/runtask -n <nthreads> --method=<algorithm> -p <algo_param> [--debug] A model specification for this Task may look like: class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for the runtask binary.\"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True # For the --method parameter # Prefer using full/absolute paths where possible. # No flag_type needed for this field executable: str = Field( \"/sdf/group/lcls/ds/tools/runtask\", description=\"Runtask Binary v1.0\" ) # We can provide a more descriptive name for -n # Let's assume it's a number of threads, or processes, etc. num_threads: int = Field( 1, description=\"Number of concurrent threads.\", flag_type=\"-\", rename_param=\"n\" ) # In this case we will use the Python variable name directly when passing # the parameter on the command-line method: str = Field(\"algo1\", description=\"Algorithm to use.\", flag_type=\"--\") # For an actual parameter we would probably have a better name. Lets assume # This parameter (-p) modifies the behaviour of the method above. method_param1: int = Field( 3, description=\"Modify method performance.\", flag_type=\"-\", rename_param=\"p\" ) # Boolean flags are only passed when True! `--debug` is an optional parameter # which is not followed by any arguments. debug: bool = Field( False, description=\"Whether to run in debug mode.\", flag_type=\"--\" ) The is_result attribute allows us to specify whether the corresponding Field points to the output/result of the associated Task . Consider a Task , RunTask2 which writes its output to a single file which is passed as a parameter. class RunTask2Parameters(ThirdPartyParameters): \"\"\"Parameters for the runtask2 binary.\"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True # This must be set here! # result_from_params: Optional[str] = None # We can use this for more complex result setups (see below). Ignore for now. # Prefer using full/absolute paths where possible. # No flag_type needed for this field executable: str = Field( \"/sdf/group/lcls/ds/tools/runtask2\", description=\"Runtask Binary v2.0\" ) # Lets assume we take one input and write one output file # We will not provide a default value, so this parameter MUST be provided input: str = Field( description=\"Path to input file.\", flag_type=\"--\" ) # We will also not provide a default for the output # BUT, we will specify that whatever is provided is the result output: str = Field( description=\"Path to write output to.\", flag_type=\"-\", rename_param=\"o\", is_result=True, # This means this parameter points to the result! ) Additional Comments 1. Model parameters of type bool are not passed with an argument and are only passed when True . This is a common use-case for boolean flags which enable things like test or debug modes, verbosity or reporting features. E.g. --debug , --test , --verbose , etc. - If you need to pass the literal words \"True\" or \"False\" , use a parameter of type str . 2. You can use pydantic types to constrain parameters beyond the basic Python types. E.g. conint can be used to define lower and upper bounds for an integer. There are also types for common categories, positive/negative numbers, paths, URLs, IP addresses, etc. - Even more custom behaviour can be achieved with validator s (see below). 3. All TaskParameters objects and its subclasses have access to a lute_config parameter, which is of type lute.io.models.base.AnalysisHeader . This special parameter is ignored when constructing the call for a binary task, but it provides access to shared/common parameters between tasks. For example, the following parameters are available through the lute_config object, and may be of use when constructing validators. All fields can be accessed with . notation. E.g. lute_config.experiment . - title : A user provided title/description of the analysis. - experiment : The current experiment name - run : The current acquisition run number - date : The date of the experiment or the analysis. - lute_version : The version of the software you are running. - task_timeout : How long a Task can run before it is killed. - work_dir : The main working directory for LUTE. Files and the database are created relative to this directory. This is separate from the run_directory config option. LUTE will write files to the work directory by default; however, the Task itself is run from run_directory if it is specified. Validators Pydantic uses validators to determine whether a value for a specific field is appropriate. There are default validators for all the standard library types and the types specified within the pydantic package; however, it is straightforward to define custom ones as well. In the template code-snippet above we imported the validator decorator. To create our own validator we define a method (with any name) with the following prototype, and decorate it with the validator decorator: @validator(\"name_of_field_to_decorate\") def my_custom_validator(cls, field: Any, values: Dict[str, Any]) -> Any: ... In this snippet, the field variable corresponds to the value for the specific field we want to validate. values is a dictionary of fields and their values which have been parsed prior to the current field. This means you can validate the value of a parameter based on the values provided for other parameters. Since pydantic always validates the fields in the order they are defined in the model, fields dependent on other fields should come later in the definition. For example, consider the method_param1 field defined above for RunTask . We can provide a custom validator which changes the default value for this field depending on what type of algorithm is specified for the --method option. We will also constrain the options for method to two specific strings. from pydantic import Field, validator, ValidationError, root_validator class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for the runtask binary.\"\"\" # [...] # In this case we will use the Python variable name directly when passing # the parameter on the command-line method: str = Field(\"algo1\", description=\"Algorithm to use.\", flag_type=\"--\") # For an actual parameter we would probably have a better name. Lets assume # This parameter (-p) modifies the behaviour of the method above. method_param1: Optional[int] = Field( description=\"Modify method performance.\", flag_type=\"-\", rename_param=\"p\" ) # We will only allow method to take on one of two values @validator(\"method\") def validate_method(cls, method: str, values: Dict[str, Any]) -> str: \"\"\"Method validator: --method can be algo1 or algo2.\"\"\" valid_methods: List[str] = [\"algo1\", \"algo2\"] if method not in valid_methods: raise ValueError(\"method must be algo1 or algo2\") return method # Lets change the default value of `method_param1` depending on `method` # NOTE: We didn't provide a default value to the Field above and made it # optional. We can use this to test whether someone is purposefully # overriding the value of it, and if not, set the default ourselves. # We set `always=True` since pydantic will normally not use the validator # if the default is not changed @validator(\"method_param1\", always=True) def validate_method_param1(cls, param1: Optional[int], values: Dict[str, Any]) -> int: \"\"\"method param1 validator\"\"\" # If someone actively defined it, lets just return that value # We could instead do some additional validation to make sure that the # value they provided is valid... if param1 is not None: return param1 # method_param1 comes after method, so this will be defined, or an error # would have been raised. method: str = values['method'] if method == \"algo1\": return 3 elif method == \"algo2\": return 5 The special root_validator(pre=False) can also be used to provide validation of the model as a whole. This is also the recommended method for specifying a result (using result_from_params ) which has a complex dependence on the parameters of the model. This latter use-case is described in FAQ 2 below. FAQ How can I specify a default value which depends on another parameter? Use a custom validator. The example above shows how to do this. The parameter that depends on another parameter must come LATER in the model defintion than the independent parameter. My TaskResult is determinable from the parameters model, but it isn't easily specified by one parameter. How can I use result_from_params to indicate the result? When a result can be identified from the set of parameters defined in a TaskParameters model, but is not as straightforward as saying it is equivalent to one of the parameters alone, we can set result_from_params using a custom validator. In the example below, we have two parameters which together determine what the result is, output_dir and out_name . Using a validator we will define a result from these two values. from pydantic import Field, root_validator class RunTask3Parameters(ThirdPartyParameters): \"\"\"Parameters for the runtask3 binary.\"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True # This must be set here! result_from_params: str = \"\" # We will set this momentarily # [...] executable, other params, etc. output_dir: str = Field( description=\"Directory to write output to.\", flag_type=\"--\", rename_param=\"dir\", ) out_name: str = Field( description=\"The name of the final output file.\", flag_type=\"--\", rename_param=\"oname\", ) # We can still provide other validators as needed # But for now, we just set result_from_params # Validator name can be anything, we set pre=False so this runs at the end @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: # Extract the values of output_dir and out_name output_dir: str = values[\"output_dir\"] out_name: str = values[\"out_name\"] result: str = f\"{output_dir}/{out_name}\" # Now we set result_from_params cls.Config.result_from_params = result # We haven't modified any other values, but we MUST return this! return values My new Task depends on the output of a previous Task , how can I specify this dependency? Parameters used to run a Task are recorded in a database for every Task . It is also recorded whether or not the execution of that specific parameter set was successful. A utility function is provided to access the most recent values from the database for a specific parameter of a specific Task . It can also be used to specify whether unsuccessful Task s should be included in the query. This utility can be used within a validator to specify dependencies. For example, suppose the input of RunTask2 (parameter input ) depends on the output location of RunTask1 (parameter outfile ). A validator of the following type can be used to retrieve the output file and make it the default value of the input parameter. from pydantic import Field, validator from .base import ThirdPartyParameters from ..db import read_latest_db_entry class RunTask2Parameters(ThirdPartyParameters): input: str = Field(\"\", description=\"Input file.\", flag_type=\"--\") @validator(\"input\") def validate_input(cls, input: str, values: Dict[str, Any]) -> str: if input == \"\": task1_out: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", # Working directory. We search for the database here. \"RunTask1\", # Name of Task we want to look up \"outfile\", # Name of parameter of the Task valid_only=True, # We only want valid output files. ) # read_latest_db_entry returns None if nothing is found if task1_out is not None: return task1_out return input There are more examples of this pattern spread throughout the various Task models. Specifying an Executor : Creating a runnable, \"managed Task \" Overview After a pydantic model has been created, the next required step is to define a managed Task . In the context of this library, a managed Task refers to the combination of an Executor and a Task to run. The Executor manages the process of Task submission and the execution environment, as well as performing an logging, eLog communication, etc. There are currently two types of Executor to choose from. For most cases you will use the first option for third-party Task s. Executor : This is the standard Executor and sufficient for most third-party uses cases. MPIExecutor : This performs all the same types of operations as the option above; however, it will submit your Task using MPI. The MPIExecutor will submit the Task using the number of available cores - 1. The number of cores is determined from the physical core/thread count on your local machine, or the number of cores allocated by SLURM when submitting on the batch nodes. As mentioned, for most cases you can setup a third-party Task to use the first type of Executor . If, however, your third-party Task uses MPI, you can use either. When using the standard Executor for a Task requiring MPI, the executable in the pydantic model must be set to mpirun . For example, a third-party Task model, that uses MPI but can be run with the Executor may look like the following. We assume this Task runs a Python script using MPI. class RunMPITaskParameters(ThirdPartyParameters): class Config(ThirdPartyParameters.Config): ... executable: str = Field(\"mpirun\", description=\"MPI executable\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) pos_arg: str = Field(\"python\", description=\"Python...\", flag_type=\"\") script: str = Field(\"\", description=\"Python script to run with MPI\", flag_type=\"\") Selecting the Executor After deciding on which Executor to use, a single line must be added to the lute/managed_tasks.py module: # Initialization: Executor(\"TaskName\") TaskRunner: Executor = Executor(\"SubmitTask\") # TaskRunner: MPIExecutor = MPIExecutor(\"SubmitTask\") ## If using the MPIExecutor In an attempt to make it easier to discern whether discussing a Task or managed Task , the standard naming convention is that the Task (class name) will have a verb in the name, e.g. RunTask , SubmitTask . The corresponding managed Task will use a related noun, e.g. TaskRunner , TaskSubmitter , etc. As a reminder, the Task name is the first part of the class name of the pydantic model, without the Parameters suffix. This name must match. E.g. if your pydantic model's class name is RunTaskParameters , the Task name is RunTask , and this is the string passed to the Executor initializer. Modifying the environment If your third-party Task can run in the standard psana environment with no further configuration files, the setup process is now complete and your Task can be run within the LUTE framework. If on the other hand your Task requires some changes to the environment, this is managed through the Executor . There are a couple principle methods that the Executor has to change the environment. Executor.update_environment : if you only need to add a few environment variables, or update the PATH this is the method to use. The method takes a Dict[str, str] as input. Any variables can be passed/defined using this method. By default, any variables in the dictionary will overwrite those variable definitions in the current environment if they are already present, except for the variable PATH . By default PATH entries in the dictionary are prepended to the current PATH available in the environment the Executor runs in (the standard psana environment). This behaviour can be changed to either append, or overwrite the PATH entirely by an optional second argument to the method. Executor.shell_source : This method will source a shell script which can perform numerous modifications of the environment (PATH changes, new environment variables, conda environments, etc.). The method takes a str which is the path to a shell script to source. As an example, we will update the PATH of one Task and source a script for a second. TaskRunner: Executor = Executor(\"RunTask\") # update_environment(env: Dict[str,str], update_path: str = \"prepend\") # \"append\" or \"overwrite\" TaskRunner.update_environment( { \"PATH\": \"/sdf/group/lcls/ds/tools\" } # This entry will be prepended to the PATH available after sourcing `psconda.sh` ) Task2Runner: Executor = Executor(\"RunTask2\") Task2Runner.shell_source(\"/sdf/group/lcls/ds/tools/new_task_setup.sh\") # Will source new_task_setup.sh script Using templates: managing third-party configuration files Some third-party executables will require their own configuration files. These are often separate JSON or YAML files, although they can also be bash or Python scripts which are intended to be edited. Since LUTE requires its own configuration YAML file, it attempts to handle these cases by using Jinja templates. When wrapping a third-party task a template can also be provided - with small modifications to the Task 's pydantic model, LUTE can process special types of parameters to render them in the template. LUTE offloads all the template rendering to Jinja, making the required additions to the pydantic model small. On the other hand, it does require understanding the Jinja syntax, and the provision of a well-formatted template, to properly parse parameters. Some basic examples of this syntax will be shown below; however, it is recommended that the Task implementer refer to the official Jinja documentation for more information. LUTE provides two additional base models which are used for template parsing in conjunction with the primary Task model. These are: - TemplateParameters objects which hold parameters which will be used to render a portion of a template. - TemplateConfig objects which hold two strings: the name of the template file to use and the full path (including filename) of where to output the rendered result. Task models which inherit from the ThirdPartyParameters model, as all third-party Task s should, allow for extra arguments. LUTE will parse any extra arguments provided in the configuration YAML as TemplateParameters objects automatically, which means that they do not need to be explicitly added to the pydantic model (although they can be). As such the only requirement on the Python-side when adding template rendering functionality to the Task is the addition of one parameter - an instance of TemplateConfig . The instance MUST be called lute_template_cfg . from pydantic import Field, validator from .base import TemplateConfig class RunTaskParamaters(ThirdPartyParameters): ... # This parameter MUST be called lute_template_cfg! lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"name_of_template.json\", output_path=\"/path/to/write/rendered_output_to.json\", ), description=\"Template rendering configuration\", ) LUTE looks for the template in config/templates , so only the name of the template file to use within that directory is required for the template_name attribute of lute_template_cfg . LUTE can write the output anywhere (the user has permissions), and with any name, so the full absolute path including filename should be used for the output_path of lute_template_cfg . The rest of the work is done by the combination of Jinja, LUTE's configuration YAML file, and the template itself. Understanding the interplay between these components is perhaps best illustrated by an example. As such, let us consider a simple third-party Task whose only input parameter (on the command-line) is the location of a configuration JSON file. We'll call the third-party executable jsonuser and our Task model, the RunJsonUserParameters . We assume the program is run like: jsonuser -i <input_file.json> The first step is to setup the pydantic model as before. from pydantic import Field, validator from .base import TemplateConfig class RunJsonUserParameters: executable: str = Field( \"/path/to/jsonuser\", description=\"Executable which requires a JSON configuration file.\" ) # Lets assume the JSON file is passed as \"-i <path_to_json>\" input_json: str = Field( \"\", description=\"Path to the input JSON file.\", flag_type=\"-\", rename_param=\"i\" ) The next step is to create a template for the JSON file. Let's assume the JSON file looks like: { \"param1\": \"arg1\", \"param2\": 4, \"param3\": { \"a\": 1, \"b\": 2 }, \"param4\": [ 1, 2, 3 ] } Any, or all of these values can be substituted for, and we can determine the way in which we will provide them. I.e. a substitution can be provided for each variable individually, or, for example for a nested hierarchy, a dictionary can be provided which will substitute all the items at once. For this simple case, let's provide variables for param1 , param2 , param3.b and assume that we want the first and second entries for param4 to be identical for our use case (i.e., we can use one variable for them both. In total, this means we will perform 5 substitutions using 4 variables. Jinja will substitute a variable anywhere it sees the following syntax, {{ variable_name }} . As such a valid template for our use-case may look like: { \"param1\": {{ str_var }}, \"param2\": {{ int_var }}, \"param3\": { \"a\": 1, \"b\": {{ p3_b }} }, \"param4\": [ {{ val }}, {{ val }}, 3 ] } We save this file as jsonuser.json in config/templates . Next, we will update the original pydantic model to include our template configuration. We still have an issue, however, in that we need to decide where to write the output of the template to. In this case, we can use the input_json parameter. We will assume that the user will provide this, although a default value can also be used. A custom validator will be added so that we can take the input_json value and update the value of lute_template_cfg.output_path with it. # from typing import Optional from pydantic import Field, validator from .base import TemplateConfig #, TemplateParameters class RunJsonUserParameters: executable: str = Field( \"jsonuser\", description=\"Executable which requires a JSON configuration file.\" ) # Lets assume the JSON file is passed as \"-i <path_to_json>\" input_json: str = Field( \"\", description=\"Path to the input JSON file.\", flag_type=\"-\", rename_param=\"i\" ) # Add template configuration! *MUST* be called `lute_template_cfg` lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"jsonuser.json\", # Only the name of the file here. output_path=\"\", ), description=\"Template rendering configuration\", ) # We do not need to include these TemplateParameters, they will be added # automatically if provided in the YAML #str_var: Optional[TemplateParameters] #int_var: Optional[TemplateParameters] #p3_b: Optional[TemplateParameters] #val: Optional[TemplateParameters] # Tell LUTE to write the rendered template to the location provided with # `input_json`. I.e. update `lute_template_cfg.output_path` @validator(\"lute_template_cfg\", always=True) def update_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"input_json\"] return lute_template_cfg All that is left to render the template, is to provide the variables we want to substitute in the LUTE configuration YAML. In our case we must provide the 4 variable names we included within the substitution syntax ( {{ var_name }} ). The names in the YAML must match those in the template. RunJsonUser: input_json: \"/my/chosen/path.json\" # We'll come back to this... str_var: \"arg1\" # Will substitute for \"param1\": \"arg1\" int_var: 4 # Will substitute for \"param2\": 4 p3_b: 2 # Will substitute for \"param3: { \"b\": 2 } val: 2 # Will substitute for \"param4\": [2, 2, 3] in the JSON If on the other hand, a user were to have an already valid JSON file, it is possible to turn off the template rendering. (ALL) Template variables ( TemplateParameters ) are simply excluded from the configuration YAML. RunJsonUser: input_json: \"/path/to/existing.json\" #str_var: ... #... Additional Jinja Syntax There are many other syntactical constructions we can use with Jinja. Some of the useful ones are: If Statements - E.g. only include portions of the template if a value is defined. {% if VARNAME is defined %} // Stuff to include {% endif %} Loops - E.g. Unpacking multiple elements from a dictionary. {% for name, value in VARNAME.items() %} // Do stuff with name and value {% endfor %} Creating a \"First-Party\" Task The process for creating a \"First-Party\" Task is very similar to that for a \"Third-Party\" Task , with the difference being that you must also write the analysis code. The steps for integration are: 1. Write the TaskParameters model. 2. Write the Task class. There are a few rules that need to be adhered to. 3. Make your Task available by modifying the import function. 4. Specify an Executor Specifying a TaskParameters Model for your Task Parameter models have a format that must be followed for \"Third-Party\" Task s, but \"First-Party\" Task s have a little more liberty in how parameters are dealt with, since the Task will do all the parsing itself. To create a model, the basic steps are: 1. If necessary, create a new module (e.g. new_task_category.py ) under lute.io.models , or find an appropriate pre-existing module in that directory. - An import statement must be added to lute.io.models._init_ if a new module is created, so it can be found. - If defining the model in a pre-existing module, make sure to modify the __all__ statement to include it. 2. Create a new model that inherits from TaskParameters . You can look at lute.models.io.tests.TestReadOutputParameters for an example. The model must be named <YourTaskName>Parameters - You should include all relevant parameters here, including input file, output file, and any potentially adjustable parameters. These parameters must be included even if there are some implicit dependencies between Task s and it would make sense for the parameter to be auto-populated based on some other output. Creating this dependency is done with validators (see step 3.). All parameters should be overridable, and all Task s should be fully-independently configurable, based solely on their model and the configuration YAML. - To follow the preferred format, parameters should be defined as: param_name: type = Field([default value], description=\"This parameter does X.\") 3. Use validators to do more complex things for your parameters, including populating default values dynamically: - E.g. create default values that depend on other parameters in the model - see for example: SubmitSMDParameters . - E.g. create default values that depend on other Task s by reading from the database - see for example: TestReadOutputParameters . 4. The model will have access to some general configuration values by inheriting from TaskParameters . These parameters are all stored in lute_config which is an instance of AnalysisHeader ( defined here ). - For example, the experiment and run number can be obtained from this object and a validator could use these values to define the default input file for the Task . A number of configuration options and Field attributes are also available for \"First-Party\" Task models. These are identical to those used for the ThirdPartyTask s, although there is a smaller selection. These options are reproduced below for convenience. Config settings and options Under the class definition for Config in the model, we can modify global options for all the parameters. In addition, there are a number of configuration options related to specifying what the outputs/results from the associated Task are, and a number of options to modify runtime behaviour. Currently, the available configuration options are: Config Parameter Meaning Default Value ThirdPartyTask-specific? run_directory If provided, can be used to specify the directory from which a Task is run. None (not provided) NO set_result bool . If True search the model definition for a parameter that indicates what the result is. False NO result_from_params If set_result is True can define a result using this option and a validator. See also is_result below. None (not provided) NO short_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s long_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s These configuration options modify how the parameter models are parsed and passed along on the command-line, as well as what we consider results and where a Task can run. The default behaviour is that parameters are assumed to be passed as -p arg and --param arg , the Task will be run in the current working directory (or scratch if submitted with the ARP), and we have no information about Task results . Setting the above options can modify this behaviour. By setting short_flags_use_eq and/or long_flags_use_eq to True parameters are instead passed as -p=arg and --param=arg . By setting run_directory to a valid path, we can force a Task to be run in a specific directory. By default the Task will be run from the directory you submit the job in, or from your scratch folder ( /sdf/scratch/... ) if you submit from the eLog. Some ThirdPartyTask s rely on searching the correct working directory in order run properly. By setting set_result to True we indicate that the TaskParameters model will provide information on what the TaskResult is. This setting must be used with one of two options, either the result_from_params Config option, described below, or the Field attribute is_result described in the next sub-section ( Field Attributes ). result_from_params is a Config option that can be used when set_result==True . In conjunction with a validator (described a sections down) we can use this option to specify a result from all the information contained in the model. E.g. if you have a Task that has parameters for an output_directory and a output_filename , you can set result_from_params==f\"{output_directory}/{output_filename}\" . Field attributes In addition to the global configuration options there are a couple of ways to specify individual parameters. The following Field attributes are used when parsing the model: Field Attribute Meaning Default Value Example description Documentation of the parameter's usage or purpose. N/A arg = Field(..., description=\"Argument for...\") is_result bool . If the set_result Config option is True , we can set this to True to indicate a result. N/A output_result = Field(..., is_result=true) Writing the Task You can write your analysis code (or whatever code to be executed) as long as it adheres to the limited rules below. You can create a new module for your Task in lute.tasks or add it to any existing module, if it makes sense for it to belong there. The Task itself is a single class constructed as: 1. Your analysis Task is a class named in a way that matches its Pydantic model. E.g. RunTask is the Task , and RunTaskParameters is the Pydantic model. 2. The class must inherit from the Task class (see template below). 3. You must provide an implementation of a _run method. This is the method that will be executed when the Task is run. You can in addition write as many methods as you need. For fine-grained execution control you can also provide _pre_run() and _post_run() methods, but this is optional. 4. For all communication (including print statements) you should use the _report_to_executor(msg: Message) method. Since the Task is run as a subprocess this method will pass information to the controlling Executor . You can pass any type of object using this method, strings, plots, arrays, etc. 5. If you did not use the set_result configuration option in your parameters model, make sure to provide a result when finished. This is done by setting self._result.payload = ... . You can set the result to be any object. If you have written the result to a file, for example, please provide a path. A minimal template is provided below. \"\"\"Standard docstring...\"\"\" __all__ = [\"RunTask\"] __author__ = \"\" # Please include so we know who the SME is # Include any imports you need here from lute.execution.ipc import Message # Message for communication from lute.io.models.base import * # For TaskParameters from lute.tasks.task import * # For Task class RunTask(Task): # Inherit from Task \"\"\"Task description goes here, or in __init__\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) # Sets up Task, parameters, etc. # Parameters will be available through: # self._task_parameters # You access with . operator: self._task_parameters.param1, etc. # Your result object is availble through: # self._result # self._result.payload <- Main result # self._result.summary <- Short summary # self._result.task_status <- Semi-automatic, but can be set manually def _run(self) -> None: # THIS METHOD MUST BE PROVIDED self.do_my_analysis() def do_my_analysis(self) -> None: # Send a message, proper way to print: msg: Message(contents=\"My message contents\", signal=\"\") self._report_to_executor(msg) # When done, set result - assume we wrote a file, e.g. self._result.payload = \"/path/to/output_file.h5\" # Optionally also set status - good practice but not obligatory self._result.task_status = TaskStatus.COMPLETED Signals in Message objects are strings and can be one of the following: LUTE_SIGNALS: Set[str] = { \"NO_PICKLE_MODE\", \"TASK_STARTED\", \"TASK_FAILED\", \"TASK_STOPPED\", \"TASK_DONE\", \"TASK_CANCELLED\", \"TASK_RESULT\", } Each of these signals is associated with a hook on the Executor -side. They are for the most part used by base classes; however, you can choose to make use of them manually as well. Making your Task available Once the Task has been written, it needs to be made available for import. Since different Task s can have conflicting dependencies and environments, this is managed through an import function. When the Task is done, or ready for testing, a condition is added to lute.tasks.__init__.import_task . For example, assume the Task is called RunXASAnalysis and it's defined in a module called xas.py , we would add the following lines to the import_task function: # in lute.tasks.__init__ # ... def import_task(task_name: str) -> Type[Task]: # ... if task_name == \"RunXASAnalysis\": from .xas import RunXASAnalysis return RunXASAnalysis Defining an Executor The process of Executor definition is identical to the process as described for ThirdPartyTask s above.","title":"Creating a new Task"},{"location":"tutorial/new_task/#integrating-a-new-task","text":"Task s can be broadly categorized into two types: - \"First-party\" - where the analysis or executed code is maintained within this library. - \"Third-party\" - where the analysis, code, or program is maintained elsewhere and is simply called by a wrapping Task . Creating a new Task of either type generally involves the same steps, although for first-party Task s, the analysis code must of course also be written. Due to this difference, as well as additional considerations for parameter handling when dealing with \"third-party\" Task s, the \"first-party\" and \"third-party\" Task integration cases will be considered separately.","title":"Integrating a New Task"},{"location":"tutorial/new_task/#creating-a-third-party-task","text":"There are two required steps for third-party Task integration, and one additional step which is optional, and may not be applicable to all possible third-party Task s. Generally, Task integration requires: 1. Defining a TaskParameters (pydantic) model which fully parameterizes the Task . This involves specifying a path to a binary, and all the required command-line arguments to run the binary. 2. Creating a managed Task by specifying an Executor for the new third-party Task . At this stage, any additional environment variables can be added which are required for the execution environment. 3. (Optional/Maybe applicable) Create a template for a third-party configuration file. If the new Task has its own configuration file, specifying a template will allow that file to be parameterized from the singular LUTE yaml configuration file. A couple of minor additions to the pydantic model specified in 1. are required to support template usage. Each of these stages will be discussed in detail below. The vast majority of the work is completed in step 1.","title":"Creating a \"Third-party\" Task"},{"location":"tutorial/new_task/#specifying-a-taskparameters-model-for-your-task","text":"A brief overview of parameters objects will be provided below. The following information goes into detail only about specifics related to LUTE configuration. An in depth description of pydantic is beyond the scope of this tutorial; please refer to the official documentation for more information. Please note that due to environment constraints pydantic is currently pinned to version 1.10! Make sure to read the appropriate documentation for this version as many things are different compared to the newer releases. At the end this document there will be an example highlighting some supported behaviour as well as a FAQ to address some common integration considerations. Task s and TaskParameter s All Task s have a corresponding TaskParameters object. These objects are linked exclusively by a named relationship. For a Task named MyThirdPartyTask , the parameters object must be named MyThirdPartyTaskParameters . For third-party Task s there are a number of additional requirements: - The model must inherit from a base class called ThirdPartyParameters . - The model must have one field specified called executable . The presence of this field indicates that the Task is a third-party Task and the specified executable must be called. This allows all third-party Task s to be defined exclusively by their parameters model. A single ThirdPartyTask class handles execution of all third-party Task s. All models are stored in lute/io/models . For any given Task , a new model can be added to an existing module contained in this directory or to a new module. If creating a new module, make sure to add an import statement to lute.io.models.__init__ . Defining TaskParameter s When specifying parameters the default behaviour is to provide a one-to-one correspondance between the Python attribute specified in the parameter model, and the parameter specified on the command-line. Single-letter attributes are assumed to be passed using - , e.g. n will be passed as -n when the executable is launched. Longer attributes are passed using -- , e.g. by default a model attribute named my_arg will be passed on the command-line as --my_arg . Positional arguments are specified using p_argX where X is a number. All parameters are passed in the order that they are specified in the model. However, because the number of possible command-line combinations is large, relying on the default behaviour above is NOT recommended . It is provided solely as a fallback. Instead, there are a number of configuration knobs which can be tuned to achieve the desired behaviour. The two main mechanisms for controlling behaviour are specification of model-wide configuration under the Config class within the model's definition, and parameter-by-parameter configuration using field attributes. For the latter, we define all parameters as Field objects. This allows parameters to have their own attributes, which are parsed by LUTE's task-layer. Given this, the preferred starting template for a TaskParameters model is the following - we assume we are integrating a new Task called RunTask : from pydantic import Field, validator # Also include any pydantic type specifications - Pydantic has many custom # validation types already, e.g. types for constrained numberic values, URL handling, etc. from .base import ThirdPartyParameters # Change class name as necessary class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for RunTask...\"\"\" class Config(ThirdPartyParameters.Config): # MUST be exactly as written here. ... # Model-wide configuration will go here executable: str = Field(\"/path/to/executable\", description=\"...\") ... # Additional params. # param1: param1Type = Field(\"default\", description=\"\", ...) Config settings and options Under the class definition for Config in the model, we can modify global options for all the parameters. In addition, there are a number of configuration options related to specifying what the outputs/results from the associated Task are, and a number of options to modify runtime behaviour. Currently, the available configuration options are: Config Parameter Meaning Default Value ThirdPartyTask-specific? run_directory If provided, can be used to specify the directory from which a Task is run. None (not provided) NO set_result bool . If True search the model definition for a parameter that indicates what the result is. False NO result_from_params If set_result is True can define a result using this option and a validator. See also is_result below. None (not provided) NO short_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s long_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s These configuration options modify how the parameter models are parsed and passed along on the command-line, as well as what we consider results and where a Task can run. The default behaviour is that parameters are assumed to be passed as -p arg and --param arg , the Task will be run in the current working directory (or scratch if submitted with the ARP), and we have no information about Task results . Setting the above options can modify this behaviour. By setting short_flags_use_eq and/or long_flags_use_eq to True parameters are instead passed as -p=arg and --param=arg . By setting run_directory to a valid path, we can force a Task to be run in a specific directory. By default the Task will be run from the directory you submit the job in, or from your scratch folder ( /sdf/scratch/... ) if you submit from the eLog. Some ThirdPartyTask s rely on searching the correct working directory in order run properly. By setting set_result to True we indicate that the TaskParameters model will provide information on what the TaskResult is. This setting must be used with one of two options, either the result_from_params Config option, described below, or the Field attribute is_result described in the next sub-section ( Field Attributes ). result_from_params is a Config option that can be used when set_result==True . In conjunction with a validator (described a sections down) we can use this option to specify a result from all the information contained in the model. E.g. if you have a Task that has parameters for an output_directory and a output_filename , you can set result_from_params==f\"{output_directory}/{output_filename}\" . Field attributes In addition to the global configuration options there are a couple of ways to specify individual parameters. The following Field attributes are used when parsing the model: Field Attribute Meaning Default Value Example flag_type Specify the type of flag for passing this argument. One of \"-\" , \"--\" , or \"\" N/A p_arg1 = Field(..., flag_type=\"\") rename_param Change the name of the parameter as passed on the command-line. N/A my_arg = Field(..., rename_param=\"my-arg\") description Documentation of the parameter's usage or purpose. N/A arg = Field(..., description=\"Argument for...\") is_result bool . If the set_result Config option is True , we can set this to True to indicate a result. N/A output_result = Field(..., is_result=true) The flag_type attribute allows us to specify whether the parameter corresponds to a positional ( \"\" ) command line argument, requires a single hyphen ( \"-\" ), or a double hyphen ( \"--\" ). By default, the parameter name is passed as-is on the command-line. However, command-line arguments can have characters which would not be valid in Python variable names. In particular, hyphens are frequently used. To handle this case, the rename_param attribute can be used to specify an alternative spelling of the parameter when it is passed on the command-line. This also allows for using more descriptive variable names internally than those used on the command-line. A description can also be provided for each Field to document the usage and purpose of that particular parameter. As an example, we can again consider defining a model for a RunTask Task . Consider an executable which would normally be called from the command-line as follows: /sdf/group/lcls/ds/tools/runtask -n <nthreads> --method=<algorithm> -p <algo_param> [--debug] A model specification for this Task may look like: class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for the runtask binary.\"\"\" class Config(ThirdPartyParameters.Config): long_flags_use_eq: bool = True # For the --method parameter # Prefer using full/absolute paths where possible. # No flag_type needed for this field executable: str = Field( \"/sdf/group/lcls/ds/tools/runtask\", description=\"Runtask Binary v1.0\" ) # We can provide a more descriptive name for -n # Let's assume it's a number of threads, or processes, etc. num_threads: int = Field( 1, description=\"Number of concurrent threads.\", flag_type=\"-\", rename_param=\"n\" ) # In this case we will use the Python variable name directly when passing # the parameter on the command-line method: str = Field(\"algo1\", description=\"Algorithm to use.\", flag_type=\"--\") # For an actual parameter we would probably have a better name. Lets assume # This parameter (-p) modifies the behaviour of the method above. method_param1: int = Field( 3, description=\"Modify method performance.\", flag_type=\"-\", rename_param=\"p\" ) # Boolean flags are only passed when True! `--debug` is an optional parameter # which is not followed by any arguments. debug: bool = Field( False, description=\"Whether to run in debug mode.\", flag_type=\"--\" ) The is_result attribute allows us to specify whether the corresponding Field points to the output/result of the associated Task . Consider a Task , RunTask2 which writes its output to a single file which is passed as a parameter. class RunTask2Parameters(ThirdPartyParameters): \"\"\"Parameters for the runtask2 binary.\"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True # This must be set here! # result_from_params: Optional[str] = None # We can use this for more complex result setups (see below). Ignore for now. # Prefer using full/absolute paths where possible. # No flag_type needed for this field executable: str = Field( \"/sdf/group/lcls/ds/tools/runtask2\", description=\"Runtask Binary v2.0\" ) # Lets assume we take one input and write one output file # We will not provide a default value, so this parameter MUST be provided input: str = Field( description=\"Path to input file.\", flag_type=\"--\" ) # We will also not provide a default for the output # BUT, we will specify that whatever is provided is the result output: str = Field( description=\"Path to write output to.\", flag_type=\"-\", rename_param=\"o\", is_result=True, # This means this parameter points to the result! ) Additional Comments 1. Model parameters of type bool are not passed with an argument and are only passed when True . This is a common use-case for boolean flags which enable things like test or debug modes, verbosity or reporting features. E.g. --debug , --test , --verbose , etc. - If you need to pass the literal words \"True\" or \"False\" , use a parameter of type str . 2. You can use pydantic types to constrain parameters beyond the basic Python types. E.g. conint can be used to define lower and upper bounds for an integer. There are also types for common categories, positive/negative numbers, paths, URLs, IP addresses, etc. - Even more custom behaviour can be achieved with validator s (see below). 3. All TaskParameters objects and its subclasses have access to a lute_config parameter, which is of type lute.io.models.base.AnalysisHeader . This special parameter is ignored when constructing the call for a binary task, but it provides access to shared/common parameters between tasks. For example, the following parameters are available through the lute_config object, and may be of use when constructing validators. All fields can be accessed with . notation. E.g. lute_config.experiment . - title : A user provided title/description of the analysis. - experiment : The current experiment name - run : The current acquisition run number - date : The date of the experiment or the analysis. - lute_version : The version of the software you are running. - task_timeout : How long a Task can run before it is killed. - work_dir : The main working directory for LUTE. Files and the database are created relative to this directory. This is separate from the run_directory config option. LUTE will write files to the work directory by default; however, the Task itself is run from run_directory if it is specified. Validators Pydantic uses validators to determine whether a value for a specific field is appropriate. There are default validators for all the standard library types and the types specified within the pydantic package; however, it is straightforward to define custom ones as well. In the template code-snippet above we imported the validator decorator. To create our own validator we define a method (with any name) with the following prototype, and decorate it with the validator decorator: @validator(\"name_of_field_to_decorate\") def my_custom_validator(cls, field: Any, values: Dict[str, Any]) -> Any: ... In this snippet, the field variable corresponds to the value for the specific field we want to validate. values is a dictionary of fields and their values which have been parsed prior to the current field. This means you can validate the value of a parameter based on the values provided for other parameters. Since pydantic always validates the fields in the order they are defined in the model, fields dependent on other fields should come later in the definition. For example, consider the method_param1 field defined above for RunTask . We can provide a custom validator which changes the default value for this field depending on what type of algorithm is specified for the --method option. We will also constrain the options for method to two specific strings. from pydantic import Field, validator, ValidationError, root_validator class RunTaskParameters(ThirdPartyParameters): \"\"\"Parameters for the runtask binary.\"\"\" # [...] # In this case we will use the Python variable name directly when passing # the parameter on the command-line method: str = Field(\"algo1\", description=\"Algorithm to use.\", flag_type=\"--\") # For an actual parameter we would probably have a better name. Lets assume # This parameter (-p) modifies the behaviour of the method above. method_param1: Optional[int] = Field( description=\"Modify method performance.\", flag_type=\"-\", rename_param=\"p\" ) # We will only allow method to take on one of two values @validator(\"method\") def validate_method(cls, method: str, values: Dict[str, Any]) -> str: \"\"\"Method validator: --method can be algo1 or algo2.\"\"\" valid_methods: List[str] = [\"algo1\", \"algo2\"] if method not in valid_methods: raise ValueError(\"method must be algo1 or algo2\") return method # Lets change the default value of `method_param1` depending on `method` # NOTE: We didn't provide a default value to the Field above and made it # optional. We can use this to test whether someone is purposefully # overriding the value of it, and if not, set the default ourselves. # We set `always=True` since pydantic will normally not use the validator # if the default is not changed @validator(\"method_param1\", always=True) def validate_method_param1(cls, param1: Optional[int], values: Dict[str, Any]) -> int: \"\"\"method param1 validator\"\"\" # If someone actively defined it, lets just return that value # We could instead do some additional validation to make sure that the # value they provided is valid... if param1 is not None: return param1 # method_param1 comes after method, so this will be defined, or an error # would have been raised. method: str = values['method'] if method == \"algo1\": return 3 elif method == \"algo2\": return 5 The special root_validator(pre=False) can also be used to provide validation of the model as a whole. This is also the recommended method for specifying a result (using result_from_params ) which has a complex dependence on the parameters of the model. This latter use-case is described in FAQ 2 below.","title":"Specifying a TaskParameters Model for your Task"},{"location":"tutorial/new_task/#faq","text":"How can I specify a default value which depends on another parameter? Use a custom validator. The example above shows how to do this. The parameter that depends on another parameter must come LATER in the model defintion than the independent parameter. My TaskResult is determinable from the parameters model, but it isn't easily specified by one parameter. How can I use result_from_params to indicate the result? When a result can be identified from the set of parameters defined in a TaskParameters model, but is not as straightforward as saying it is equivalent to one of the parameters alone, we can set result_from_params using a custom validator. In the example below, we have two parameters which together determine what the result is, output_dir and out_name . Using a validator we will define a result from these two values. from pydantic import Field, root_validator class RunTask3Parameters(ThirdPartyParameters): \"\"\"Parameters for the runtask3 binary.\"\"\" class Config(ThirdPartyParameters.Config): set_result: bool = True # This must be set here! result_from_params: str = \"\" # We will set this momentarily # [...] executable, other params, etc. output_dir: str = Field( description=\"Directory to write output to.\", flag_type=\"--\", rename_param=\"dir\", ) out_name: str = Field( description=\"The name of the final output file.\", flag_type=\"--\", rename_param=\"oname\", ) # We can still provide other validators as needed # But for now, we just set result_from_params # Validator name can be anything, we set pre=False so this runs at the end @root_validator(pre=False) def define_result(cls, values: Dict[str, Any]) -> Dict[str, Any]: # Extract the values of output_dir and out_name output_dir: str = values[\"output_dir\"] out_name: str = values[\"out_name\"] result: str = f\"{output_dir}/{out_name}\" # Now we set result_from_params cls.Config.result_from_params = result # We haven't modified any other values, but we MUST return this! return values My new Task depends on the output of a previous Task , how can I specify this dependency? Parameters used to run a Task are recorded in a database for every Task . It is also recorded whether or not the execution of that specific parameter set was successful. A utility function is provided to access the most recent values from the database for a specific parameter of a specific Task . It can also be used to specify whether unsuccessful Task s should be included in the query. This utility can be used within a validator to specify dependencies. For example, suppose the input of RunTask2 (parameter input ) depends on the output location of RunTask1 (parameter outfile ). A validator of the following type can be used to retrieve the output file and make it the default value of the input parameter. from pydantic import Field, validator from .base import ThirdPartyParameters from ..db import read_latest_db_entry class RunTask2Parameters(ThirdPartyParameters): input: str = Field(\"\", description=\"Input file.\", flag_type=\"--\") @validator(\"input\") def validate_input(cls, input: str, values: Dict[str, Any]) -> str: if input == \"\": task1_out: Optional[str] = read_latest_db_entry( f\"{values['lute_config'].work_dir}\", # Working directory. We search for the database here. \"RunTask1\", # Name of Task we want to look up \"outfile\", # Name of parameter of the Task valid_only=True, # We only want valid output files. ) # read_latest_db_entry returns None if nothing is found if task1_out is not None: return task1_out return input There are more examples of this pattern spread throughout the various Task models.","title":"FAQ"},{"location":"tutorial/new_task/#specifying-an-executor-creating-a-runnable-managed-task","text":"Overview After a pydantic model has been created, the next required step is to define a managed Task . In the context of this library, a managed Task refers to the combination of an Executor and a Task to run. The Executor manages the process of Task submission and the execution environment, as well as performing an logging, eLog communication, etc. There are currently two types of Executor to choose from. For most cases you will use the first option for third-party Task s. Executor : This is the standard Executor and sufficient for most third-party uses cases. MPIExecutor : This performs all the same types of operations as the option above; however, it will submit your Task using MPI. The MPIExecutor will submit the Task using the number of available cores - 1. The number of cores is determined from the physical core/thread count on your local machine, or the number of cores allocated by SLURM when submitting on the batch nodes. As mentioned, for most cases you can setup a third-party Task to use the first type of Executor . If, however, your third-party Task uses MPI, you can use either. When using the standard Executor for a Task requiring MPI, the executable in the pydantic model must be set to mpirun . For example, a third-party Task model, that uses MPI but can be run with the Executor may look like the following. We assume this Task runs a Python script using MPI. class RunMPITaskParameters(ThirdPartyParameters): class Config(ThirdPartyParameters.Config): ... executable: str = Field(\"mpirun\", description=\"MPI executable\") np: PositiveInt = Field( max(int(os.environ.get(\"SLURM_NPROCS\", len(os.sched_getaffinity(0)))) - 1, 1), description=\"Number of processes\", flag_type=\"-\", ) pos_arg: str = Field(\"python\", description=\"Python...\", flag_type=\"\") script: str = Field(\"\", description=\"Python script to run with MPI\", flag_type=\"\") Selecting the Executor After deciding on which Executor to use, a single line must be added to the lute/managed_tasks.py module: # Initialization: Executor(\"TaskName\") TaskRunner: Executor = Executor(\"SubmitTask\") # TaskRunner: MPIExecutor = MPIExecutor(\"SubmitTask\") ## If using the MPIExecutor In an attempt to make it easier to discern whether discussing a Task or managed Task , the standard naming convention is that the Task (class name) will have a verb in the name, e.g. RunTask , SubmitTask . The corresponding managed Task will use a related noun, e.g. TaskRunner , TaskSubmitter , etc. As a reminder, the Task name is the first part of the class name of the pydantic model, without the Parameters suffix. This name must match. E.g. if your pydantic model's class name is RunTaskParameters , the Task name is RunTask , and this is the string passed to the Executor initializer. Modifying the environment If your third-party Task can run in the standard psana environment with no further configuration files, the setup process is now complete and your Task can be run within the LUTE framework. If on the other hand your Task requires some changes to the environment, this is managed through the Executor . There are a couple principle methods that the Executor has to change the environment. Executor.update_environment : if you only need to add a few environment variables, or update the PATH this is the method to use. The method takes a Dict[str, str] as input. Any variables can be passed/defined using this method. By default, any variables in the dictionary will overwrite those variable definitions in the current environment if they are already present, except for the variable PATH . By default PATH entries in the dictionary are prepended to the current PATH available in the environment the Executor runs in (the standard psana environment). This behaviour can be changed to either append, or overwrite the PATH entirely by an optional second argument to the method. Executor.shell_source : This method will source a shell script which can perform numerous modifications of the environment (PATH changes, new environment variables, conda environments, etc.). The method takes a str which is the path to a shell script to source. As an example, we will update the PATH of one Task and source a script for a second. TaskRunner: Executor = Executor(\"RunTask\") # update_environment(env: Dict[str,str], update_path: str = \"prepend\") # \"append\" or \"overwrite\" TaskRunner.update_environment( { \"PATH\": \"/sdf/group/lcls/ds/tools\" } # This entry will be prepended to the PATH available after sourcing `psconda.sh` ) Task2Runner: Executor = Executor(\"RunTask2\") Task2Runner.shell_source(\"/sdf/group/lcls/ds/tools/new_task_setup.sh\") # Will source new_task_setup.sh script","title":"Specifying an Executor: Creating a runnable, \"managed Task\""},{"location":"tutorial/new_task/#using-templates-managing-third-party-configuration-files","text":"Some third-party executables will require their own configuration files. These are often separate JSON or YAML files, although they can also be bash or Python scripts which are intended to be edited. Since LUTE requires its own configuration YAML file, it attempts to handle these cases by using Jinja templates. When wrapping a third-party task a template can also be provided - with small modifications to the Task 's pydantic model, LUTE can process special types of parameters to render them in the template. LUTE offloads all the template rendering to Jinja, making the required additions to the pydantic model small. On the other hand, it does require understanding the Jinja syntax, and the provision of a well-formatted template, to properly parse parameters. Some basic examples of this syntax will be shown below; however, it is recommended that the Task implementer refer to the official Jinja documentation for more information. LUTE provides two additional base models which are used for template parsing in conjunction with the primary Task model. These are: - TemplateParameters objects which hold parameters which will be used to render a portion of a template. - TemplateConfig objects which hold two strings: the name of the template file to use and the full path (including filename) of where to output the rendered result. Task models which inherit from the ThirdPartyParameters model, as all third-party Task s should, allow for extra arguments. LUTE will parse any extra arguments provided in the configuration YAML as TemplateParameters objects automatically, which means that they do not need to be explicitly added to the pydantic model (although they can be). As such the only requirement on the Python-side when adding template rendering functionality to the Task is the addition of one parameter - an instance of TemplateConfig . The instance MUST be called lute_template_cfg . from pydantic import Field, validator from .base import TemplateConfig class RunTaskParamaters(ThirdPartyParameters): ... # This parameter MUST be called lute_template_cfg! lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"name_of_template.json\", output_path=\"/path/to/write/rendered_output_to.json\", ), description=\"Template rendering configuration\", ) LUTE looks for the template in config/templates , so only the name of the template file to use within that directory is required for the template_name attribute of lute_template_cfg . LUTE can write the output anywhere (the user has permissions), and with any name, so the full absolute path including filename should be used for the output_path of lute_template_cfg . The rest of the work is done by the combination of Jinja, LUTE's configuration YAML file, and the template itself. Understanding the interplay between these components is perhaps best illustrated by an example. As such, let us consider a simple third-party Task whose only input parameter (on the command-line) is the location of a configuration JSON file. We'll call the third-party executable jsonuser and our Task model, the RunJsonUserParameters . We assume the program is run like: jsonuser -i <input_file.json> The first step is to setup the pydantic model as before. from pydantic import Field, validator from .base import TemplateConfig class RunJsonUserParameters: executable: str = Field( \"/path/to/jsonuser\", description=\"Executable which requires a JSON configuration file.\" ) # Lets assume the JSON file is passed as \"-i <path_to_json>\" input_json: str = Field( \"\", description=\"Path to the input JSON file.\", flag_type=\"-\", rename_param=\"i\" ) The next step is to create a template for the JSON file. Let's assume the JSON file looks like: { \"param1\": \"arg1\", \"param2\": 4, \"param3\": { \"a\": 1, \"b\": 2 }, \"param4\": [ 1, 2, 3 ] } Any, or all of these values can be substituted for, and we can determine the way in which we will provide them. I.e. a substitution can be provided for each variable individually, or, for example for a nested hierarchy, a dictionary can be provided which will substitute all the items at once. For this simple case, let's provide variables for param1 , param2 , param3.b and assume that we want the first and second entries for param4 to be identical for our use case (i.e., we can use one variable for them both. In total, this means we will perform 5 substitutions using 4 variables. Jinja will substitute a variable anywhere it sees the following syntax, {{ variable_name }} . As such a valid template for our use-case may look like: { \"param1\": {{ str_var }}, \"param2\": {{ int_var }}, \"param3\": { \"a\": 1, \"b\": {{ p3_b }} }, \"param4\": [ {{ val }}, {{ val }}, 3 ] } We save this file as jsonuser.json in config/templates . Next, we will update the original pydantic model to include our template configuration. We still have an issue, however, in that we need to decide where to write the output of the template to. In this case, we can use the input_json parameter. We will assume that the user will provide this, although a default value can also be used. A custom validator will be added so that we can take the input_json value and update the value of lute_template_cfg.output_path with it. # from typing import Optional from pydantic import Field, validator from .base import TemplateConfig #, TemplateParameters class RunJsonUserParameters: executable: str = Field( \"jsonuser\", description=\"Executable which requires a JSON configuration file.\" ) # Lets assume the JSON file is passed as \"-i <path_to_json>\" input_json: str = Field( \"\", description=\"Path to the input JSON file.\", flag_type=\"-\", rename_param=\"i\" ) # Add template configuration! *MUST* be called `lute_template_cfg` lute_template_cfg: TemplateConfig = Field( TemplateConfig( template_name=\"jsonuser.json\", # Only the name of the file here. output_path=\"\", ), description=\"Template rendering configuration\", ) # We do not need to include these TemplateParameters, they will be added # automatically if provided in the YAML #str_var: Optional[TemplateParameters] #int_var: Optional[TemplateParameters] #p3_b: Optional[TemplateParameters] #val: Optional[TemplateParameters] # Tell LUTE to write the rendered template to the location provided with # `input_json`. I.e. update `lute_template_cfg.output_path` @validator(\"lute_template_cfg\", always=True) def update_output_path( cls, lute_template_cfg: TemplateConfig, values: Dict[str, Any] ) -> TemplateConfig: if lute_template_cfg.output_path == \"\": lute_template_cfg.output_path = values[\"input_json\"] return lute_template_cfg All that is left to render the template, is to provide the variables we want to substitute in the LUTE configuration YAML. In our case we must provide the 4 variable names we included within the substitution syntax ( {{ var_name }} ). The names in the YAML must match those in the template. RunJsonUser: input_json: \"/my/chosen/path.json\" # We'll come back to this... str_var: \"arg1\" # Will substitute for \"param1\": \"arg1\" int_var: 4 # Will substitute for \"param2\": 4 p3_b: 2 # Will substitute for \"param3: { \"b\": 2 } val: 2 # Will substitute for \"param4\": [2, 2, 3] in the JSON If on the other hand, a user were to have an already valid JSON file, it is possible to turn off the template rendering. (ALL) Template variables ( TemplateParameters ) are simply excluded from the configuration YAML. RunJsonUser: input_json: \"/path/to/existing.json\" #str_var: ... #...","title":"Using templates: managing third-party configuration files"},{"location":"tutorial/new_task/#additional-jinja-syntax","text":"There are many other syntactical constructions we can use with Jinja. Some of the useful ones are: If Statements - E.g. only include portions of the template if a value is defined. {% if VARNAME is defined %} // Stuff to include {% endif %} Loops - E.g. Unpacking multiple elements from a dictionary. {% for name, value in VARNAME.items() %} // Do stuff with name and value {% endfor %}","title":"Additional Jinja Syntax"},{"location":"tutorial/new_task/#creating-a-first-party-task","text":"The process for creating a \"First-Party\" Task is very similar to that for a \"Third-Party\" Task , with the difference being that you must also write the analysis code. The steps for integration are: 1. Write the TaskParameters model. 2. Write the Task class. There are a few rules that need to be adhered to. 3. Make your Task available by modifying the import function. 4. Specify an Executor","title":"Creating a \"First-Party\" Task"},{"location":"tutorial/new_task/#specifying-a-taskparameters-model-for-your-task_1","text":"Parameter models have a format that must be followed for \"Third-Party\" Task s, but \"First-Party\" Task s have a little more liberty in how parameters are dealt with, since the Task will do all the parsing itself. To create a model, the basic steps are: 1. If necessary, create a new module (e.g. new_task_category.py ) under lute.io.models , or find an appropriate pre-existing module in that directory. - An import statement must be added to lute.io.models._init_ if a new module is created, so it can be found. - If defining the model in a pre-existing module, make sure to modify the __all__ statement to include it. 2. Create a new model that inherits from TaskParameters . You can look at lute.models.io.tests.TestReadOutputParameters for an example. The model must be named <YourTaskName>Parameters - You should include all relevant parameters here, including input file, output file, and any potentially adjustable parameters. These parameters must be included even if there are some implicit dependencies between Task s and it would make sense for the parameter to be auto-populated based on some other output. Creating this dependency is done with validators (see step 3.). All parameters should be overridable, and all Task s should be fully-independently configurable, based solely on their model and the configuration YAML. - To follow the preferred format, parameters should be defined as: param_name: type = Field([default value], description=\"This parameter does X.\") 3. Use validators to do more complex things for your parameters, including populating default values dynamically: - E.g. create default values that depend on other parameters in the model - see for example: SubmitSMDParameters . - E.g. create default values that depend on other Task s by reading from the database - see for example: TestReadOutputParameters . 4. The model will have access to some general configuration values by inheriting from TaskParameters . These parameters are all stored in lute_config which is an instance of AnalysisHeader ( defined here ). - For example, the experiment and run number can be obtained from this object and a validator could use these values to define the default input file for the Task . A number of configuration options and Field attributes are also available for \"First-Party\" Task models. These are identical to those used for the ThirdPartyTask s, although there is a smaller selection. These options are reproduced below for convenience. Config settings and options Under the class definition for Config in the model, we can modify global options for all the parameters. In addition, there are a number of configuration options related to specifying what the outputs/results from the associated Task are, and a number of options to modify runtime behaviour. Currently, the available configuration options are: Config Parameter Meaning Default Value ThirdPartyTask-specific? run_directory If provided, can be used to specify the directory from which a Task is run. None (not provided) NO set_result bool . If True search the model definition for a parameter that indicates what the result is. False NO result_from_params If set_result is True can define a result using this option and a validator. See also is_result below. None (not provided) NO short_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s long_flags_use_eq Use equals sign instead of space for arguments of - parameters. False YES - Only affects ThirdPartyTask s These configuration options modify how the parameter models are parsed and passed along on the command-line, as well as what we consider results and where a Task can run. The default behaviour is that parameters are assumed to be passed as -p arg and --param arg , the Task will be run in the current working directory (or scratch if submitted with the ARP), and we have no information about Task results . Setting the above options can modify this behaviour. By setting short_flags_use_eq and/or long_flags_use_eq to True parameters are instead passed as -p=arg and --param=arg . By setting run_directory to a valid path, we can force a Task to be run in a specific directory. By default the Task will be run from the directory you submit the job in, or from your scratch folder ( /sdf/scratch/... ) if you submit from the eLog. Some ThirdPartyTask s rely on searching the correct working directory in order run properly. By setting set_result to True we indicate that the TaskParameters model will provide information on what the TaskResult is. This setting must be used with one of two options, either the result_from_params Config option, described below, or the Field attribute is_result described in the next sub-section ( Field Attributes ). result_from_params is a Config option that can be used when set_result==True . In conjunction with a validator (described a sections down) we can use this option to specify a result from all the information contained in the model. E.g. if you have a Task that has parameters for an output_directory and a output_filename , you can set result_from_params==f\"{output_directory}/{output_filename}\" . Field attributes In addition to the global configuration options there are a couple of ways to specify individual parameters. The following Field attributes are used when parsing the model: Field Attribute Meaning Default Value Example description Documentation of the parameter's usage or purpose. N/A arg = Field(..., description=\"Argument for...\") is_result bool . If the set_result Config option is True , we can set this to True to indicate a result. N/A output_result = Field(..., is_result=true)","title":"Specifying a TaskParameters Model for your Task"},{"location":"tutorial/new_task/#writing-the-task","text":"You can write your analysis code (or whatever code to be executed) as long as it adheres to the limited rules below. You can create a new module for your Task in lute.tasks or add it to any existing module, if it makes sense for it to belong there. The Task itself is a single class constructed as: 1. Your analysis Task is a class named in a way that matches its Pydantic model. E.g. RunTask is the Task , and RunTaskParameters is the Pydantic model. 2. The class must inherit from the Task class (see template below). 3. You must provide an implementation of a _run method. This is the method that will be executed when the Task is run. You can in addition write as many methods as you need. For fine-grained execution control you can also provide _pre_run() and _post_run() methods, but this is optional. 4. For all communication (including print statements) you should use the _report_to_executor(msg: Message) method. Since the Task is run as a subprocess this method will pass information to the controlling Executor . You can pass any type of object using this method, strings, plots, arrays, etc. 5. If you did not use the set_result configuration option in your parameters model, make sure to provide a result when finished. This is done by setting self._result.payload = ... . You can set the result to be any object. If you have written the result to a file, for example, please provide a path. A minimal template is provided below. \"\"\"Standard docstring...\"\"\" __all__ = [\"RunTask\"] __author__ = \"\" # Please include so we know who the SME is # Include any imports you need here from lute.execution.ipc import Message # Message for communication from lute.io.models.base import * # For TaskParameters from lute.tasks.task import * # For Task class RunTask(Task): # Inherit from Task \"\"\"Task description goes here, or in __init__\"\"\" def __init__(self, *, params: TaskParameters) -> None: super().__init__(params=params) # Sets up Task, parameters, etc. # Parameters will be available through: # self._task_parameters # You access with . operator: self._task_parameters.param1, etc. # Your result object is availble through: # self._result # self._result.payload <- Main result # self._result.summary <- Short summary # self._result.task_status <- Semi-automatic, but can be set manually def _run(self) -> None: # THIS METHOD MUST BE PROVIDED self.do_my_analysis() def do_my_analysis(self) -> None: # Send a message, proper way to print: msg: Message(contents=\"My message contents\", signal=\"\") self._report_to_executor(msg) # When done, set result - assume we wrote a file, e.g. self._result.payload = \"/path/to/output_file.h5\" # Optionally also set status - good practice but not obligatory self._result.task_status = TaskStatus.COMPLETED Signals in Message objects are strings and can be one of the following: LUTE_SIGNALS: Set[str] = { \"NO_PICKLE_MODE\", \"TASK_STARTED\", \"TASK_FAILED\", \"TASK_STOPPED\", \"TASK_DONE\", \"TASK_CANCELLED\", \"TASK_RESULT\", } Each of these signals is associated with a hook on the Executor -side. They are for the most part used by base classes; however, you can choose to make use of them manually as well.","title":"Writing the Task"},{"location":"tutorial/new_task/#making-your-task-available","text":"Once the Task has been written, it needs to be made available for import. Since different Task s can have conflicting dependencies and environments, this is managed through an import function. When the Task is done, or ready for testing, a condition is added to lute.tasks.__init__.import_task . For example, assume the Task is called RunXASAnalysis and it's defined in a module called xas.py , we would add the following lines to the import_task function: # in lute.tasks.__init__ # ... def import_task(task_name: str) -> Type[Task]: # ... if task_name == \"RunXASAnalysis\": from .xas import RunXASAnalysis return RunXASAnalysis","title":"Making your Task available"},{"location":"tutorial/new_task/#defining-an-executor","text":"The process of Executor definition is identical to the process as described for ThirdPartyTask s above.","title":"Defining an Executor"}]}